{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trajectory Recommendation - Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Random Guessing](#1.-Random-Guessing)\n",
    "1. [POI Scoring](#2.-POI-Scoring)\n",
    " 1. [Pairwise POI ranking using RankSVM](#2.1-Pairwise-POI-ranking-using-RankSVM)\n",
    " 1. [POI occurrence prediction](#2.2-POI-occurrence-prediction)\n",
    " 1. [Direct rank/location prediction](#2.3-Direct-rank/location-prediction)\n",
    " 1. [Pairwise POI ranking using logistic regression](#2.4-Pairwise-POI-ranking-using-logistic-regression)\n",
    "1. [Trajectory Scoring](#3.-Trajectory-Scoring)\n",
    " 1. [Transition Matrix between POIs](#3.1-Transition-Matrix-between-POIs)\n",
    " 1. [Inference: Viterbi Decoding vs ILP](#3.2-Inference:-Viterbi-Decoding-vs-ILP)\n",
    " 1. [Naively combine POI ranking and transition probabilities](#3.3-Naively-combine-POI-ranking-and-transition-probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#% matplotlib inline\n",
    "\n",
    "import os, sys, time, pickle, tempfile\n",
    "import math, random, itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import kron\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import cython\n",
    "import pulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234567890)\n",
    "ranksvm_dir = '$HOME/work/ranksvm'  # directory that contains rankSVM binaries: train, predict, svm-scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dat_ix``` is required in notebook ```shared.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_ix = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run notebook ```shared.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run 'shared.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_JOBS = 6         # number of parallel jobs\n",
    "USE_GUROBI = False # whether to use GUROBI as ILP solver\n",
    "ABS_SCALER = True  # feature scaling, True: MaxAbsScaler, False: MinMaxScaler #False: StandardScaler\n",
    "C_SET = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000]  # regularisation parameter\n",
    "ALPHA_SET = [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99]  # trade-off parameter\n",
    "MC_PORTION = 0.1   # the portion of data that sampled by Monte-Carlo cross-validation\n",
    "MC_NITER = 5       # number of iterations for Monte-Carlo cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method switches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_rand = False\n",
    "run_rank = False\n",
    "run_logreg = False\n",
    "run_linreg = False\n",
    "run_logpwr = True\n",
    "run_tran = False\n",
    "run_comb = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Random Guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_rand == True:\n",
    "    recdict_rand = dict()\n",
    "    cnt = 1\n",
    "    keys = sorted(TRAJ_GROUP_DICT.keys())\n",
    "\n",
    "    # outer loop to evaluate the test performance by cross validation\n",
    "    for i in range(len(keys)):\n",
    "        ps, L = keys[i]\n",
    "\n",
    "        # train model using all examples in training set and measure performance on test set\n",
    "        trajid_set_train = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "        poi_set = set()\n",
    "        for tid in trajid_set_train:\n",
    "            poi_set = poi_set | set(traj_dict[tid])\n",
    "        if ps not in poi_set: continue\n",
    "            \n",
    "        poi_list = sorted(poi_set)\n",
    "        np.random.shuffle(poi_list)\n",
    "        recdict_rand[(ps, L)] = {'PRED': [ps] + poi_list[:L-1]}\n",
    "       \n",
    "        print_progress(cnt, len(keys)); cnt += 1; sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_rand == True:\n",
    "    F1_rand = []; pF1_rand = []; Tau_rand = []\n",
    "    for key in sorted(recdict_rand.keys()):\n",
    "        F1, pF1, tau = evaluate(recdict_rand[key]['PRED'], TRAJ_GROUP_DICT[key])\n",
    "        F1_rand.append(F1); pF1_rand.append(pF1); Tau_rand.append(tau)\n",
    "    print('Random: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_rand), np.std(F1_rand)/np.sqrt(len(F1_rand)), \\\n",
    "           np.mean(pF1_rand), np.std(pF1_rand)/np.sqrt(len(pF1_rand)), \\\n",
    "           np.mean(Tau_rand), np.std(Tau_rand)/np.sqrt(len(Tau_rand))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if run_rand == True:\n",
    "    frand = os.path.join(data_dir, 'rand-' + dat_suffix[dat_ix] + '.pkl')\n",
    "    pickle.dump(recdict_rand, open(frand, 'bw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. POI Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query-based Leave-one-out Evaluation**\n",
    "\n",
    "Group trajectories according to queries, hold every group for test and use all other groups for training.\n",
    "\n",
    "The performance of a prediction $\\bf \\bar{y}$ given a query $\\bf x$ is the maximum (or average) similarity score between $\\bf \\bar{y}$ and every trajectory $\\bf y$ in test set (which conforms to $\\bf x$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Pairwise POI ranking using RankSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Training DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data are generated as follows:\n",
    "1. each input tuple $(\\text{startPOI}, \\text{#POI})$ form a `query` (in IR terminology).\n",
    "1. the label of a specific POI is the number of presence of that POI in the set of trajectories grouped by a specific `query`, excluding the presence as $\\text{startPOI}$. (the label of all absence POIs w.r.t. that `query` got a label `0`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of training data matrix is `#(qid, poi)` by `#feature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_train_subdf(poi_id, query_id_set, poi_info, poi_clusters, cats, clusters, query_id_rdict):\n",
    "    assert(isinstance(cats, list))\n",
    "    assert(isinstance(clusters, list))\n",
    "    \n",
    "    columns = DF_COLUMNS\n",
    "    poi_distmat = POI_DISTMAT\n",
    "    df_ = pd.DataFrame(index=np.arange(len(query_id_set)), columns=columns)\n",
    "    \n",
    "    pop, nvisit = poi_info.loc[poi_id, 'popularity'], poi_info.loc[poi_id, 'nVisit']\n",
    "    cat, cluster = poi_info.loc[poi_id, 'poiCat'], poi_clusters.loc[poi_id, 'clusterID'] \n",
    "    duration = poi_info.loc[poi_id, 'avgDuration']\n",
    "    \n",
    "    for j in range(len(query_id_set)):\n",
    "        qid = query_id_set[j]\n",
    "        assert(qid in query_id_rdict) # qid --> (start, end, length)\n",
    "        (p0, trajLen) = query_id_rdict[qid]\n",
    "        idx = df_.index[j]\n",
    "        df_.loc[idx, 'poiID'] = poi_id\n",
    "        df_.loc[idx, 'queryID'] = qid\n",
    "        df_.set_value(idx, 'category', tuple((cat == np.array(cats)).astype(np.int) * 2 - 1))\n",
    "        df_.set_value(idx, 'neighbourhood', tuple((cluster == np.array(clusters)).astype(np.int) * 2 - 1))\n",
    "        df_.loc[idx, 'popularity'] = LOG_SMALL if pop < 1 else np.log10(pop)\n",
    "        df_.loc[idx, 'nVisit'] = LOG_SMALL if nvisit < 1 else np.log10(nvisit)\n",
    "        df_.loc[idx, 'avgDuration'] = LOG_SMALL if duration < 1 else np.log10(duration)\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_info.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi_id, p0]\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'sameNeighbourhoodStart'] = 1 if cluster == poi_clusters.loc[p0, 'clusterID'] else -1\n",
    "        \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_train_df(trajid_list, traj_dict, poi_info, poi_clusters, cats, clusters, n_jobs=-1):    \n",
    "    columns = DF_COLUMNS\n",
    "    poi_distmat = POI_DISTMAT\n",
    "    query_id_dict = QUERY_ID_DICT\n",
    "    train_trajs = [traj_dict[x] for x in trajid_list if len(traj_dict[x]) >= 2]\n",
    "    \n",
    "    qid_set = sorted(set([query_id_dict[(t[0], len(t))] for t in train_trajs]))\n",
    "    poi_set = set()\n",
    "    for tr in train_trajs:\n",
    "        poi_set = poi_set | set(tr)\n",
    "    \n",
    "    query_id_rdict = dict()\n",
    "    for k, v in query_id_dict.items(): \n",
    "        query_id_rdict[v] = k  # qid --> (start, length)\n",
    "    \n",
    "    train_df_list = Parallel(n_jobs=n_jobs)\\\n",
    "                            (delayed(gen_train_subdf)(poi, qid_set, poi_info, poi_clusters,cats,clusters,query_id_rdict) \n",
    "                             for poi in poi_set)\n",
    "                        \n",
    "    assert(len(train_df_list) > 0)\n",
    "    df_ = train_df_list[0]\n",
    "    for j in range(1, len(train_df_list)):\n",
    "        df_ = df_.append(train_df_list[j], ignore_index=True)            \n",
    "        \n",
    "    # set label\n",
    "    df_.set_index(['queryID', 'poiID'], inplace=True)\n",
    "    df_['label'] = 0\n",
    "    for t in train_trajs:\n",
    "        qid = query_id_dict[(t[0], len(t))]\n",
    "        for poi in t[1:]:  # do NOT count if the POI is startPOI/endPOI\n",
    "            df_.loc[(qid, poi), 'label'] += 1\n",
    "\n",
    "    df_.reset_index(inplace=True)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Test DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data are generated the same way as training data, except that the labels of testing data (unknown) could be arbitrary values as suggested in [libsvm FAQ](http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f431).\n",
    "The reported accuracy (by `svm-predict` command) is meaningless as it is calculated based on these labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of training data matrix is `#poi` by `#feature` with one specific `query`, i.e. tuple $(\\text{startPOI}, \\text{#POI})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_test_df(startPOI, nPOI, poi_info, poi_clusters, cats, clusters):\n",
    "    assert(isinstance(cats, list))\n",
    "    assert(isinstance(clusters, list))\n",
    "    \n",
    "    columns = DF_COLUMNS\n",
    "    poi_distmat = POI_DISTMAT\n",
    "    query_id_dict = QUERY_ID_DICT\n",
    "    key = (p0, trajLen) = (startPOI, nPOI)\n",
    "    assert(key in query_id_dict)\n",
    "    assert(p0 in poi_info.index)\n",
    "    \n",
    "    df_ = pd.DataFrame(index=np.arange(poi_info.shape[0]), columns=columns)\n",
    "    poi_list = sorted(poi_info.index)\n",
    "    \n",
    "    qid = query_id_dict[key]\n",
    "    df_['queryID'] = qid\n",
    "    df_['label'] = np.random.rand(df_.shape[0]) # label for test data is arbitrary according to libsvm FAQ\n",
    "\n",
    "    for i in range(df_.index.shape[0]):\n",
    "        poi = poi_list[i]\n",
    "        lon, lat = poi_info.loc[poi, 'poiLon'], poi_info.loc[poi, 'poiLat']\n",
    "        pop, nvisit = poi_info.loc[poi, 'popularity'], poi_info.loc[poi, 'nVisit']\n",
    "        cat, cluster = poi_info.loc[poi, 'poiCat'], poi_clusters.loc[poi, 'clusterID']\n",
    "        duration = poi_info.loc[poi, 'avgDuration']\n",
    "        idx = df_.index[i]\n",
    "        df_.loc[idx, 'poiID'] = poi\n",
    "        df_.set_value(idx, 'category', tuple((cat == np.array(cats)).astype(np.int) * 2 - 1))\n",
    "        df_.set_value(idx, 'neighbourhood', tuple((cluster == np.array(clusters)).astype(np.int) * 2 - 1))\n",
    "        df_.loc[idx, 'popularity'] = LOG_SMALL if pop < 1 else np.log10(pop)\n",
    "        df_.loc[idx, 'nVisit'] = LOG_SMALL if nvisit < 1 else np.log10(nvisit)\n",
    "        df_.loc[idx, 'avgDuration'] = LOG_SMALL if duration < 1 else np.log10(duration)\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_all.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi, p0]\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'sameNeighbourhoodStart'] = 1 if cluster == poi_clusters.loc[p0, 'clusterID'] else -1\n",
    "        \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a string for a training/test data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data_str(df_, df_columns=DF_COLUMNS):\n",
    "    for col in df_columns:\n",
    "        assert(col in df_.columns)\n",
    "        \n",
    "    lines = []\n",
    "    for idx in df_.index:\n",
    "        slist = [str(df_.loc[idx, 'label'])]\n",
    "        slist.append(' qid:')\n",
    "        slist.append(str(int(df_.loc[idx, 'queryID'])))\n",
    "        fid = 1\n",
    "        for j in range(3, len(df_columns)):\n",
    "            values_ = df_.get_value(idx, df_columns[j])\n",
    "            values_ = values_ if isinstance(values_, tuple) else [values_]\n",
    "            for v in values_:\n",
    "                slist.append(' ')\n",
    "                slist.append(str(fid)); fid += 1\n",
    "                slist.append(':')\n",
    "                slist.append(str(v))\n",
    "        slist.append('\\n')\n",
    "        lines.append(''.join(slist))\n",
    "    return ''.join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Ranking POIs using rankSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the [rankSVM implementation](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#large_scale_ranksvm) could be [liblinear-ranksvm](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ranksvm/liblinear-ranksvm-2.1.zip) or [libsvm-ranksvm](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ranksvm/libsvm-ranksvm-3.20.zip), please read `README.ranksvm` in the zip file for installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [softmax function](https://en.wikipedia.org/wiki/Softmax_function) to convert ranking scores to a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x1 = x.copy()\n",
    "    x1 -= np.max(x1)  # numerically more stable, REF: http://cs231n.github.io/linear-classify/#softmax\n",
    "    expx = np.exp(x1)\n",
    "    return expx / np.sum(expx, axis=0) # column-wise sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a python wrapper of the `svm-train` or `train` and `svm-predict` or `predict` commands of rankSVM with ranking probabilities $P(p_i \\lvert (p_s, p_e, len))$ computed using [softmax function](https://en.wikipedia.org/wiki/Softmax_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python wrapper of rankSVM\n",
    "class RankSVM:\n",
    "    def __init__(self, bin_dir, useLinear=True, debug=False):\n",
    "        dir_ = !echo $bin_dir  # deal with environmental variables in path\n",
    "        assert(os.path.exists(dir_[0]))\n",
    "        self.bin_dir = dir_[0]\n",
    "        \n",
    "        self.bin_train = 'svm-train'\n",
    "        self.bin_predict = 'svm-predict'\n",
    "        if useLinear:\n",
    "            self.bin_train = 'train'\n",
    "            self.bin_predict = 'predict'\n",
    "        \n",
    "        assert(isinstance(debug, bool))\n",
    "        self.debug = debug\n",
    "        \n",
    "        # create named tmp files for model and feature scaling parameters\n",
    "        self.fmodel = None\n",
    "        self.fscale = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fmodel = fd.name\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fscale = fd.name\n",
    "        \n",
    "        if self.debug:\n",
    "            print('model file:', self.fmodel)\n",
    "            print('feature scaling parameter file:', self.fscale)\n",
    "    \n",
    "    \n",
    "    def __del__(self):\n",
    "        # remove tmp files\n",
    "        if self.debug == False:\n",
    "            if self.fmodel is not None and os.path.exists(self.fmodel):\n",
    "                os.unlink(self.fmodel)\n",
    "            if self.fscale is not None and os.path.exists(self.fscale):\n",
    "                os.unlink(self.fscale)\n",
    "\n",
    "    \n",
    "    def train(self, train_df, cost=1):\n",
    "        # cost is parameter C in SVM\n",
    "        # write train data to file\n",
    "        ftrain = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain = fd.name\n",
    "            datastr = gen_data_str(train_df)\n",
    "            fd.write(datastr)\n",
    "        \n",
    "        # feature scaling\n",
    "        ftrain_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -s $self.fscale $ftrain > $ftrain_scaled\n",
    "        \n",
    "        if self.debug:\n",
    "            print('cost:', cost)\n",
    "            print('train data file:', ftrain)\n",
    "            print('feature scaled train data file:', ftrain_scaled)\n",
    "        \n",
    "        # train rank svm and generate model file, if the model file exists, rewrite it\n",
    "        result = !$self.bin_dir/$self.bin_train -c $cost $ftrain_scaled $self.fmodel\n",
    "        if self.debug:\n",
    "            print('Training finished.')\n",
    "            for i in range(len(result)): print(result[i])\n",
    "\n",
    "        # remove train data file\n",
    "        if self.debug == False:\n",
    "            os.unlink(ftrain)\n",
    "            os.unlink(ftrain_scaled)        \n",
    "\n",
    "    \n",
    "    def predict(self, test_df):\n",
    "        # predict ranking scores for the given feature matrix\n",
    "        if self.fmodel is None or not os.path.exists(self.fmodel):\n",
    "            print('Model should be trained before prediction')\n",
    "            return\n",
    "        \n",
    "        # write test data to file\n",
    "        ftest = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftest = fd.name\n",
    "            datastr = gen_data_str(test_df)\n",
    "            fd.write(datastr)\n",
    "                \n",
    "        # feature scaling\n",
    "        ftest_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            ftest_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -r $self.fscale $ftest > $ftest_scaled\n",
    "            \n",
    "        # generate prediction file\n",
    "        fpredict = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            fpredict = fd.name\n",
    "            \n",
    "        if self.debug:\n",
    "            print('test data file:', ftest)\n",
    "            print('feature scaled test data file:', ftest_scaled)\n",
    "            print('predict result file:', fpredict)\n",
    "            \n",
    "        # predict using trained model and write prediction to file\n",
    "        result = !$self.bin_dir/$self.bin_predict $ftest_scaled $self.fmodel $fpredict\n",
    "        if self.debug:\n",
    "            print('Predict result: %-30s  %s' % (result[0], result[1]))\n",
    "        \n",
    "        # generate prediction DataFrame from prediction file\n",
    "        poi_rank_df = pd.read_csv(fpredict, header=None)\n",
    "        poi_rank_df.rename(columns={0:'rank'}, inplace=True)\n",
    "        poi_rank_df['poiID'] = test_df['poiID'].astype(np.int)\n",
    "        poi_rank_df.set_index('poiID', inplace=True)\n",
    "        poi_rank_df['probability'] = softmax(poi_rank_df['rank'])\n",
    "        \n",
    "        # remove test file and prediction file\n",
    "        if self.debug == False:\n",
    "            os.unlink(ftest)\n",
    "            os.unlink(ftest_scaled)\n",
    "            os.unlink(fpredict)\n",
    "\n",
    "        return poi_rank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross-validation with Monte-Carlo cross-validation as inner loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_rank == True:\n",
    "    recdict_rank = dict()\n",
    "    recdict_rank_pop = dict()\n",
    "    cnt = 1\n",
    "    keys = sorted(TRAJ_GROUP_DICT.keys())\n",
    "\n",
    "    # outer loop to evaluate the test performance by cross validation\n",
    "    for i in range(len(keys)):\n",
    "        ps, L = keys[i]\n",
    "\n",
    "        best_C = 1\n",
    "        #best_F1 = 0; best_pF1 = 0\n",
    "        best_Tau = 0\n",
    "        keys_cv = keys[:i] + keys[i+1:]\n",
    "\n",
    "        # tune regularisation constant C\n",
    "        for rank_C in C_SET:\n",
    "            print('\\n--------------- try_C: %f ---------------\\n' % rank_C); sys.stdout.flush() \n",
    "            F1_rank = []; pF1_rank = []; Tau_rank = []        \n",
    "\n",
    "            # inner loop to evaluate the performance of a model with a specified C by Monte-Carlo cross validation\n",
    "            for j in range(MC_NITER):\n",
    "                while True: # make sure the start POI in test set are also in training set\n",
    "                    rand_ix = np.arange(len(keys_cv)); np.random.shuffle(rand_ix)\n",
    "                    test_ix = rand_ix[:int(MC_PORTION*len(rand_ix))]\n",
    "                    assert(len(test_ix) > 0)\n",
    "                    trajid_set_train = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "                    for j in test_ix: \n",
    "                        trajid_set_train = trajid_set_train - TRAJ_GROUP_DICT[keys_cv[j]]\n",
    "                    poi_set = set()\n",
    "                    for tid in trajid_set_train: poi_set = poi_set | set(traj_dict[tid])\n",
    "                    good_partition = True\n",
    "                    for j in test_ix: \n",
    "                        if keys_cv[j][0] not in poi_set: good_partition = False; break\n",
    "                    if good_partition == True: break\n",
    "\n",
    "                # train\n",
    "                poi_info = calc_poi_info(list(trajid_set_train), traj_all, poi_all)\n",
    "                train_df = gen_train_df(list(trajid_set_train), traj_dict, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                        cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST, n_jobs=N_JOBS)\n",
    "                ranksvm = RankSVM(ranksvm_dir, useLinear=True)\n",
    "                ranksvm.train(train_df, cost=rank_C)\n",
    "                \n",
    "                # test\n",
    "                for j in test_ix:  # test\n",
    "                    ps_cv, L_cv = keys_cv[j]\n",
    "                    test_df = gen_test_df(ps_cv, L_cv, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                          cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "                    rank_df = ranksvm.predict(test_df)\n",
    "                    rank_df.sort_values(by='rank', ascending=False, inplace=True)\n",
    "                    y_hat = [ps_cv] + [p for p in rank_df.index.tolist() if p != ps_cv][:L_cv-1]\n",
    "                    F1, pF1, tau = evaluate(y_hat, TRAJ_GROUP_DICT[keys_cv[j]])\n",
    "                    F1_rank.append(F1); pF1_rank.append(pF1); Tau_rank.append(tau)\n",
    "                    \n",
    "            #mean_F1 = np.mean(F1_rank); mean_pF1 = np.mean(pF1_rank)\n",
    "            mean_Tau = np.mean(Tau_rank)\n",
    "            print('mean_Tau: %.3f' % mean_Tau)\n",
    "            if mean_Tau > best_Tau:\n",
    "                best_Tau = mean_Tau\n",
    "                best_C = rank_C\n",
    "        print('\\n--------------- %d/%d, Query: (%d, %d), Best_C: %f ---------------\\n' % (cnt,len(keys),ps,L,best_C))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # train model using all examples in training set and measure performance on test set\n",
    "        trajid_set_train = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "        poi_info = calc_poi_info(list(trajid_set_train), traj_all, poi_all)\n",
    "        train_df = gen_train_df(list(trajid_set_train), traj_dict, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST, n_jobs=N_JOBS)\n",
    "        ranksvm = RankSVM(ranksvm_dir, useLinear=True)\n",
    "        ranksvm.train(train_df, cost=best_C)\n",
    "        test_df = gen_test_df(ps, L, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                              cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "        rank_df = ranksvm.predict(test_df)\n",
    "        rank_df.sort_values(by='rank', ascending=False, inplace=True)\n",
    "        y_hat = [ps] + [p for p in rank_df.index.tolist() if p != ps][:L-1]\n",
    "        recdict_rank[(ps, L)] = {'PRED': y_hat, 'C':best_C}\n",
    "        \n",
    "        # POI popularity based ranking\n",
    "        poi_info.sort_values(by='popularity', ascending=False, inplace=True)\n",
    "        y_hat_pop = [ps] + [p for p in poi_info.index.tolist() if p != ps][:L-1]        \n",
    "        recdict_rank_pop[(ps, L)] = {'PRED': y_hat_pop}\n",
    "\n",
    "        cnt += 1; #print_progress(cnt, len(keys)); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_rank == True:\n",
    "    F1_rank = []; pF1_rank = []; Tau_rank = []\n",
    "    F1_pop  = []; pF1_pop  = []; Tau_pop  = []\n",
    "    for key in sorted(recdict_rank.keys()):\n",
    "        F1, pF1, tau = evaluate(recdict_rank[key]['PRED'], TRAJ_GROUP_DICT[key])\n",
    "        F1_rank.append(F1); pF1_rank.append(pF1); Tau_rank.append(tau)\n",
    "        assert(key in recdict_rank_pop)\n",
    "        F1, pF1, tau = evaluate(recdict_rank_pop[key]['PRED'], TRAJ_GROUP_DICT[key])\n",
    "        F1_pop.append(F1); pF1_pop.append(pF1); Tau_pop.append(tau)\n",
    "    print('RankSVM: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_rank), np.std(F1_rank)/np.sqrt(len(F1_rank)), \\\n",
    "           np.mean(pF1_rank), np.std(pF1_rank)/np.sqrt(len(pF1_rank)), \\\n",
    "           np.mean(Tau_rank), np.std(Tau_rank)/np.sqrt(len(Tau_rank))))\n",
    "    print('Popularity: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_pop), np.std(F1_pop)/np.sqrt(len(F1_pop)), \\\n",
    "           np.mean(pF1_pop), np.std(pF1_pop)/np.sqrt(len(pF1_pop)), \\\n",
    "           np.mean(Tau_pop), np.std(Tau_pop)/np.sqrt(len(Tau_pop))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if run_rank == True:\n",
    "    frank = os.path.join(data_dir, 'rank-' + dat_suffix[dat_ix] + '.pkl')\n",
    "    fpop  = os.path.join(data_dir, 'pop-' + dat_suffix[dat_ix] + '.pkl')\n",
    "    pickle.dump(recdict_rank, open(frank, 'bw'))\n",
    "    pickle.dump(recdict_rank_pop, open(fpop, 'bw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 POI occurrence prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_features(startPOI, nPOI, poi_info, poi_clusters, cats, clusters):\n",
    "    \"\"\"\n",
    "    Generate feature vectors for all POIs given query (startPOI, nPOI)\n",
    "    \"\"\"\n",
    "    assert(isinstance(cats, list))\n",
    "    assert(isinstance(clusters, list))\n",
    "    \n",
    "    columns = DF_COLUMNS[3:]\n",
    "    poi_distmat = POI_DISTMAT\n",
    "    query_id_dict = QUERY_ID_DICT\n",
    "    key = (p0, trajLen) = (startPOI, nPOI)\n",
    "    assert(key in query_id_dict)\n",
    "    assert(p0 in poi_info.index)\n",
    "    \n",
    "    df_ = pd.DataFrame(index=np.arange(poi_info.shape[0]), columns=columns)\n",
    "    poi_list = sorted(poi_info.index)\n",
    "        \n",
    "    for i in range(df_.index.shape[0]):\n",
    "        poi = poi_list[i]\n",
    "        lon, lat = poi_info.loc[poi, 'poiLon'], poi_info.loc[poi, 'poiLat']\n",
    "        pop, nvisit = poi_info.loc[poi, 'popularity'], poi_info.loc[poi, 'nVisit']\n",
    "        cat, cluster = poi_info.loc[poi, 'poiCat'], poi_clusters.loc[poi, 'clusterID']\n",
    "        duration = poi_info.loc[poi, 'avgDuration']\n",
    "        idx = df_.index[i]\n",
    "        df_.set_value(idx, 'category', tuple((cat == np.array(cats)).astype(np.int) * 2 - 1))\n",
    "        df_.set_value(idx, 'neighbourhood', tuple((cluster == np.array(clusters)).astype(np.int) * 2 - 1))\n",
    "        df_.loc[idx, 'popularity'] = LOG_SMALL if pop < 1 else np.log10(pop)\n",
    "        df_.loc[idx, 'nVisit'] = LOG_SMALL if nvisit < 1 else np.log10(nvisit)\n",
    "        df_.loc[idx, 'avgDuration'] = LOG_SMALL if duration < 1 else np.log10(duration)\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_all.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi, p0]\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'sameNeighbourhoodStart'] = 1 if cluster == poi_clusters.loc[p0, 'clusterID'] else -1\n",
    "    \n",
    "    # convert to matrix\n",
    "    nrows = df_.shape[0]\n",
    "    ncols = df_.shape[1] + len(cats) + len(clusters) - 2\n",
    "    \n",
    "    # features other than category and neighbourhood\n",
    "    X = df_[list(set(df_.columns) - {'category', 'neighbourhood'})].values  \n",
    "    \n",
    "    # boolean features: category (+1, -1)\n",
    "    cat_features = np.vstack([list(df_.loc[x, 'category']) for x in df_.index])\n",
    "    \n",
    "    # boolean features: neighbourhood (+1, -1)\n",
    "    neigh_features = np.vstack([list(df_.loc[x, 'neighbourhood']) for x in df_.index])\n",
    "    \n",
    "    return np.hstack([X, cat_features.astype(np.float), neigh_features.astype(np.float)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_labels(traj_truth, poi_info, binarise=False):\n",
    "    \"\"\"\n",
    "    Generate labels for all POIs given a ground truth trajectory\n",
    "    \"\"\"\n",
    "    poi_list = sorted(poi_info.index)\n",
    "    ranks = np.zeros(len(poi_list), dtype=np.float)\n",
    "    for j in range(len(poi_list)):\n",
    "        try:\n",
    "            poi = poi_list[j]\n",
    "            ranks[j] = (len(poi_list) - traj_truth.index(poi)) / len(poi_list) # normalise\n",
    "        except ValueError:\n",
    "            pass # default rank is 0\n",
    "        \n",
    "    if binarise == True:\n",
    "        return (ranks > 0).astype(np.float)*2 - 1 # binary labels (+1, -1)\n",
    "    \n",
    "    return ranks.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI occurrence prediction: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LOGIT_REG:\n",
    "    def __init__(self, C=1.0, debug=False):\n",
    "        self.C = C\n",
    "        self.debug = debug\n",
    "        self.trained = False\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self, trajid_set_train):\n",
    "        # generate training data\n",
    "        self.poi_info = calc_poi_info(list(trajid_set_train), traj_all, poi_all)\n",
    "        self.poi_list = sorted(self.poi_info.index)\n",
    "        train_traj_list = [traj_dict[x] for x in trajid_set_train if len(traj_dict[x]) >= 2]\n",
    "        feature_list = Parallel(n_jobs=N_JOBS)\\\n",
    "                               (delayed(gen_features)(tr[0], len(tr), self.poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                                      cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "                                for tr in train_traj_list)\n",
    "        labels_logreg = Parallel(n_jobs=N_JOBS)\\\n",
    "                                (delayed(gen_labels)(tr, self.poi_info, binarise=True) for tr in train_traj_list)\n",
    "        X = np.vstack(feature_list)\n",
    "        Y_logreg = np.hstack(labels_logreg)\n",
    "\n",
    "        # feature scaling\n",
    "        if ABS_SCALER == True:\n",
    "            self.scaler = MaxAbsScaler(copy=False)\n",
    "        else:\n",
    "            self.scaler = MinMaxScaler(feature_range=(-1,1), copy=False)\n",
    "            #self.scaler = StandardScaler(copy=False)\n",
    "        X = self.scaler.fit_transform(X)\n",
    "\n",
    "        # train\n",
    "        self.logreg = LogisticRegression(C=self.C, n_jobs=N_JOBS)\n",
    "        self.logreg.fit(X, Y_logreg)\n",
    "        self.trained = True\n",
    "        \n",
    "    \n",
    "    \n",
    "    def predict(self, startPOI, nPOI):\n",
    "        assert(self.trained == True)\n",
    "        if startPOI not in self.poi_list: return None\n",
    "        X_test = gen_features(startPOI, nPOI, self.poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                              cats=POI_CAT_LIST,clusters=POI_CLUSTER_LIST)\n",
    "        X_test = self.scaler.transform(X_test)\n",
    "        scores_logreg = self.logreg.decision_function(X_test)\n",
    "        \n",
    "        # form recommendation\n",
    "        assert(len(scores_logreg) == len(self.poi_list))\n",
    "        p0_ix = self.poi_list.index(startPOI)\n",
    "        scores_logreg[p0_ix] = -1e6  # mask the start POI\n",
    "        topk_logreg = list(np.argsort(-np.asarray(scores_logreg))[:nPOI-1])\n",
    "        y_hat = [startPOI] + list(np.array(self.poi_list)[topk_logreg])\n",
    "        return np.array(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross-validation with Monte-Carlo cross-validation as inner loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_logreg == True:\n",
    "    recdict_logreg = dict()\n",
    "    cnt = 1\n",
    "    keys = sorted(TRAJ_GROUP_DICT.keys())\n",
    "\n",
    "    # outer loop to evaluate the test performance by cross validation\n",
    "    for i in range(len(keys)):\n",
    "        ps, L = keys[i]\n",
    "\n",
    "        best_C = 1\n",
    "        #best_F1 = 0; best_pF1 = 0\n",
    "        best_Tau = 0\n",
    "        keys_cv = keys[:i] + keys[i+1:]\n",
    "\n",
    "        # tune regularisation constant C\n",
    "        for logit_C in C_SET:\n",
    "            print('\\n--------------- try_C: %f ---------------\\n' % logit_C); sys.stdout.flush() \n",
    "            F1_logreg = []; pF1_logreg = []; Tau_logreg = []        \n",
    "\n",
    "            # inner loop to evaluate the performance of a model with a specified C by Monte-Carlo cross validation\n",
    "            for j in range(MC_NITER):\n",
    "                while True: # make sure the start POI in test set are also in training set\n",
    "                    rand_ix = np.arange(len(keys_cv)); np.random.shuffle(rand_ix)\n",
    "                    test_ix = rand_ix[:int(MC_PORTION*len(rand_ix))]\n",
    "                    assert(len(test_ix) > 0)\n",
    "                    trajid_set_train = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "                    for j in test_ix: \n",
    "                        trajid_set_train = trajid_set_train - TRAJ_GROUP_DICT[keys_cv[j]]\n",
    "                    poi_set = set()\n",
    "                    for tid in trajid_set_train: poi_set = poi_set | set(traj_dict[tid])\n",
    "                    good_partition = True\n",
    "                    for j in test_ix: \n",
    "                        if keys_cv[j][0] not in poi_set: good_partition = False; break\n",
    "                    if good_partition == True: break\n",
    "                        \n",
    "                # train\n",
    "                logreg = LOGIT_REG(C=logit_C, debug=True)\n",
    "                logreg.train(list(trajid_set_train))\n",
    "                \n",
    "                # test\n",
    "                for j in test_ix:\n",
    "                    ps_cv, L_cv = keys_cv[j]\n",
    "                    y_hat = logreg.predict(ps_cv, L_cv)\n",
    "                    if y_hat is not None:\n",
    "                        F1, pF1, tau = evaluate(y_hat, TRAJ_GROUP_DICT[keys_cv[j]])\n",
    "                        F1_logreg.append(F1); pF1_logreg.append(pF1); Tau_logreg.append(tau)\n",
    "                        \n",
    "            #mean_F1 = np.mean(F1_logreg); mean_pF1 = np.mean(pF1_logreg)\n",
    "            mean_Tau = np.mean(Tau_logreg)\n",
    "            print('mean_Tau: %.3f' % mean_Tau)\n",
    "            if mean_Tau > best_Tau:\n",
    "                best_Tau = mean_Tau\n",
    "                best_C = logit_C\n",
    "        print('\\n--------------- %d/%d, Query: (%d, %d), Best_C: %f ---------------\\n' % (cnt, len(keys), ps, L, best_C))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # train model using all examples in training set and measure performance on test set\n",
    "        trajid_set_train = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "        logreg = LOGIT_REG(C=best_C, debug=True)\n",
    "        logreg.train(trajid_set_train)\n",
    "        y_hat = logreg.predict(ps, L)\n",
    "        if y_hat is not None:\n",
    "            recdict_logreg[(ps, L)] = {'PRED': y_hat, 'C': logreg.C}\n",
    "\n",
    "        cnt += 1; #print_progress(cnt, len(keys)); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_logreg == True:\n",
    "    F1_logreg = []; pF1_logreg = []; Tau_logreg = []\n",
    "    for key in sorted(recdict_logreg.keys()):\n",
    "        F1, pF1, tau = evaluate(recdict_logreg[key]['PRED'], TRAJ_GROUP_DICT[key])\n",
    "        F1_logreg.append(F1); pF1_logreg.append(pF1); Tau_logreg.append(tau)\n",
    "    print('LogReg: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_logreg), np.std(F1_logreg)/np.sqrt(len(F1_logreg)), \\\n",
    "           np.mean(pF1_logreg), np.std(pF1_logreg)/np.sqrt(len(pF1_logreg)), \\\n",
    "           np.mean(Tau_logreg), np.std(Tau_logreg)/np.sqrt(len(Tau_logreg))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if run_logreg == True:\n",
    "    flogreg = os.path.join(data_dir, 'logreg-' + dat_suffix[dat_ix] + '.pkl')\n",
    "    pickle.dump(recdict_logreg, open(flogreg, 'bw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Direct rank/location prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct rank/location prediction: linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LINEAR_REG:\n",
    "    def __init__(self, debug=False):\n",
    "        self.debug = debug\n",
    "        self.trained = False\n",
    "        \n",
    "    \n",
    "    def train(self, trajid_set_train):\n",
    "        self.poi_info = calc_poi_info(list(trajid_set_train), traj_all, poi_all)\n",
    "        self.poi_list = sorted(self.poi_info.index)\n",
    "          \n",
    "        # generate training data\n",
    "        train_traj_list = [traj_dict[x] for x in trajid_set_train if len(traj_dict[x]) >= 2]    \n",
    "        feature_list = Parallel(n_jobs=N_JOBS)\\\n",
    "                               (delayed(gen_features)(tr[0], len(tr), self.poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                        cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "                                for tr in train_traj_list)\n",
    "        labels_linreg = Parallel(n_jobs=N_JOBS)\\\n",
    "                                (delayed(gen_labels)(tr, self.poi_info, binarise=False) for tr in train_traj_list)\n",
    "        X = np.vstack(feature_list)\n",
    "        Y_linreg = np.hstack(labels_linreg)\n",
    "        \n",
    "        # remove training examples with label '0', i.e., features of POIs that do not exist in trajectory\n",
    "        #valid_ix = np.nonzero(Y_linreg)[0]\n",
    "        #X = X[valid_ix, :]\n",
    "        #Y_linreg = Y_linreg[valid_ix]\n",
    "        \n",
    "        # feature scaling\n",
    "        if ABS_SCALER == True:\n",
    "            self.scaler = MaxAbsScaler(copy=False)\n",
    "        else:\n",
    "            self.scaler = MinMaxScaler(feature_range=(-1,1), copy=False)\n",
    "            #self.scaler = StandardScaler(copy=False)\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # train\n",
    "        self.linreg = LinearRegression(normalize=True, n_jobs=N_JOBS)\n",
    "        self.linreg.fit(X, Y_linreg)\n",
    "        self.trained = True        \n",
    "    \n",
    "    \n",
    "    def predict(self, startPOI, nPOI):\n",
    "        assert(self.trained == True)\n",
    "        if startPOI not in self.poi_list: return None\n",
    "        X_test = gen_features(startPOI, nPOI, self.poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                              cats=POI_CAT_LIST,clusters=POI_CLUSTER_LIST)\n",
    "        X_test = self.scaler.transform(X_test)\n",
    "        scores_linreg = self.linreg.predict(X_test)\n",
    "        \n",
    "        # form recommendation\n",
    "        assert(len(scores_linreg) == len(self.poi_list))\n",
    "        p0_ix = self.poi_list.index(startPOI)\n",
    "        scores_linreg[p0_ix] = -1e6  # mask the start POI\n",
    "        topk_linreg = list(np.argsort(-np.asarray(scores_linreg))[:nPOI-1])\n",
    "        y_hat = [startPOI] + list(np.array(self.poi_list)[topk_linreg])\n",
    "        return np.array(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave-one-out cross-validation, NO hyper-parameters to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_linreg == True:\n",
    "    recdict_linreg = dict()\n",
    "    cnt = 1\n",
    "    keys = sorted(TRAJ_GROUP_DICT.keys())\n",
    "\n",
    "    # outer loop to evaluate the test performance by cross validation\n",
    "    for i in range(len(keys)):\n",
    "        ps, L = keys[i]\n",
    "\n",
    "        print('\\n--------------- %d/%d, Query: (%d, %d) ---------------\\n' % (cnt, len(keys), ps, L))\n",
    "\n",
    "        # train model using all examples in training set and measure performance on test set\n",
    "        trajid_set_train = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "        linreg = LINEAR_REG()\n",
    "        linreg.train(list(trajid_set_train))\n",
    "        y_hat = linreg.predict(ps, L)\n",
    "        if y_hat is not None:\n",
    "            recdict_linreg[(ps, L)] = {'PRED': y_hat}\n",
    "\n",
    "        cnt += 1; #print_progress(cnt, len(keys)); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_linreg == True:\n",
    "    F1_linreg = []; pF1_linreg = []; Tau_linreg = []\n",
    "    for key in sorted(recdict_linreg.keys()):\n",
    "        F1, pF1, tau = evaluate(recdict_linreg[key]['PRED'], TRAJ_GROUP_DICT[key])\n",
    "        F1_linreg.append(F1); pF1_linreg.append(pF1); Tau_linreg.append(tau)\n",
    "    print('LinReg: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_linreg), np.std(F1_linreg)/np.sqrt(len(F1_linreg)), \\\n",
    "           np.mean(pF1_linreg), np.std(pF1_linreg)/np.sqrt(len(pF1_linreg)), \\\n",
    "           np.mean(Tau_linreg), np.std(Tau_linreg)/np.sqrt(len(Tau_linreg))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if run_linreg == True:\n",
    "    flinreg = os.path.join(data_dir, 'linreg-' + dat_suffix[dat_ix] + '.pkl')\n",
    "    pickle.dump(recdict_linreg, open(flinreg, 'bw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Pairwise POI ranking using logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function for pairwise POI ranking using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "cpdef cost_logpwr(w, X, Y, float C, long M):\n",
    "    \"\"\"\n",
    "    w - parameter vector\n",
    "    X - feature matrix for all training examples (features of all POIs for the 1st example, then 2nd, ...)\n",
    "    Y - labels/ranks for all training examples (labels of all POIs for the 1st example, then 2nd, ...)\n",
    "    C - regularisation constant\n",
    "    M - total number of POIs\n",
    "    \"\"\"\n",
    "    assert(C > 0)\n",
    "    assert(M > 0)\n",
    "    cdef N, D, i, pj, pk, ix_j, ix_k\n",
    "    N = int(np.shape(X)[0]/M)\n",
    "    D = np.shape(X)[1]\n",
    "    assert(D == np.shape(w)[0])\n",
    "    assert(np.shape(X)[0] == np.shape(Y)[0])\n",
    "    \n",
    "    cdef double result, l_jk\n",
    "    result = 0.0\n",
    "    for i in range(N):\n",
    "        for pj in range(M):\n",
    "            ix_j = i*M + pj  # index of feature vector/label for POI pj\n",
    "            for pk in range(M):\n",
    "                if pj == pk:\n",
    "                    #result += np.log(2)  # constant\n",
    "                    continue\n",
    "                ix_k = i*M + pk\n",
    "                l_jk = 1.0\n",
    "                if Y[ix_j] < Y[ix_k]: l_jk = -1.0\n",
    "                result += np.log1p(np.exp(-1.0 * l_jk * np.dot(w, (X[ix_j] - X[ix_k])))) \n",
    "    \n",
    "    return 0.5 * np.dot(w, w) + result * C / N  # note the normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of the cost function for pairwise POI ranking using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "cpdef grad_logpwr(w, X, Y, float C, long M):\n",
    "    \"\"\"\n",
    "    w - parameter vector\n",
    "    X - feature matrix for all training examples (features of all POIs for the 1st example, then 2nd, ...)\n",
    "    Y - labels/ranks for all training examples (labels of all POIs for the 1st example, then 2nd, ...)\n",
    "    C - regularisation constant\n",
    "    M - total number of POIs\n",
    "    \"\"\"\n",
    "    assert(C > 0)\n",
    "    assert(M > 0)\n",
    "    cdef int N, D, i, pj, pk, ix_j, ix_k\n",
    "    N = int(np.shape(X)[0]/M)\n",
    "    D = np.shape(X)[1]\n",
    "    assert(D == np.shape(w)[0])\n",
    "    assert(np.shape(X)[0] == np.shape(Y)[0])\n",
    "    \n",
    "    cdef double l_jk\n",
    "    grad = np.zeros(D, dtype=np.float)\n",
    "    for i in range(N):\n",
    "        for pj in range(M):\n",
    "            ix_j = i*M + pj  # index of feature vector/label for POI pj\n",
    "            for pk in range(M):\n",
    "                if pj == pk: continue\n",
    "                ix_k = i*M + pk\n",
    "                l_jk = 1.0\n",
    "                if Y[ix_j] < Y[ix_k]: l_jk = -1.0\n",
    "                term = l_jk * (X[ix_j] - X[ix_k])  # vector\n",
    "                grad = grad + term * (-1.0 / (1.0 + np.exp(np.dot(w, term))))\n",
    "    \n",
    "    return w + grad * C / N  # note the normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairwise POI ranking using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LOGIT_PWR:\n",
    "    def __init__(self, C=1.0, debug=False):\n",
    "        self.C = C\n",
    "        self.debug = debug\n",
    "        self.trained = False\n",
    "        \n",
    "       \n",
    "    def train(self, trajid_set_train):\n",
    "        self.poi_info = calc_poi_info(list(trajid_set_train), traj_all, poi_all)\n",
    "        self.poi_list = sorted(self.poi_info.index)\n",
    "        \n",
    "        train_traj_list = [traj_dict[x] for x in trajid_set_train if len(traj_dict[x]) >= 2]            \n",
    "        feature_list = Parallel(n_jobs=N_JOBS)\\\n",
    "                               (delayed(gen_features)(tr[0], len(tr), self.poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                        cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "                                for tr in train_traj_list)\n",
    "        labels_linreg = Parallel(n_jobs=N_JOBS)\\\n",
    "                                (delayed(gen_labels)(tr, self.poi_info, binarise=False) for tr in train_traj_list)\n",
    "        X = np.vstack(feature_list)\n",
    "        Y_linreg = np.hstack(labels_linreg)\n",
    "        \n",
    "        # remove training examples with label '0', i.e., features of POIs that do not exist in trajectory\n",
    "        #valid_ix = np.nonzero(Y_linreg)[0]\n",
    "        #X = X[valid_ix, :]\n",
    "        #Y_linreg = Y_linreg[valid_ix]\n",
    "        \n",
    "        # feature scaling\n",
    "        if ABS_SCALER == True:\n",
    "            self.scaler = MaxAbsScaler(copy=False)\n",
    "        else:\n",
    "            self.scaler = MinMaxScaler(feature_range=(-1,1), copy=False)\n",
    "            #self.scaler = StandardScaler(copy=False)\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        \n",
    "        w = np.random.rand(np.shape(X)[1])  # initial guess\n",
    "        opt_method = 'BFGS' #'Newton-CG' \n",
    "        options = {'disp': True} if self.debug == True else dict()\n",
    "        opt = minimize(cost_logpwr, w, args=(X, Y_linreg, self.C, len(self.poi_list)), \\\n",
    "                       method=opt_method, jac=grad_logpwr, options=options)\n",
    "        if opt.success == True:\n",
    "            self.w = opt.x\n",
    "            self.trained = True\n",
    "        else:\n",
    "            sys.stderr.write('Optimisation failed, C=%f\\n' % self.C)\n",
    "            self.trained = False\n",
    "        return self.trained\n",
    "        \n",
    "    \n",
    "    def predict(self, startPOI, nPOI):\n",
    "        assert(self.trained == True)\n",
    "        if startPOI not in self.poi_list: return None\n",
    "        X_test = gen_features(startPOI, nPOI, self.poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                              cats=POI_CAT_LIST,clusters=POI_CLUSTER_LIST)\n",
    "        X_test = self.scaler.transform(X_test)\n",
    "        scores_logpwr = 1.0 / (1.0 + np.exp(-1.0 * np.dot(X_test, self.w)))\n",
    "        \n",
    "        # form recommendation\n",
    "        assert(len(scores_logpwr) == len(self.poi_list))\n",
    "        p0_ix = self.poi_list.index(startPOI)\n",
    "        scores_logpwr[p0_ix] = -1e6  # mask the start POI\n",
    "        topk_logpwr = list(np.argsort(-np.asarray(scores_logpwr))[:nPOI-1])\n",
    "        y_hat = [startPOI] + list(np.array(self.poi_list)[topk_logpwr])\n",
    "        return np.array(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross-validation with Monte-Carlo cross-validation as inner loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_logpwr == True:\n",
    "    recdict_logpwr = dict()\n",
    "    cnt = 1\n",
    "    keys = sorted(TRAJ_GROUP_DICT.keys())\n",
    "\n",
    "    # outer loop to evaluate the test performance by cross validation\n",
    "    for i in range(len(keys)):\n",
    "        ps, L = keys[i]\n",
    "\n",
    "        best_C = 1\n",
    "        #best_F1 = 0; best_pF1 = 0\n",
    "        best_Tau = 0\n",
    "        keys_cv = keys[:i] + keys[i+1:]\n",
    "\n",
    "        # tune regularisation constant C\n",
    "        for logit_C in C_SET:\n",
    "            print('\\n--------------- try_C: %f ---------------\\n' % logit_C); sys.stdout.flush() \n",
    "            F1_logpwr = []; pF1_logpwr = []; Tau_logpwr = []        \n",
    "\n",
    "            # inner loop to evaluate the performance of a model with a specified C by Monte-Carlo cross validation\n",
    "            for j in range(MC_NITER):\n",
    "                while True: # make sure the start POI in test set are also in training set\n",
    "                    rand_ix = np.arange(len(keys_cv)); np.random.shuffle(rand_ix)\n",
    "                    test_ix = rand_ix[:int(MC_PORTION*len(rand_ix))]\n",
    "                    assert(len(test_ix) > 0)\n",
    "                    trajid_set_train = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "                    for j in test_ix: \n",
    "                        trajid_set_train = trajid_set_train - TRAJ_GROUP_DICT[keys_cv[j]]\n",
    "                    poi_set = set()\n",
    "                    for tid in trajid_set_train: poi_set = poi_set | set(traj_dict[tid])\n",
    "                    good_partition = True\n",
    "                    for j in test_ix: \n",
    "                        if keys_cv[j][0] not in poi_set: good_partition = False; break\n",
    "                    if good_partition == True: break\n",
    "\n",
    "                # train\n",
    "                logpwr = LOGIT_PWR(C=logit_C, debug=True)\n",
    "                if logpwr.train(list(trajid_set_train)) == True:\n",
    "                    for j in test_ix:  # test\n",
    "                        ps_cv, L_cv = keys_cv[j]\n",
    "                        y_hat = logpwr.predict(ps_cv, L_cv)\n",
    "                        if y_hat is not None:\n",
    "                            F1, pF1, tau = evaluate(y_hat, TRAJ_GROUP_DICT[keys_cv[j]])\n",
    "                            F1_logpwr.append(F1); pF1_logpwr.append(pF1); Tau_logpwr.append(tau)\n",
    "                else:  # if training is failed\n",
    "                    for j in test_ix:\n",
    "                        F1_logpwr.append(0); pF1_logpwr.append(0); Tau_logpwr.append(0)\n",
    "\n",
    "            #mean_F1 = np.mean(F1_logpwr); mean_pF1 = np.mean(pF1_logpwr)\n",
    "            mean_Tau = np.mean(Tau_logpwr)\n",
    "            print('mean_Tau: %.3f' % mean_Tau)\n",
    "            if mean_Tau > best_Tau:\n",
    "                best_Tau = mean_Tau\n",
    "                best_C = logit_C\n",
    "        print('\\n--------------- %d/%d, Query: (%d, %d), Best_C: %f ---------------\\n' % (cnt, len(keys), ps, L, best_C))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # train model using all examples in training set and measure performance on test set\n",
    "        trajid_set_train = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "        logpwr = LOGIT_PWR(C=best_C, debug=True)\n",
    "        if logpwr.train(trajid_set_train) == True:\n",
    "            y_hat = logpwr.predict(ps, L)\n",
    "            if y_hat is not None:\n",
    "                recdict_logpwr[(ps, L)] = {'PRED': y_hat, 'W': logpwr.w, 'C': logpwr.C}\n",
    "\n",
    "        cnt += 1; #print_progress(cnt, len(keys)); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_logpwr == True:\n",
    "    F1_logpwr = []; pF1_logpwr = []; Tau_logpwr = []\n",
    "    for key in sorted(recdict_logpwr.keys()):\n",
    "        F1, pF1, tau = evaluate(recdict_logpwr[key]['PRED'], TRAJ_GROUP_DICT[key])\n",
    "        F1_logpwr.append(F1); pF1_logpwr.append(pF1); Tau_logpwr.append(tau)\n",
    "    print('LogPwr: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_logpwr), np.std(F1_logpwr)/np.sqrt(len(F1_logpwr)), \\\n",
    "           np.mean(pF1_logpwr), np.std(pF1_logpwr)/np.sqrt(len(pF1_logpwr)), \\\n",
    "           np.mean(Tau_logpwr), np.std(Tau_logpwr)/np.sqrt(len(Tau_logpwr))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if run_logpwr == True:\n",
    "    flogpwr = os.path.join(data_dir, 'logpwr-' + dat_suffix[dat_ix] + '.pkl')\n",
    "    pickle.dump(recdict_logpwr, open(flogpwr, 'bw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Trajectory Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Transition Matrix between POIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate transition probabilities (matrix) between different POI features (vector) using the [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product) of individual transition matrix corresponding to each feature, i.e., POI category, POI popularity (discritized), POI average visit duration (discritized) and POI neighborhoods (clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with features without corresponding POIs and feature with more than one corresponding POIs. (*Before Normalisation*)\n",
    "- For features without corresponding POIs, just remove the rows and columns from the matrix obtained by Kronecker product.\n",
    "- For different POIs with the exact same feature, \n",
    "  - Let POIs with the same feature as a POI group,\n",
    "  - The *incoming* **transition value (i.e., unnormalised transition probability)** of this POI group \n",
    "    should be divided uniformly among the group members, \n",
    "    *which corresponds to choose a group member uniformly at random in the incoming case*.\n",
    "  - The *outgoing* transition value should be duplicated (i.e., the same) among all group members, \n",
    "    **as we were already in that group in the outgoing case**.\n",
    "  - For each POI in the group, the allocation transition value of the *self-loop of the POI group* is similar to \n",
    "    that in the *outgoing* case, **as we were already in that group**, so just duplicate and then divide uniformly among \n",
    "    the transitions from this POI to other POIs in the same group, \n",
    "    *which corresponds to choose a outgoing transition uniformly at random from all outgoing transitions\n",
    "    excluding the self-loop of this POI*.\n",
    "- **Concretely**, for a POI group with $n$ POIs, \n",
    "    1. If the *incoming* transition value of POI group is $m_1$,\n",
    "       then the corresponding *incoming* transition value for each group member is $\\frac{m_1}{n}$.\n",
    "    1. If the *outgoing* transition value of POI group is $m_2$,\n",
    "       then the corresponding *outgoing* transition value for each group member is also $m_2$.\n",
    "    1. If the transition value of *self-loop of the POI group* is $m_3$,\n",
    "       then transition value of *self-loop of individual POIs* should be $0$,  \n",
    "       and *other in-group transitions* with value $\\frac{m_3}{n-1}$\n",
    "       as the total number of outgoing transitions to other POIs in the same group is $n-1$ (excluding the self-loop),\n",
    "       i.e. $n-1$ choose $1$.\n",
    "       \n",
    "**NOTE**: execute the above division before or after row normalisation will lead to the same result, *as the division itself does NOT change the normalising constant of each row (i.e., the sum of each row before normalising)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_poi_logtransmat(trajid_list, poi_set, traj_dict, poi_info, debug=False):\n",
    "    transmat_cat                        = gen_transmat_cat(trajid_list, traj_dict, poi_info)\n",
    "    transmat_pop,      logbins_pop      = gen_transmat_pop(trajid_list, traj_dict, poi_info)\n",
    "    transmat_visit,    logbins_visit    = gen_transmat_visit(trajid_list, traj_dict, poi_info)\n",
    "    transmat_duration, logbins_duration = gen_transmat_duration(trajid_list, traj_dict, poi_info)\n",
    "    transmat_neighbor, poi_clusters     = gen_transmat_neighbor(trajid_list, traj_dict, poi_info)\n",
    "\n",
    "    # Kronecker product\n",
    "    transmat_ix = list(itertools.product(transmat_cat.index, transmat_pop.index, transmat_visit.index, \\\n",
    "                                         transmat_duration.index, transmat_neighbor.index))\n",
    "    transmat_value = transmat_cat.values\n",
    "    for transmat in [transmat_pop, transmat_visit, transmat_duration, transmat_neighbor]:\n",
    "        transmat_value = kron(transmat_value, transmat.values)\n",
    "    transmat_feature = pd.DataFrame(data=transmat_value, index=transmat_ix, columns=transmat_ix)\n",
    "    \n",
    "    poi_train = sorted(poi_set)\n",
    "    feature_names = ['poiCat', 'popularity', 'nVisit', 'avgDuration', 'clusterID']\n",
    "    poi_features = pd.DataFrame(data=np.zeros((len(poi_train), len(feature_names))), \\\n",
    "                                columns=feature_names, index=poi_train)\n",
    "    poi_features.index.name = 'poiID'\n",
    "    poi_features['poiCat'] = poi_info.loc[poi_train, 'poiCat']\n",
    "    poi_features['popularity'] = np.digitize(poi_info.loc[poi_train, 'popularity'], logbins_pop)\n",
    "    poi_features['nVisit'] = np.digitize(poi_info.loc[poi_train, 'nVisit'], logbins_visit)\n",
    "    poi_features['avgDuration'] = np.digitize(poi_info.loc[poi_train, 'avgDuration'], logbins_duration)\n",
    "    poi_features['clusterID'] = poi_clusters.loc[poi_train, 'clusterID']\n",
    "    \n",
    "    # shrink the result of Kronecker product and deal with POIs with the same features\n",
    "    poi_logtransmat = pd.DataFrame(data=np.zeros((len(poi_train), len(poi_train)), dtype=np.float), \\\n",
    "                                   columns=poi_train, index=poi_train)\n",
    "    for p1 in poi_logtransmat.index:\n",
    "        rix = tuple(poi_features.loc[p1])\n",
    "        for p2 in poi_logtransmat.columns:\n",
    "            cix = tuple(poi_features.loc[p2])\n",
    "            value_ = transmat_feature.loc[(rix,), (cix,)]\n",
    "            poi_logtransmat.loc[p1, p2] = value_.values[0, 0]\n",
    "    \n",
    "    # group POIs with the same features\n",
    "    features_dup = dict()\n",
    "    for poi in poi_features.index:\n",
    "        key = tuple(poi_features.loc[poi])\n",
    "        if key in features_dup:\n",
    "            features_dup[key].append(poi)\n",
    "        else:\n",
    "            features_dup[key] = [poi]\n",
    "    if debug == True:\n",
    "        for key in sorted(features_dup.keys()):\n",
    "            print(key, '->', features_dup[key])\n",
    "            \n",
    "    # deal with POIs with the same features\n",
    "    for feature in sorted(features_dup.keys()):\n",
    "        n = len(features_dup[feature])\n",
    "        if n > 1:\n",
    "            group = features_dup[feature]\n",
    "            v1 = poi_logtransmat.loc[group[0], group[0]]  # transition value of self-loop of POI group\n",
    "            \n",
    "            # divide incoming transition value (i.e. unnormalised transition probability) uniformly among group members\n",
    "            for poi in group:\n",
    "                poi_logtransmat[poi] /= n\n",
    "                \n",
    "            # outgoing transition value has already been duplicated (value copied above)\n",
    "            \n",
    "            # duplicate & divide transition value of self-loop of POI group uniformly among all outgoing transitions,\n",
    "            # from a POI to all other POIs in the same group (excluding POI self-loop)\n",
    "            v2 = v1 / (n - 1)\n",
    "            for pair in itertools.permutations(group, 2):\n",
    "                poi_logtransmat.loc[pair[0], pair[1]] = v2\n",
    "                            \n",
    "    # normalise each row\n",
    "    for p1 in poi_logtransmat.index:\n",
    "        poi_logtransmat.loc[p1, p1] = 0\n",
    "        rowsum = poi_logtransmat.loc[p1].sum()\n",
    "        assert(rowsum > 0)\n",
    "        logrowsum = np.log10(rowsum)\n",
    "        for p2 in poi_logtransmat.columns:\n",
    "            if p1 == p2:\n",
    "                poi_logtransmat.loc[p1, p2] = LOG_ZERO  # deal with log(0) explicitly\n",
    "            else:\n",
    "                poi_logtransmat.loc[p1, p2] = np.log10(poi_logtransmat.loc[p1, p2]) - logrowsum\n",
    "    \n",
    "    return poi_logtransmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#transmat_ = gen_poi_logtransmat(trajid_set_all, set(poi_info_all.index), traj_dict, poi_info_all, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Inference: Viterbi Decoding vs ILP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dynamic programming to find a possibly non-simple path, i.e., walk.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_viterbi(V, E, ps, L, withNodeWeight=False, alpha=0.5):\n",
    "    assert(isinstance(V, pd.DataFrame))\n",
    "    assert(isinstance(E, pd.DataFrame))\n",
    "    assert(ps in V.index)\n",
    "    assert(2 <= L <= V.index.shape[0])\n",
    "    if withNodeWeight == True:\n",
    "        assert(0 < alpha < 1)\n",
    "        beta = 1 - alpha\n",
    "    else:\n",
    "        alpha = 0\n",
    "        beta = 1\n",
    "        weightkey = 'weight'\n",
    "        if weightkey not in V.columns:\n",
    "            V['weight'] = 1  # dummy weights, will not be used as alpha=0\n",
    "    poi_id_dict, poi_id_rdict = dict(), dict()\n",
    "    for ix, poi in enumerate(V.index):\n",
    "        poi_id_dict[poi] = ix\n",
    "        poi_id_rdict[ix] = poi\n",
    "    M = V.shape[0]\n",
    "    assert(E.shape[0] >= M)\n",
    "    assert(E.shape[1] >= M)\n",
    "    \n",
    "    A = np.zeros((L-1, M), dtype=np.float)     # scores matrix\n",
    "    B = np.ones((L-1, M), dtype=np.int) * (-1) # backtracking pointers\n",
    "    \n",
    "    for p in range(M): # ps--p1\n",
    "        p1 = poi_id_rdict[p]\n",
    "        A[0, p] = alpha * V.loc[p1, 'weight'] + beta * E.loc[ps, p1] if p1 != ps else -np.inf\n",
    "        B[0, p] = poi_id_dict[ps]\n",
    "\n",
    "    for t in range(0, L-2): # ps~~p2'--p3\n",
    "        for p in range(M):\n",
    "            p3 = poi_id_rdict[p]\n",
    "            scores = [A[t, p2] + alpha * V.loc[p3, 'weight'] + beta * E.loc[poi_id_rdict[p2], p3] \\\n",
    "                      if poi_id_rdict[p2] not in {ps, p3} else -np.inf for p2 in range(M)] \n",
    "            maxix = np.argmax(scores)\n",
    "            A[t+1, p] = scores[maxix]\n",
    "            B[t+1, p] = maxix\n",
    "            \n",
    "    y_hat = [np.argmax(A[L-2, :])]\n",
    "    p, t = y_hat[-1], L-2\n",
    "    while t >= 0:\n",
    "        y_hat.append(B[t, p])\n",
    "        p, t = y_hat[-1], t-1\n",
    "    y_hat.reverse()\n",
    "\n",
    "    return np.asarray([poi_id_rdict[p] for p in y_hat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use integer linear programming (ILP) to find a simple path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_ILP(V, E, ps, L, withNodeWeight=False, alpha=0.5):\n",
    "    assert(isinstance(V, pd.DataFrame))\n",
    "    assert(isinstance(E, pd.DataFrame))\n",
    "    assert(ps in V.index)\n",
    "    assert(2 <= L <= V.index.shape[0])\n",
    "    if withNodeWeight == True:\n",
    "        assert(0 < alpha < 1)\n",
    "    beta = 1 - alpha\n",
    "    \n",
    "    p0 = str(ps); M = V.index.shape[0]\n",
    "    \n",
    "    # REF: pythonhosted.org/PuLP/index.html\n",
    "    pois = [str(p) for p in V.index] # create a string list for each POI\n",
    "    pb = pulp.LpProblem('MostLikelyTraj', pulp.LpMaximize) # create problem\n",
    "    # visit_i_j = 1 means POI i and j are visited in sequence\n",
    "    visit_vars = pulp.LpVariable.dicts('visit', (pois, pois), 0, 1, pulp.LpInteger) \n",
    "    # isend_l = 1 means POI l is the END POI of trajectory\n",
    "    isend_vars = pulp.LpVariable.dicts('isend', pois, 0, 1, pulp.LpInteger) \n",
    "    # a dictionary contains all dummy variables\n",
    "    dummy_vars = pulp.LpVariable.dicts('u', [x for x in pois if x != p0], 2, M, pulp.LpInteger)\n",
    "    \n",
    "    # add objective\n",
    "    objlist = []\n",
    "    if withNodeWeight == True:\n",
    "        objlist.append(alpha * V.loc[int(p0), 'weight'])\n",
    "    for pi in [x for x in pois]:     # from\n",
    "        for pj in [y for y in pois if y != p0]: # to\n",
    "            if withNodeWeight == True:\n",
    "                objlist.append(visit_vars[pi][pj] * (alpha * V.loc[int(pj), 'weight'] + beta * E.loc[int(pi), int(pj)]))\n",
    "            else:\n",
    "                objlist.append(visit_vars[pi][pj] * E.loc[int(pi), int(pj)])\n",
    "    pb += pulp.lpSum(objlist), 'Objective'\n",
    "    \n",
    "    # add constraints, each constraint should be in ONE line\n",
    "    pb += pulp.lpSum([visit_vars[pi][pi] for pi in pois]) == 0, 'NoSelfLoops'\n",
    "    pb += pulp.lpSum([visit_vars[p0][pj] for pj in pois]) == 1, 'StartAt_p0'\n",
    "    pb += pulp.lpSum([visit_vars[pi][p0] for pi in pois]) == 0, 'NoIncoming_p0'\n",
    "    pb += pulp.lpSum([visit_vars[pi][pj] for pi in pois for pj in pois]) == L-1, 'Length'\n",
    "    pb += pulp.lpSum([isend_vars[pi] for pi in pois]) == 1, 'OneEnd'\n",
    "    pb += isend_vars[p0] == 0, 'StartNotEnd'\n",
    "    \n",
    "    for pk in [x for x in pois if x != p0]:\n",
    "        pb += pulp.lpSum([visit_vars[pi][pk] for pi in pois]) == isend_vars[pk] + \\\n",
    "              pulp.lpSum([visit_vars[pk][pj] for pj in pois if pj != p0]), 'ConnectedAt_' + pk\n",
    "        pb += pulp.lpSum([visit_vars[pi][pk] for pi in pois]) <= 1, 'Enter_' + pk + '_AtMostOnce'\n",
    "        pb += pulp.lpSum([visit_vars[pk][pj] for pj in pois if pj != p0]) + isend_vars[pk] <= 1, \\\n",
    "              'Leave_' + pk + '_AtMostOnce'\n",
    "    for pi in [x for x in pois if x != p0]:\n",
    "        for pj in [y for y in pois if y != p0]:\n",
    "            pb += dummy_vars[pi] - dummy_vars[pj] + 1 <= (M - 1) * (1 - visit_vars[pi][pj]), \\\n",
    "                    'SubTourElimination_' + pi + '_' + pj\n",
    "    #pb.writeLP(\"traj_tmp.lp\")\n",
    "    \n",
    "    # solve problem: solver should be available in PATH\n",
    "    if USE_GUROBI == True:\n",
    "        gurobi_options = [('TimeLimit', '7200'), ('Threads', str(N_JOBS)), ('NodefileStart', '0.2'), ('Cuts', '2')]\n",
    "        pb.solve(pulp.GUROBI_CMD(path='gurobi_cl', options=gurobi_options)) # GUROBI\n",
    "    else:\n",
    "        pb.solve(pulp.COIN_CMD(path='cbc', options=['-threads', str(N_JOBS), '-strategy', '1', '-maxIt', '2000000']))#CBC\n",
    "    visit_mat = pd.DataFrame(data=np.zeros((len(pois), len(pois)), dtype=np.float), index=pois, columns=pois)\n",
    "    isend_vec = pd.Series(data=np.zeros(len(pois), dtype=np.float), index=pois)\n",
    "    for pi in pois:\n",
    "        isend_vec.loc[pi] = isend_vars[pi].varValue\n",
    "        for pj in pois: visit_mat.loc[pi, pj] = visit_vars[pi][pj].varValue\n",
    "    #visit_mat.to_csv('visit.csv')\n",
    "\n",
    "    # build the recommended trajectory\n",
    "    recseq = [p0]\n",
    "    while True:\n",
    "        pi = recseq[-1]\n",
    "        pj = visit_mat.loc[pi].idxmax()\n",
    "        value = visit_mat.loc[pi, pj]\n",
    "        #print(value, int(round(value)))\n",
    "        #print(recseq)\n",
    "        assert(int(round(value)) == 1)\n",
    "        recseq.append(pj)\n",
    "        if len(recseq) == L: \n",
    "            assert(int(round(isend_vec[pj])) == 1)\n",
    "            #print('===:', recseq, ':====')\n",
    "            return np.asarray([int(x) for x in recseq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend trajectories by leveraging POI-POI transition probabilities (maximise the transition likelihood)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave-one-out cross-validation, NO hyper-parameters to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_tran == True:\n",
    "    recdict_tran_DP = dict()\n",
    "    recdict_tran_ILP = dict()\n",
    "    cnt = 1\n",
    "    keys = sorted(TRAJ_GROUP_DICT.keys())\n",
    "\n",
    "    # outer loop to evaluate the test performance by cross validation\n",
    "    for i in range(len(keys)):\n",
    "        ps, L = keys[i]\n",
    "        \n",
    "        print('\\n--------------- %d/%d, Query: (%d, %d) ---------------\\n' % (cnt, len(keys), ps, L))\n",
    "\n",
    "        # train model using all examples in training set and measure performance on test set\n",
    "        trajid_set_train = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "        poi_info = calc_poi_info(list(trajid_set_train), traj_all, poi_all)\n",
    "           \n",
    "        if ps not in poi_info.index: continue\n",
    "        \n",
    "        poi_logtransmat = gen_poi_logtransmat(trajid_set_train, set(poi_info.index), traj_dict, poi_info)\n",
    "        edges = poi_logtransmat\n",
    "\n",
    "        recdict_tran_DP[(ps, L)] = {'PRED': find_viterbi(poi_info.copy(), edges.copy(), ps, L)}\n",
    "        recdict_tran_ILP[(ps, L)] = {'PRED': find_ILP(poi_info, edges, ps, L)}\n",
    "        assert(len(recdict_tran_ILP[(ps, L)]) == len(set(recdict_tran_ILP[(ps, L)])))\n",
    "       \n",
    "        cnt += 1; #print_progress(cnt, len(keys)); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_tran == True:\n",
    "    F1_DP = []; pF1_DP = []; Tau_DP = []\n",
    "    F1_ILP = []; pF1_ILP = []; Tau_ILP = []\n",
    "    for key in sorted(recdict_tran_DP.keys()):\n",
    "        F1, pF1, tau = evaluate(recdict_tran_DP[key]['PRED'], TRAJ_GROUP_DICT[key])\n",
    "        F1_DP.append(F1); pF1_DP.append(pF1); Tau_DP.append(tau)\n",
    "        assert(key in recdict_tran_ILP)\n",
    "        F1, pF1, tau = evaluate(recdict_tran_ILP[key]['PRED'], TRAJ_GROUP_DICT[key])\n",
    "        F1_ILP.append(F1); pF1_ILP.append(pF1); Tau_ILP.append(tau)\n",
    "    print('Transition Viterbi: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_DP), np.std(F1_DP)/np.sqrt(len(F1_DP)), \\\n",
    "           np.mean(pF1_DP), np.std(pF1_DP)/np.sqrt(len(pF1_DP)), \\\n",
    "           np.mean(Tau_DP), np.std(Tau_DP)/np.sqrt(len(Tau_DP))))\n",
    "    print('Transition ILP: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_ILP), np.std(F1_ILP)/np.sqrt(len(F1_ILP)), \\\n",
    "           np.mean(pF1_ILP), np.std(pF1_ILP)/np.sqrt(len(pF1_ILP)), \\\n",
    "           np.mean(Tau_ILP), np.std(Tau_ILP)/np.sqrt(len(Tau_ILP))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if run_tran == True:\n",
    "    fDP = os.path.join(data_dir, 'tranDP-' + dat_suffix[dat_ix] + '.pkl')\n",
    "    fILP = os.path.join(data_dir, 'tranILP-' + dat_suffix[dat_ix] + '.pkl')\n",
    "    pickle.dump(recdict_tran_DP, open(fDP, 'bw'))\n",
    "    pickle.dump(recdict_tran_ILP, open(fILP, 'bw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Naively combine POI ranking and transition probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_methods = [find_viterbi, find_ILP]\n",
    "methods_str = ['DP', 'ILP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method_ix = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross-validation with Monte-Carlo cross-validation as inner loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_comb == True:\n",
    "    recdict_comb = dict()\n",
    "    cnt = 1\n",
    "    keys = sorted(TRAJ_GROUP_DICT.keys())\n",
    "    inference_fun = comb_methods[method_ix]\n",
    "\n",
    "    # outer loop to evaluate the test performance by cross validation\n",
    "    for i in range(len(keys)):\n",
    "        ps, L = keys[i]\n",
    "\n",
    "        best_C = 1\n",
    "        best_alpha = 0.5\n",
    "        #best_F1 = 0; best_pF1 = 0\n",
    "        best_Tau = 0\n",
    "        keys_cv = keys[:i] + keys[i+1:]\n",
    "\n",
    "        # tune regularisation constant C\n",
    "        for rank_C in C_SET:\n",
    "            for alpha in ALPHA_SET:\n",
    "                print('\\n--------------- try_C: %.3f, try_alpha: %.3f ---------------\\n' % (rank_C, alpha))\n",
    "                sys.stdout.flush()\n",
    "                F1_comb = []; pF1_comb = []; Tau_comb = []        \n",
    "\n",
    "                # inner loop to evaluate the performance of a model with a specified C by Monte-Carlo cross validation\n",
    "                for j in range(MC_NITER):\n",
    "                    while True: # make sure the start POI in test set are also in training set\n",
    "                        rand_ix = np.arange(len(keys_cv)); np.random.shuffle(rand_ix)\n",
    "                        test_ix = rand_ix[:int(MC_PORTION*len(rand_ix))]\n",
    "                        assert(len(test_ix) > 0)\n",
    "                        trajid_set_train = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "                        for j in test_ix: \n",
    "                            trajid_set_train = trajid_set_train - TRAJ_GROUP_DICT[keys_cv[j]]\n",
    "                        poi_set = set()\n",
    "                        for tid in trajid_set_train: poi_set = poi_set | set(traj_dict[tid])\n",
    "                        good_partition = True\n",
    "                        for j in test_ix: \n",
    "                            if keys_cv[j][0] not in poi_set: good_partition = False; break\n",
    "                        if good_partition == True: break\n",
    "\n",
    "                    # train\n",
    "                    poi_info = calc_poi_info(list(trajid_set_train), traj_all, poi_all)\n",
    "                    train_df = gen_train_df(list(trajid_set_train), traj_dict, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                            cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST, n_jobs=N_JOBS)\n",
    "                    ranksvm = RankSVM(ranksvm_dir, useLinear=True)\n",
    "                    ranksvm.train(train_df, cost=rank_C)\n",
    "                    poi_logtransmat = gen_poi_logtransmat(trajid_set_train, set(poi_info.index), traj_dict, poi_info)\n",
    "                    edges = poi_logtransmat                \n",
    "\n",
    "                    # test\n",
    "                    for j in test_ix:  # test\n",
    "                        ps_cv, L_cv = keys_cv[j]\n",
    "                        test_df = gen_test_df(ps_cv, L_cv, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                              cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "                        rank_df = ranksvm.predict(test_df)\n",
    "                        nodes = rank_df.copy()\n",
    "                        nodes['weight'] = np.log10(nodes['probability'])\n",
    "\n",
    "                        y_hat = inference_fun(nodes, edges.copy(), ps_cv, L_cv, withNodeWeight=True, alpha=alpha)\n",
    "                        F1, pF1, tau = evaluate(y_hat, TRAJ_GROUP_DICT[keys_cv[j]])\n",
    "                        F1_comb.append(F1); pF1_comb.append(pF1); Tau_comb.append(tau)\n",
    "\n",
    "                #mean_F1 = np.mean(F1_comb); mean_pF1 = np.mean(pF1_comb)\n",
    "                mean_Tau = np.mean(Tau_comb)\n",
    "                print('mean_Tau: %.3f' % mean_Tau)\n",
    "                if mean_Tau > best_Tau:\n",
    "                    best_Tau = mean_Tau\n",
    "                    best_C = rank_C\n",
    "                    best_alpha = alpha\n",
    "        print('\\n--------------- %d/%d, Query: (%d, %d), Best_C: %.3f, Best_alpha: %.3f ---------------\\n' % \\\n",
    "              (cnt, len(keys), ps, L, best_C, best_alpha))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # train model using all examples in training set and measure performance on test set\n",
    "        trajid_set_train = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "        poi_info = calc_poi_info(list(trajid_set_train), traj_all, poi_all)\n",
    "        train_df = gen_train_df(list(trajid_set_train), traj_dict, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST, n_jobs=N_JOBS)\n",
    "        ranksvm = RankSVM(ranksvm_dir, useLinear=True)\n",
    "        ranksvm.train(train_df, cost=best_C)\n",
    "        test_df = gen_test_df(ps, L, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                              cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "        rank_df = ranksvm.predict(test_df)\n",
    "        nodes = rank_df.copy()\n",
    "        nodes['weight'] = np.log10(nodes['probability'])\n",
    "        poi_logtransmat = gen_poi_logtransmat(trajid_set_train, set(poi_info.index), traj_dict, poi_info)\n",
    "        edges = poi_logtransmat \n",
    "\n",
    "        y_hat = inference_fun(nodes, edges, ps, L, withNodeWeight=True, alpha=best_alpha)\n",
    "        recdict_comb[(ps, L)] = {'PRED': y_hat, 'C': best_C, 'alpha': best_alpha}\n",
    "\n",
    "        cnt += 1; #print_progress(cnt, len(keys)); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_comb == True:\n",
    "    F1_comb = []; pF1_comb = []; Tau_comb = []\n",
    "    for key in sorted(recdict_comb.keys()):\n",
    "        F1, pF1, tau = evaluate(recdict_comb[key]['PRED'], TRAJ_GROUP_DICT[key])\n",
    "        F1_comb.append(F1); pF1_comb.append(pF1); Tau_comb.append(tau)\n",
    "    print('Comb ' + methods_str[method_ix] + ': F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_comb), np.std(F1_comb)/np.sqrt(len(F1_comb)), \\\n",
    "           np.mean(pF1_comb), np.std(pF1_comb)/np.sqrt(len(pF1_comb)), \\\n",
    "           np.mean(Tau_comb), np.std(Tau_comb)/np.sqrt(len(Tau_comb))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if run_comb == True:\n",
    "    fcomb = os.path.join(data_dir, 'comb' + methods_str[method_ix] + '-' + dat_suffix[dat_ix] + '.pkl')\n",
    "    pickle.dump(recdict_comb, open(fcomb, 'bw'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
