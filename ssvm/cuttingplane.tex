%\documentclass[10pt,a4paper]{article}
%\documentclass[twocolumn,10pt,a4paper]{article}
%\documentclass[twocolumn,a4wide,9pt]{extarticle}
\documentclass[9pt]{extarticle}
\usepackage[a4paper,top=0.85in,left=0.75in,bottom=1in,right=0.52in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[sc]{mathpazo}
\linespread{1.05}         % Palladio needs more leading (space between lines)
\usepackage[T1]{fontenc}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\eat}[1]{}
\setlength{\columnsep}{1.5em} % spacing between columns

\title{Notes on Cutting-plane Methods}

\author{Dawei Chen}

\date{\today}

\begin{document}

\maketitle

\section{Problem Setting}
\label{sec:problem}

The goal of cutting-plane methods is to find/localise a point in a convex \textit{target set} $Z \in \mathbb{R}^n$,
or determine that $Z$ is empty in some cases. 
The method does not assume any direct access to the description of $Z$,
such as the objective and constraint functions in an optimisation problem, except through a \textit{cutting-plane oracle}.
The method generates a query point $q$ and pass it to the oracle, 
the oracle either tells us that $q \in Z$ (in which case we are done), or it returns a hyperplane which separates $q$ from $Z$.
This hyperplane is called a \textit{cutting-plane}, or \textit{cut}, since it eliminates a half-space from our search.

Cutting-plane methods are also known as \textit{localisation} methods. 
A conceptual description of cutting-plane methods is shown in Algorithm~\ref{alg:cutting-plane}.


\begin{algorithm}[htbp]
\caption{Cutting-plane algorithm}
\label{alg:cutting-plane}
\begin{algorithmic}[1]
\STATE \textbf{Given}: an initial polyhedron $\mathcal{P}_0$ that contains $Z$.
\STATE $k = 0$
\REPEAT
    \STATE Generate a query point $q^{(k+1)}$ in $\mathcal{P}_k$
    \STATE Query the oracle at $q^{(k+1)}$
    \IF{~The oracle determines that $q^{(k+1)} \in Z$~}
        \RETURN $q^{(k+1)}$
    \ELSIF{~The oracle returns a cutting-plane $a_{k+1}^\top z \le b_{k+1}$~}
        \STATE Update constraints: $\mathcal{P}_{k+1} = \mathcal{P}_k \cap \{z | a_{k+1}^\top z \le b_{k+1} \}$
    \ENDIF
    \STATE $k = k + 1$
\UNTIL{Convergence or $\mathcal{P}_{k+1} = \emptyset$}
\end{algorithmic}
\end{algorithm}


\noindent
For a convex optimisation problem with $m$ constraints,

\begin{equation}
\label{eq:cvxprob}
\begin{aligned}
\min_{x} ~& f_0(x)        & \\
s.t.~~   ~& f_i(x) \le 0, & i = 1, \dots, m
\end{aligned} 
\end{equation}
where $f_0, \dots, f_m$ are convex and differentiable, the target set $Z$ is the optimal (or $\varepsilon$-suboptimal) set.

Given a query point $q$, the oracle first checks for feasibility.
If $q$ is not feasible, we choose one of the violated constraints $f_j(q) > 0$ and construct a cut
\begin{equation*}
f_j(q) + \nabla f_j(q)^\top (x - q) \le 0,
\end{equation*}
which is called a \textit{feasibility cut} for problem (\ref{eq:cvxprob}) since it cuts away the half-space 
$\{z | f_j(q) + \nabla f_j(q)^\top (z - q) > 0 \}$ with infeasible points.

\eat{
Proof of infeasibility.
}

\eat{
Multiple cuts for more than one violated constraints.
}
If $q$ is feasible, we form a cutting-plane
\begin{equation*}
\nabla f_0(q)^\top (x - q) \le 0,
\end{equation*}
which is called an \textit{objective cut} for problem (\ref{eq:cvxprob}) and it cuts out the half-space
$\{z | \nabla f_0(q)^\top (x - q) > 0 \}$
since all such points have an objective value larger than $f_0(q)$ and hence cannot be optimal.

\eat{
Proof of non-optimal.
}

If $q$ is feasible and $\nabla f_0(q) = 0$ then $q$ is optimal.
For non-differentiable problems, the gradients $\nabla f_j(z)$ can generally be replaced by sub-gradients.


\section{Generate query points}
We would like to generate a query point $q^{(k+1)}$ in the current polyhedron $\mathcal{P}_{k}$ such that 
the resulting cut reduces the size of $\mathcal{P}_{k+1}$ as much as possible.
However, when we query the oracle at point $q^{(k+1)}$, we do not know in which direction the generated cut will be excluded.
If we measure the informativeness of the $k$-th cut using the volume reduction ratio $\frac{V(\mathcal{P}_{k+1})}{V(\mathcal{P}_{k})}$,
we seek a point $q^{(k+1)}$ such that, no matter which direction to cut (returned by the oracle), we can obtain a certain guaranteed volume reduction.

\subsection{Method of Kelley-Cheney-Goldstein}
Given query points $q^{(1)}, \dots, q^{(k)}$, 
one approach to choose the next query point $q^{(k+1)}$ is to greedily use the vertex of the current polyhedron $\mathcal{P}_k$ 
that minimises the objective, which can be found by solving 
\begin{equation}
\label{eq:kcg}
\begin{aligned}
\min_{z} ~& \theta  \\
s.t.~~   ~& \theta \ge f_0(q^{(i)}) + \nabla f_0(q^{(i)})^\top (z - q^{(i)}),~ \forall i \le k \\
          & a_i^\top z \le b_i,~ \forall i \le k \\
          & f_j(z) \le 0,~ j = 1, \dots, m
\end{aligned}
\end{equation}
where $a_i, b_i$ are the set of existing cutting planes returned by querying the oracle at points $q^{(i)}, i \le k$.


\subsection{Chebyshev center method}
If we rescale the gradients $\nabla f_0(q^{(i)})$ to unit length in problem (\ref{eq:kcg}), 
it results in finding the center of the largest Euclidean ball that lies inside the current polyhedron $\mathcal{P}_k$,
in other words, we find the next query point $q^{(k+1)}$ by solving
\begin{equation}
\label{eq:chebyshev}
\begin{aligned}
\min_{z} ~& \theta  \\
s.t.~~   ~& \theta \ge f_0(q^{(i)}) + \frac{\nabla f_0(q^{(i)})}{\|\nabla f_0(q^{(i)})\|} ^\top (z - q^{(i)}),~ \forall i \le k \\
          & a_i^\top z \le b_i,~ \forall i \le k \\
          & f_j(z) \le 0.~ j = 1, \dots, m
\end{aligned}
\end{equation}
This variant is called the Chebyshev center method, which is shown to have significantly better convergence properties than the method of Kelley-Cheney-Goldstein.
\eat{citation}


\subsection{Analytic center cutting plane method}
Given a linear constraint $a_i^\top z \le b_i$, we define a slack variable $s_i \in \mathbb{R}$ as $s_i = b_i - a_i^\top z$,
that is, $s_i$ measures how far the current solution is from the constraint.
The analytic center is defined as the unique maximiser of the function
\begin{equation}
\label{eq:accpm}
\argmax_z \prod_i s_i = \argmax_z ~ \sum_{i=1}^k \log(b_i - a_i^\top z) + \sum_{j=1}^m \log(d_j - c_j^\top z),
\end{equation}
where we assume constraints $f_j(z) \le 0$ in problem (\ref{eq:cvxprob}) are linear and rewrite them as $c_j^\top z \le d_j$.
The unique maximiser of (\ref{eq:accpm}) can be efficiently found using Newton iterations.

\eat{citation}

The analytic center cutting plane method (ACCPM) chooses the analytic center of polyhedron 
\begin{equation*}
\mathcal{P}_k = \{ z | c_j^\top z \le d_j, ~ j=1, \dots, m \text{~and~} a_i^\top z \le b_i, ~ i=1, \dots, k \}
\end{equation*}
to query the oracle.
ACCPM seems to give a good trade-off in terms of simplicity and practical performance.

\eat{citation}


\subsection{Center of gravity/Bayes point method}
Assume set $\mathcal{C} \subseteq \mathbb{R}^n$ is bounded and has nonempty interior. 
The center of gravity of $\mathcal{C}$ is defined as
\begin{equation}
\textbf{cg}(\mathcal{C}) = \frac{\int_\mathcal{C} z dz}{\int_\mathcal{C} dz}.
\end{equation}

The center of gravity (CG) method chooses the point $q^{(k+1)} = \textbf{cg}(\mathcal{P}_{k})$ to query the oracle.
It turns out that this method has a very good convergence property in terms of the worst-case volume reduction factor,
in particular, we always have
\begin{equation}
\frac{V(\mathcal{P}_{k+1})}{V(\mathcal{P}_{k})} \le 1 - \frac{1}{e} \approx 0.63,
\end{equation}
in other words, the volume of the localisation polyhedron is reduced by at least $37\%$ at each iteration,
and this guarantee is completely independent of all problem parameters, including the dimension $n$.
However, it is \textit{extremely difficult} to compute the center of gravity of a polyhedron in $\mathbb{R}^n$, described by a set of linear inequalities,
which makes this method impractical.
Variants that compute an approximate center of gravity have been developed, and some of these approximations can be used to create a practical CG method.

\eat{citation}



\section{Train structured SVMs using cutting-plane methods}
\label{sec:ssvm}

Given $n$ training examples $(\mathbf{x}_1, \mathbf{y}_1), \dots, (\mathbf{x}_n, \mathbf{y}_n)$, 
the structured SVM with margin-rescaling\footnote{For brevity, structured SVMs with slack-rescaling are not described in this document.}
can be formulated as a quadratic program (QP)
\begin{equation}
\label{eq:nslackform}
\begin{aligned}
\min_{\mathbf{w}, ~\bm{\xi} \ge 0} ~& \frac{1}{2} \mathbf{w}^\top \mathbf{w} + \frac{C}{n} \sum_{i=1}^n \xi_i \\
s.t.~~ ~& \mathbf{w}^\top \Psi(\mathbf{x}_i, \mathbf{y}_i) - \mathbf{w}^\top \Psi(\mathbf{x}_i, \bar{\mathbf{y}}) \ge 
       \Delta(\mathbf{y}_i, \bar{\mathbf{y}}) - \xi_i, ~(\forall i, \bar{\mathbf{y}} \neq \mathbf{y}_i)
\end{aligned}
\end{equation}
where $\mathbf{w}$ is the parameter vector, $C > 0$ is a regularisation constant, and $\xi_i$
is a slack variable that represents the \emph{hinge loss} associated with the prediction for the $i$-th example,
\begin{equation*}
\xi_i = \max \left( 0,~ 
        \max_{\bar{\mathbf{y}} \in \mathcal{Y}} 
        \left\{ \Delta(\mathbf{y}_i, \bar{\mathbf{y}}) + \mathbf{w}^\top \Psi(\mathbf{x}_i, \bar{\mathbf{y}}) \right\} -
        \mathbf{w}^\top \Psi(\mathbf{x}_i, \mathbf{y}_i) \right).
\end{equation*}
This formulation is called "$n$-slack" as we have one slack variable for each example in training set. \eat{citation}

The algorithm to train the $n$-slack formulation of structured SVMs is described in Algorithm~\ref{alg:nslacktrain}.

\begin{algorithm}[htbp]
\caption{Cutting-plane algorithm for training $n$-slack formulation of structured SVMs (with margin-rescaling)}
\label{alg:nslacktrain}
\begin{algorithmic}[1]
\STATE \textbf{Input}: $S = \left( (\mathbf{x}_1, \mathbf{y}_1), \dots, (\mathbf{x}_n, \mathbf{y}_n) \right),~ C,~ \varepsilon$
\STATE $\mathcal{W} = \emptyset,~ \mathbf{w} = \mathbf{0},~ \bm{\xi} = \mathbf{0}$
\REPEAT
    \FOR{$i = 1,\dots,n$}
        \STATE Query the oracle at point $(\mathbf{w}, \bm{\xi})$:~
               $\hat{\mathbf{y}} = \argmax_{\bar{\mathbf{y}} \in \mathcal{Y}} \{ \Delta(\mathbf{y}_i, \bar{\mathbf{y}}) + 
               \mathbf{w}^\top \Psi(\mathbf{x}_i, \bar{\mathbf{y}}) \}$
        \IF{$~\mathbf{w}^\top \Psi(\mathbf{x}_i, \mathbf{y}_i) - \mathbf{w}^\top \Psi(\mathbf{x}_i, \hat{\mathbf{y}}) + \varepsilon < 
            \Delta(\mathbf{y}_i, \hat{\mathbf{y}}) - \xi_i~$}
            \STATE Update constraints:~
                   $\mathcal{W} = \mathcal{W} \cup 
                    \{\mathbf{w}^\top \Psi(\mathbf{x}_i, \mathbf{y}_i) - \mathbf{w}^\top \Psi(\mathbf{x}_i, \hat{\mathbf{y}}) \ge 
                    \Delta(\mathbf{y}_i, \hat{\mathbf{y}}) - \xi_i \}$
            \STATE Generate next query point $(\mathbf{w}, \bm{\xi})$ by solving QP~(\ref{eq:nslackform}) w.r.t. all constraints in $\mathcal{W}$
        \ENDIF
    \ENDFOR
\UNTIL{$\mathcal{W}$ has not changed during iteration}
\RETURN $(\mathbf{w}, \bm{\xi})$
\end{algorithmic}
\end{algorithm}


Another formulation of structured SVMs which results in more efficient training is called "$1$-slack" formulation (with margin-rescaling),
it replaces the $n$ cutting-plane models of the hinge loss (one for each training example) with a single cutting-plane model for 
the sum of the hinge-losses, as a result, only one slack variable is needed,
\begin{equation}
\label{eq:1slackform}
\begin{aligned}
\min_{\mathbf{w}, ~\xi \ge 0} ~& \frac{1}{2} \mathbf{w}^\top \mathbf{w} + C \xi \\
s.t.~~ ~& \forall(\bar{\mathbf{y}}_1, \dots, \bar{\mathbf{y}}_n) \in \mathcal{Y}^n: 
          \frac{1}{n} \sum_{i=1}^n 
          \left( \mathbf{w}^\top \Psi(\mathbf{x}_i, \mathbf{y}_i) - \mathbf{w}^\top \Psi(\mathbf{x}_i, \bar{\mathbf{y}}_i) \right) \ge
          \frac{1}{n} \sum_{i=1}^n \Delta(\mathbf{y}_i, \bar{\mathbf{y}}_i) - \xi.
\end{aligned}
\end{equation}
Here the slack variable $\xi$ represents the \emph{sum of the hinge-losses} over all training examples,
\begin{equation*}
\xi = \max \left( 0,~ 
      \max_{(\bar{\mathbf{y}}_1, \dots, \bar{\mathbf{y}}_n) \in \mathcal{Y}^n} 
      \left\{ 
      \frac{1}{n} \sum_{i=1}^n \left( \Delta(\mathbf{y}_i, \bar{\mathbf{y}}_i) + \mathbf{w}^\top \Psi(\mathbf{x}_i, \bar{\mathbf{y}}_i) \right)
      \right\} - \frac{1}{n} \sum_{i=1}^n \mathbf{w}^\top \Psi(\mathbf{x}_i, \mathbf{y}_i)
      \right).
\end{equation*}

The algorithm to train the $1$-slack formulation of structured SVMs is described in Algorithm~\ref{alg:1slacktrain}.

\begin{algorithm}[htbp]
\caption{Cutting-plane algorithm for training $1$-slack formulation of structured SVMs (with margin-rescaling)}
\label{alg:1slacktrain}
\begin{algorithmic}[1]
\STATE \textbf{Input}: $S = \left( (\mathbf{x}_1, \mathbf{y}_1), \dots, (\mathbf{x}_n, \mathbf{y}_n) \right),~ C,~ \varepsilon$
\STATE $\mathcal{W} = \emptyset$
\REPEAT
    \STATE Generate query point $(\mathbf{w}, \xi)$ by solving QP~(\ref{eq:1slackform}) w.r.t. all constraints in $\mathcal{W}$
    \STATE Query the oracle at point $(\mathbf{w}, \xi)$:~
           $\hat{\mathbf{y}}_i = \argmax_{\bar{\mathbf{y}} \in \mathcal{Y}} \{ \Delta(\mathbf{y}_i, \bar{\mathbf{y}}) + 
           \mathbf{w}^\top \Psi(\mathbf{x}_i, \bar{\mathbf{y}}) \},~ \forall i$
    \STATE Update constraints:~
           $\mathcal{W} = \mathcal{W} \cup \{ (\hat{\mathbf{y}}_1, \dots, \hat{\mathbf{y}}_n) \}$
\UNTIL{$\frac{1}{n} \sum_{i=1}^n 
        \left( \mathbf{w}^\top \Psi(\mathbf{x}_i, \mathbf{y}_i) - \mathbf{w}^\top \Psi(\mathbf{x}_i, \hat{\mathbf{y}}_i) \right) + 
        \varepsilon \ge \frac{1}{n} \sum_{i=1}^n \Delta(\mathbf{y}_i, \hat{\mathbf{y}}_i) - \xi$}
\RETURN $(\mathbf{w}, \xi)$
\end{algorithmic}
\end{algorithm}



\end{document}
