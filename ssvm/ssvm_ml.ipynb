{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trajectory Recommendation - Multi-label Structured SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of contents:\n",
    "1. [Description of multi-label SSVM](#1.-Description-of-multi-label-SSVM)\n",
    "1. [Inference](#2.-Inference)\n",
    " 1. [Brute force search](#2.1-Brute-force-search)\n",
    " 1. [The list Viterbi algorithm](#2.2-The-list-Viterbi-algorithm)\n",
    "1. [Structured SVM](#3.-Structured-SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#% matplotlib inline\n",
    "\n",
    "import os, sys, time, pickle, tempfile\n",
    "import math, random, itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq as hq\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "\n",
    "from pystruct.models import StructuredModel\n",
    "from pystruct.learners import OneSlackSSVM\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import cython\n",
    "import pulp\n",
    "import cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(1234554321)\n",
    "np.random.seed(123456789)\n",
    "cvxopt.base.setseed(123456789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dat_ix``` is required in notebook ```shared.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_ix = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run notebook ```shared.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run 'shared.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_JOBS = 6         # number of parallel jobs\n",
    "USE_GUROBI = False # whether to use GUROBI as ILP solver\n",
    "ABS_SCALER = False  # feature scaling, True: MaxAbsScaler, False: MinMaxScaler #False: StandardScaler\n",
    "C_SET = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000, 3000]  # regularisation parameter\n",
    "MC_PORTION = 0.1   # the portion of data that sampled by Monte-Carlo cross-validation\n",
    "MC_NITER = 5       # number of iterations for Monte-Carlo cross-validation\n",
    "SSVM_SHARE_PARAMS = False  # share params among POIs/transitions in SSVM\n",
    "LVITERBI_MAXITER = 1e6  # maximum number of iterations in the list Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Description of multi-label SSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n-slack formulation of multi-label structured SVM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\min_{\\mathbf{w}, \\xi_{ij} \\ge 0} ~& \\frac{1}{2} \\mathbf{w}^T \\mathbf{w} + \\frac{C}{n} \\sum_{i=1}^n \\sum_{j=1}^{m_i} \\xi_{ij} \\\\\n",
    "s.t. ~& \\langle \\mathbf{w}, \\Psi(x_i, y_{ij}) \\rangle - \\langle \\mathbf{w}, \\Psi(x_i, \\bar{y} \\rangle \\ge \n",
    "\\Delta(y_{ij}, \\bar{y}) - \\xi_{ij},~ \\bar{y} \\in \\mathcal{Y}_i,~ j=1,\\dots,m_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where \n",
    "- $\\mathbf{w}$ is the parameter vector\n",
    "- $m_i$ is the number ground truth in training set that conform to query $x_i$\n",
    "- $\\xi_{ij}$ is the slack variable for the $j$-th ground truth of query $x_i$ \n",
    "- $\\Psi(x_i, y_{ij})$ is the joint feature (vector) related to example $x_i$ and its label $y_{ij}$\n",
    "- $\\mathcal{Y}_i = \\mathcal{Y} \\setminus \\{y_{ij}\\}_{j=1}^{m_i}$ where $\\mathcal{Y}$ is the set of all possible labels that conform to query $x_i$\n",
    "- $\\Delta(\\centerdot)$ is the loss function, here we use Hamming loss, i.e., per-variable 0-1 loss, as indicated by function [loss()](https://github.com/pystruct/pystruct/blob/master/pystruct/models/base.py) and [fit()](https://github.com/pystruct/pystruct/blob/master/pystruct/learners/one_slack_ssvm.py)\n",
    "- $n$ is the total number of training examples, $C$ is the regularisation parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference for SSVM: loss-augmented inference for cutting-plane training and inference for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples for sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M0, L0 = 5, 3\n",
    "w_u = np.array([1, 2, 3, 2, 3]).reshape((M0, 1))\n",
    "f_u = np.array([2, 1, 1, 3, 1]).reshape((M0, 1))\n",
    "w_p = np.array([1,1,1,1,3, 1,1,1,2,1, 1,3,1,1,1, 2,1,1,1,1, 1,1,3,1,1]).reshape((M0, M0, 1))\n",
    "f_p = np.array([1,2,1,1,1, 1,1,1,1,3, 2,1,1,1,1, 1,1,3,1,1, 1,1,1,2,1]).reshape((M0, M0, 1))\n",
    "ps0, y_true0 = 1, [1, 0, 2]\n",
    "y_true_list0 = [[1, 0, 2], [1, 3, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M0, L0 = 6, 4\n",
    "w_u = np.array([1, 1, 1, 2, 1, 2]).reshape((M0, 1))\n",
    "f_u = np.array([2, 1, 1, 2, 1, 1]).reshape((M0, 1))\n",
    "w_p = np.array([1,1,1,1,3,2, 1,1,1,2,1,1, 1,3,1,1,1,2, 2,1,1,1,1,1, 1,1,3,1,1,1, 1,2,1,1,2,1]).reshape((M0, M0, 1))\n",
    "f_p = np.array([1,2,1,1,1,1, 1,1,1,1,3,2, 2,1,1,1,1,2, 1,1,3,1,1,1, 1,1,1,2,1,1, 2,1,1,2,1,1]).reshape((M0, M0, 1))\n",
    "ps0, y_true0 = 1, [1, 2, 0, 5]\n",
    "y_true_list0 = [[1, 2, 0, 5], [1, 3, 2, 5], [1, 3, 2, 0], [1, 4, 3, 0], [1, 4, 3, 2], [1, 5, 3, 0], [1, 5, 3, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Brute force search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference using **brute force search** (for sanity check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_inference_bruteForce(ps, L, M, unary_params, pw_params, unary_features, pw_features, \n",
    "                            y_true=None, y_true_list=None, debug=False):\n",
    "    assert(L > 1)\n",
    "    assert(L <= M)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < M)\n",
    "    if y_true is not None: assert(y_true_list is not None and type(y_true_list) == list)\n",
    "    \n",
    "    Cu = np.zeros(M, dtype=np.float)      # unary_param[p] x unary_features[p]\n",
    "    Cp = np.zeros((M, M), dtype=np.float) # pw_param[pi, pj] x pw_features[pi, pj]\n",
    "    # a intermediate POI should NOT be the start POI, NO self-loops\n",
    "    for pi in range(M):\n",
    "        Cu[pi] = np.dot(unary_params[pi, :], unary_features[pi, :]) # if pi != ps else -np.inf\n",
    "        for pj in range(M):\n",
    "            Cp[pi, pj] = -np.inf if (pj == ps or pi == pj) else np.dot(pw_params[pi, pj, :], pw_features[pi, pj, :])\n",
    "    \n",
    "    max_score = 0\n",
    "    y_best = None\n",
    "    for x in itertools.permutations([p for p in range(M) if p != ps], int(L-1)):\n",
    "        y = [ps] + list(x)\n",
    "        score = 0\n",
    "        \n",
    "        if y_true is not None and np.any([np.all(np.array(y) == np.asarray(yj)) for yj in y_true_list]) == True: continue\n",
    "        \n",
    "        for j in range(1, L): score += Cp[y[j-1], y[j]] + Cu[y[j]]\n",
    "        if y_true is not None: score += np.sum(np.asarray(y) != np.asarray(y_true))\n",
    "        \n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            y_best = y\n",
    "    if debug == True: print(max_score)\n",
    "    return y_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "do_inference_bruteForce(ps0, L0, M0, w_u, w_p, f_u, f_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "do_inference_bruteForce(ps0, L0, M0, w_u, w_p, f_u, f_p, y_true=y_true0, y_true_list=y_true_list0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 The list Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference using **the List Viterbi algorithm**, which *sequentially* find the (k+1)-th best path/walk given the 1st, 2nd, ..., k-th best paths/walks.\n",
    "\n",
    "Reference papers:\n",
    "- [*Sequentially finding the N-Best List in Hidden Markov Models*](http://www.eng.biu.ac.il/~goldbej/papers/ijcai01.pdf), Dennis Nilsson and Jacob Goldberger, IJCAI 2001.\n",
    "- [*A tutorial on hidden Markov models and selected applications in speech recognition*](http://www.cs.ubc.ca/~murphyk/Bayes/rabiner.pdf), L.R. Rabiner, Proceedings of the IEEE, 1989.\n",
    "\n",
    "Implementation is adapted from the above references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HeapItem:  # an item in heapq (min-heap)\n",
    "    def __init__(self, priority, task):\n",
    "        self.priority = priority\n",
    "        self.task = task\n",
    "        self.string = str(priority) + ': ' + str(task)\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        return self.priority < other.priority\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.string\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_inference_listViterbi(ps, L, M, unary_params, pw_params, unary_features, pw_features, \n",
    "                             y_true=None, y_true_list=None, debug=False):\n",
    "    assert(L > 1)\n",
    "    assert(M >= L)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < M)\n",
    "    if y_true is not None: assert(y_true_list is not None and type(y_true_list) == list)\n",
    "    \n",
    "    Cu = np.zeros(M, dtype=np.float)      # unary_param[p] x unary_features[p]\n",
    "    Cp = np.zeros((M, M), dtype=np.float) # pw_param[pi, pj] x pw_features[pi, pj]\n",
    "    \n",
    "    # a intermediate POI should NOT be the start POI, NO self-loops\n",
    "    for pi in range(M):\n",
    "        Cu[pi] = np.dot(unary_params[pi, :], unary_features[pi, :]) # if pi != ps else -np.inf\n",
    "        for pj in range(M):\n",
    "            Cp[pi, pj] = -np.inf if (pj == ps or pi == pj) else np.dot(pw_params[pi, pj, :], pw_features[pi, pj, :])\n",
    "            \n",
    "    # forward-backward procedure: adapted from the Rabiner paper\n",
    "    Alpha = np.zeros((L, M), dtype=np.float)  # alpha_t(p_i)\n",
    "    Beta  = np.zeros((L, M), dtype=np.float)  # beta_t(p_i)\n",
    "    \n",
    "    for pj in range(M): Alpha[1, pj] = Cp[ps, pj] + Cu[pj] + (0 if y_true is None else float(pj != y_true[1]))\n",
    "    for t in range(2, L):\n",
    "        for pj in range(M): # ps~~pi--pj\n",
    "            loss = 0 if y_true is None else float(pj != y_true[t])  # pi varies, pj fixed\n",
    "            Alpha[t, pj] = loss + np.max([Alpha[t-1, pi] + Cp[pi, pj] + Cu[pj] for pi in range(M)])\n",
    "    \n",
    "    for pi in range(M): Beta[L-1, pi] = 0 if y_true is None else float(pi != y_true[L-1])\n",
    "    for t in range(L-1, 1, -1):\n",
    "        for pi in range(M): # ps~~pi--pj\n",
    "            loss = 0 if y_true is None else float(pi != y_true[t-1])  # pi fixed, pj varies\n",
    "            Beta[t-1, pi] = loss + np.max([Cp[pi, pj] + Cu[pj] + Beta[t, pj] for pj in range(M)])\n",
    "    Beta[0, ps] = np.max([Cp[ps, pj] + Cu[pj] + Beta[1, pj] for pj in range(M)])\n",
    "    \n",
    "    Fp = np.zeros((L-1, M, M), dtype=np.float)  # f_{t, t+1}(p, p')\n",
    "    \n",
    "    for t in range(L-1):\n",
    "        for pi in range(M):\n",
    "            for pj in range(M):\n",
    "                Fp[t, pi, pj] = Alpha[t, pi] + Cp[pi, pj] + Cu[pj] + Beta[t+1, pj]\n",
    "                \n",
    "    # identify the best path/walk: adapted from the IJCAI01 paper\n",
    "    y_best = np.ones(L, dtype=np.int) * (-1)\n",
    "    y_best[0] = ps\n",
    "    y_best[1] = np.argmax(Fp[0, ps, :])  # the start POI is specified\n",
    "    for t in range(2, L): \n",
    "        y_best[t] = np.argmax(Fp[t-1, y_best[t-1], :])\n",
    "        \n",
    "    Q = []  # priority queue (min-heap)\n",
    "    maxIter = LVITERBI_MAXITER\n",
    "    with np.errstate(invalid='raise'):  # deal with overflow\n",
    "        try: maxIter = np.power(M, L-1) - np.prod([M-kx for kx in range(1,L)]) + 1 + \\\n",
    "                       (0 if y_true is None else len(y_true_list))\n",
    "        except: maxIter = LVITERBI_MAXITER\n",
    "    if debug == True: maxIter = np.min([maxIter, 200]); print('#iterations:', maxIter)\n",
    "    else: maxIter = np.min([maxIter, LVITERBI_MAXITER])\n",
    "        \n",
    "    # heap item for the best path/walk\n",
    "    priority, partition_index, exclude_set = -np.max(Alpha[L-1, :]), None, set()  # -1 * score as priority\n",
    "    hq.heappush(Q, HeapItem(priority, (y_best, partition_index, exclude_set)))\n",
    "    \n",
    "    if debug == True: histories = set()\n",
    "        \n",
    "    k = 0; y_last = None\n",
    "    while len(Q) > 0 and k < maxIter:\n",
    "        #print('------------------\\n', Q, '\\n------------------')\n",
    "        hitem = hq.heappop(Q)\n",
    "        k_priority, (k_best, k_partition_index, k_exclude_set) = hitem.priority, hitem.task\n",
    "        k += 1; y_last = k_best\n",
    "        \n",
    "        if debug == True: \n",
    "            histories.add(''.join([str(x) + ',' for x in k_best]))\n",
    "            #print(k, len(histories))\n",
    "            #print('pop:', k_priority, k_best, k_partition_index, k_exclude_set)\n",
    "            print(k_best, -k_priority)\n",
    "        else:\n",
    "            if len(set(k_best)) == L:\n",
    "                if y_true is None:\n",
    "                    if debug == True: print(-k_priority)\n",
    "                    return k_best\n",
    "                else: # return k_best if it is NOT one of the ground truth labels\n",
    "                    if not np.any([np.all(np.asarray(k_best) == np.asarray(yj)) for yj in y_true_list]):\n",
    "                        if debug == True: print(-k_priority)\n",
    "                        return k_best\n",
    "            \n",
    "        \n",
    "        # identify the (k+1)-th best path/walk given the 1st, 2nd, ..., k-th best: adapted from the IJCAI01 paper\n",
    "        partition_index_start = 1\n",
    "        if k_partition_index is not None:\n",
    "            assert(k_partition_index > 0)\n",
    "            assert(k_partition_index < L)\n",
    "            partition_index_start = k_partition_index\n",
    "            \n",
    "        for parix in range(partition_index_start, L):    \n",
    "            new_exclude_set = set({k_best[parix]})\n",
    "            if parix == partition_index_start:\n",
    "                new_exclude_set = new_exclude_set | k_exclude_set\n",
    "            \n",
    "            new_best = np.ones(L, dtype=np.int) * (-1)\n",
    "            for pk in range(parix):\n",
    "                new_best[pk] = k_best[pk]\n",
    "            \n",
    "            candidate_points = [p for p in range(M) if p not in new_exclude_set]\n",
    "            if len(candidate_points) == 0: continue\n",
    "            candidate_maxix = np.argmax([Fp[parix-1, k_best[parix-1], p] for p in candidate_points])\n",
    "            new_best[parix] = candidate_points[candidate_maxix]\n",
    "            \n",
    "            for pk in range(parix+1, L):\n",
    "                new_best[pk] = np.argmax([Fp[pk-1, new_best[pk-1], p] for p in range(M)])\n",
    "            \n",
    "            new_priority = Fp[parix-1, k_best[parix-1], new_best[parix]]\n",
    "            if k_partition_index is not None:\n",
    "                new_priority += (-k_priority) - Fp[parix-1, k_best[parix-1], k_best[parix]]\n",
    "            new_priority *= -1.0  # NOTE: -np.inf - np.inf + np.inf = nan\n",
    "            \n",
    "            #if debug == True and np.isnan(new_priority):\n",
    "            #    print(Fp[parix-1,k_best[parix-1],new_best[parix]], (-k_priority), \\\n",
    "            #          Fp[parix-1,k_best[parix-1],k_best[parix]])\n",
    "            #    print(Fp[parix-1,k_best[parix-1],new_best[parix]] - k_priority - \\\n",
    "            #          Fp[parix-1,k_best[parix-1],k_best[parix]])   \n",
    "            #    print(' '*3, 'push:', new_priority, new_best, parix, new_exclude_set)\n",
    "            \n",
    "            hq.heappush(Q, HeapItem(new_priority, (new_best, parix, new_exclude_set)))\n",
    "            #print('------------------\\n', Q, '\\n------------------')\n",
    "    if debug == True: print('#iterations: %d, #distinct_trajectories: %d' % (k, len(histories)))\n",
    "    if k >= maxIter: \n",
    "        sys.stderr.write('WARN: reaching max number of iterations, NO optimal solution found, return the last one.\\n')\n",
    "    if len(Q) == 0:\n",
    "        sys.stderr.write('WARN: empty queue, return the last one\\n')\n",
    "    return y_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#do_inference_listViterbi(ps0, L0, M0, w_u, w_p, f_u, f_p, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do_inference_listViterbi(ps0, L0, M0, w_u, w_p, f_u, f_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_inference_listViterbi(ps0, L0, M0, w_u, w_p, f_u, f_p, y_true=y_true0, y_true_list=y_true_list0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sanity check using random weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M0 = 90\n",
    "w_u = np.random.rand(M0).reshape(M0, 1)\n",
    "f_u = np.random.rand(M0).reshape(M0, 1)\n",
    "w_p = np.random.rand(M0*M0).reshape(M0, M0, 1)\n",
    "f_p = np.random.rand(M0*M0).reshape(M0, M0, 1)\n",
    "ps0 = np.random.choice(np.arange(M0))\n",
    "L0 = np.random.choice(np.arange(2, 6))\n",
    "indices0 = [x for x in range(M0) if x != ps0]; np.random.shuffle(indices0)\n",
    "y_true0 = [ps0] + indices0[:L0-1]\n",
    "y_true_list0 = [y_true0]\n",
    "for j in range(6):\n",
    "    np.random.shuffle(indices0); y_true_list0.append([ps0] + indices0[:L0-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print('M: %d\\nQuery: (%d, %d)' % (M0, ps0, L0))\n",
    "#print('w_u:', w_u)\n",
    "#print('f_u:', f_u)\n",
    "#print('w_p:', w_p)\n",
    "#print('f_p:', f_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#do_inference_bruteForce(ps0, L0, M0, w_u, w_p, f_u, f_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do_inference_listViterbi(ps0, L0, M0, w_u, w_p, f_u, f_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#do_inference_bruteForce(ps0, L0, M0, w_u, w_p, f_u, f_p, y_true=y_true0, y_true_list=y_true_list0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do_inference_listViterbi(ps0, L0, M0, w_u, w_p, f_u, f_p, y_true=y_true0, y_true_list=y_true_list0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Structured SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyModel(StructuredModel):\n",
    "    \n",
    "    def __init__(self, n_states=None, n_features=None, n_edge_features=None, \n",
    "                 inference_fun=do_inference_listViterbi, share_params=SSVM_SHARE_PARAMS):\n",
    "        self.inference_method = 'customized'\n",
    "        self.inference_fun = inference_fun\n",
    "        self.class_weight = None\n",
    "        self.inference_calls = 0\n",
    "        self.n_states = n_states\n",
    "        self.n_features = n_features\n",
    "        self.n_edge_features = n_edge_features\n",
    "        self.share_params = share_params\n",
    "        self._set_size_joint_feature()\n",
    "        self._set_class_weight()\n",
    "\n",
    "        \n",
    "    def _set_size_joint_feature(self):\n",
    "        if None not in [self.n_states, self.n_features, self.n_edge_features]:\n",
    "            if self.share_params == True: # share params among POIs/transitions\n",
    "                self.size_joint_feature = self.n_features + self.n_edge_features\n",
    "            else:\n",
    "                self.size_joint_feature = self.n_states * self.n_features + \\\n",
    "                                          self.n_states * self.n_states * self.n_edge_features\n",
    "   \n",
    "\n",
    "    def loss(self, y, y_hat):\n",
    "        #return np.mean(np.asarray(y) != np.asarray(y_hat))     # hamming loss (normalised)\n",
    "        return np.sum(np.asarray(y) != np.asarray(y_hat))     # hamming loss\n",
    "        #return loss_F1(y, y_hat)      # F1 loss\n",
    "        #return loss_pairsF1(y, y_hat) # pairsF1 loss\n",
    "        #return loss_pairsF1(np.array(y), np.array(y_hat)) # pairsF1 loss\n",
    "\n",
    "    \n",
    "    def initialize(self, X, Y):\n",
    "        assert(len(X) == len(Y))\n",
    "        n_features = X[0][0].shape[1]\n",
    "        if self.n_features is None: \n",
    "            self.n_features = n_features\n",
    "        else:\n",
    "            assert(self.n_features == n_featurees)\n",
    "\n",
    "        n_states = len(np.unique(np.hstack([y.ravel() for y in Y])))\n",
    "        if self.n_states is None: \n",
    "            self.n_states = n_states\n",
    "        else:\n",
    "            assert(self.n_states == n_states)\n",
    "            \n",
    "        n_edge_features = X[0][1].shape[2]\n",
    "        if self.n_edge_features is None:\n",
    "            self.n_edge_features = n_edge_features\n",
    "        else:\n",
    "            assert(self.n_edge_features == n_edge_features)\n",
    "            \n",
    "        self._set_size_joint_feature()\n",
    "        self._set_class_weight()\n",
    "        \n",
    "        self.traj_group_dict = dict()\n",
    "        for i in range(len(X)):\n",
    "            query = X[i][2]\n",
    "            if query in self.traj_group_dict: self.traj_group_dict[query].append(Y[i])\n",
    "            else: self.traj_group_dict[query] = [Y[i]]\n",
    "        \n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\"%s(n_states: %d, inference_method: %s, n_features: %d, n_edge_features: %d)\"\n",
    "                % (type(self).__name__, self.n_states, self.inference_method, self.n_features, self.n_edge_features))\n",
    "    \n",
    "    \n",
    "    def joint_feature(self, x, y):\n",
    "        assert(not isinstance(y, tuple))\n",
    "        unary_features = x[0] # unary features of all POIs: n_POIs x n_features\n",
    "        pw_features = x[1]    # pairwise features of all transitions: n_POIs x n_POIs x n_edge_features\n",
    "        query = x[2]          # query = (startPOI, length)\n",
    "        n_nodes = query[1]\n",
    "        \n",
    "        #print('y:', y)\n",
    "        \n",
    "        #assert(unary_features.ndim == 2)\n",
    "        #assert(pw_features.ndim == 3)\n",
    "        #assert(len(query) == 3)\n",
    "        assert(n_nodes == len(y))\n",
    "        assert(unary_features.shape == (self.n_states, self.n_features))\n",
    "        assert(pw_features.shape == (self.n_states, self.n_states, self.n_edge_features))\n",
    "        \n",
    "        if self.share_params == True:\n",
    "            node_features = np.zeros((self.n_features), dtype=np.float)\n",
    "            edge_features = np.zeros((self.n_edge_features), dtype=np.float)\n",
    "            node_features = unary_features[y[0], :]\n",
    "            for j in range(len(y)-1):\n",
    "                ss, tt = y[j], y[j+1]\n",
    "                node_features = node_features + unary_features[tt, :]\n",
    "                edge_features = edge_features + pw_features[ss, tt, :]\n",
    "        else: \n",
    "            node_features = np.zeros((self.n_states, self.n_features), dtype=np.float)\n",
    "            edge_features = np.zeros((self.n_states, self.n_states, self.n_edge_features), dtype=np.float)\n",
    "            node_features[y[0], :] = unary_features[y[0], :]\n",
    "            for j in range(len(y)-1):\n",
    "                ss, tt = y[j], y[j+1]\n",
    "                node_features[tt, :] = unary_features[tt, :]\n",
    "                edge_features[ss, tt, :] = pw_features[ss, tt, :]\n",
    "\n",
    "        # sum node/edge features after scaling: \n",
    "        # equivalent to share parameters between features of different POIs/transitions\n",
    "        joint_feature_vector = np.hstack([node_features.ravel(), edge_features.ravel()])\n",
    "        \n",
    "        return joint_feature_vector\n",
    "            \n",
    "    \n",
    "    def loss_augmented_inference(self, x, y, w, relaxed=None):\n",
    "        #print('loss_augmented_inference:', y)\n",
    "        # inference procedure for training: (x, y) from training set (with features already scaled)\n",
    "        #\n",
    "        # argmax_y_hat np.dot(w, joint_feature(x, y_hat)) + loss(y, y_hat)\n",
    "        # \n",
    "        # the loss function should be decomposible in order to use Viterbi decoding, here we use Hamming loss\n",
    "        #\n",
    "        # x[0]: (unscaled) unary features of all POIs: n_POIs x n_features\n",
    "        # x[1]: (unscaled) pairwise features of all transitions: n_POIs x n_POIs x n_edge_features\n",
    "        # x[2]: query = (startPOI, length)\n",
    "        unary_features = x[0]\n",
    "        pw_features = x[1]\n",
    "        query = x[2]\n",
    "        \n",
    "        assert(unary_features.ndim == 2)\n",
    "        assert(pw_features.ndim == 3)\n",
    "        assert(len(query) == 2)\n",
    "        \n",
    "        ps = query[0]\n",
    "        L = query[1]\n",
    "        M = unary_features.shape[0]  # total number of POIs\n",
    "        \n",
    "        self._check_size_w(w)\n",
    "        if self.share_params == True:\n",
    "            unary_params = w[:self.n_features]\n",
    "            pw_params = w[self.n_features:].reshape(self.n_edge_features)\n",
    "            # duplicate params so that inference procedures work the same way no matter params shared or not\n",
    "            unary_params = np.tile(unary_params, (self.n_states, 1))\n",
    "            pw_params = np.tile(pw_params, (self.n_states, self.n_states, 1))\n",
    "        else:\n",
    "            unary_params = w[:self.n_states * self.n_features].reshape((self.n_states, self.n_features))\n",
    "            pw_params = w[self.n_states * self.n_features:].reshape((self.n_states, self.n_states, self.n_edge_features))\n",
    "        \n",
    "        #y_hat = do_inference_bruteForce(ps, L, M, unary_params, pw_params, unary_features, pw_features, \n",
    "        #                                y_true=y, y_true_list=self.traj_group_dict[query])\n",
    "        y_hat = do_inference_listViterbi(ps, L, M, unary_params, pw_params, unary_features, pw_features, \n",
    "                                         y_true=y, y_true_list=self.traj_group_dict[query])\n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "    def inference(self, x, w, relaxed=False, return_energy=False):\n",
    "        #print('inference')\n",
    "        # inference procedure for testing: x from test set (features needs to be scaled)\n",
    "        #\n",
    "        # argmax_y np.dot(w, joint_feature(x, y))\n",
    "        #\n",
    "        # x[0]: (unscaled) unary features of all POIs: n_POIs x n_features\n",
    "        # x[1]: (unscaled) pairwise features of all transitions: n_POIs x n_POIs x n_edge_features\n",
    "        # x[2]: query = (startPOI, length)\n",
    "        unary_features = x[0]\n",
    "        pw_features = x[1]\n",
    "        query = x[2]\n",
    "        \n",
    "        assert(unary_features.ndim == 2)\n",
    "        assert(pw_features.ndim == 3)\n",
    "        assert(len(query) == 2)\n",
    "        \n",
    "        ps = query[0]\n",
    "        L = query[1]\n",
    "        M = unary_features.shape[0]  # total number of POIs\n",
    "        \n",
    "        self._check_size_w(w)\n",
    "        if self.share_params == True:\n",
    "            unary_params = w[:self.n_features]\n",
    "            pw_params = w[self.n_features:].reshape(self.n_edge_features)\n",
    "            # duplicate params so that inference procedures work the same way no matter params shared or not\n",
    "            unary_params = np.tile(unary_params, (self.n_states, 1))\n",
    "            pw_params = np.tile(pw_params, (self.n_states, self.n_states, 1))\n",
    "        else:\n",
    "            unary_params = w[:self.n_states * self.n_features].reshape((self.n_states, self.n_features))\n",
    "            pw_params = w[self.n_states * self.n_features:].reshape((self.n_states, self.n_states, self.n_edge_features))\n",
    "        \n",
    "        #y_pred = do_inference_listViterbi(ps, L, M, unary_params, pw_params, unary_features, pw_features)\n",
    "        #assert(len(y_pred) == len(set(y_pred)))\n",
    "        y_pred = self.inference_fun(ps, L, M, unary_params, pw_params, unary_features, pw_features)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute node features (singleton)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_node_features(startPOI, nPOI, poi_ix, poi_info, poi_clusters, cats, clusters):\n",
    "    \"\"\"\n",
    "    Generate feature vectors for all POIs given query (startPOI, nPOI)\n",
    "    \"\"\"\n",
    "    assert(isinstance(cats, list))\n",
    "    assert(isinstance(clusters, list))\n",
    "    \n",
    "    columns = DF_COLUMNS[3:]\n",
    "    poi_distmat = POI_DISTMAT\n",
    "    p0, trajLen = startPOI, nPOI\n",
    "    assert(p0 in poi_info.index)\n",
    "    \n",
    "    # DEBUG: use uniform node features\n",
    "    nrows = len(poi_ix)\n",
    "    ncols = len(columns) + len(cats) + len(clusters) - 2\n",
    "    #return np.ones((nrows, ncols), dtype=np.float)\n",
    "    #return np.zeros((nrows, ncols), dtype=np.float)\n",
    "    \n",
    "    poi_list = poi_ix\n",
    "    df_ = pd.DataFrame(index=poi_list, columns=columns)\n",
    "        \n",
    "    for poi in poi_list:\n",
    "        lon, lat = poi_info.loc[poi, 'poiLon'], poi_info.loc[poi, 'poiLat']\n",
    "        pop, nvisit = poi_info.loc[poi, 'popularity'], poi_info.loc[poi, 'nVisit']\n",
    "        cat, cluster = poi_info.loc[poi, 'poiCat'], poi_clusters.loc[poi, 'clusterID']\n",
    "        duration = poi_info.loc[poi, 'avgDuration']\n",
    "        idx = poi\n",
    "        df_.set_value(idx, 'category', tuple((cat == np.array(cats)).astype(np.int) * 2 - 1))\n",
    "        df_.set_value(idx, 'neighbourhood', tuple((cluster == np.array(clusters)).astype(np.int) * 2 - 1))\n",
    "        df_.loc[idx, 'popularity'] = LOG_SMALL if pop < 1 else np.log10(pop)\n",
    "        df_.loc[idx, 'nVisit'] = LOG_SMALL if nvisit < 1 else np.log10(nvisit)\n",
    "        df_.loc[idx, 'avgDuration'] = LOG_SMALL if duration < 1 else np.log10(duration)\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_all.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi, p0]\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'sameNeighbourhoodStart'] = 1 if cluster == poi_clusters.loc[p0, 'clusterID'] else -1\n",
    "        \n",
    "    # features other than category and neighbourhood\n",
    "    feature_name = ['popularity', 'nVisit', 'avgDuration', 'trajLen', 'sameCatStart', 'distStart', \n",
    "                    'diffPopStart', 'diffNVisitStart', 'diffDurationStart', 'sameNeighbourhoodStart']\n",
    "    #X = df_[sorted(set(df_.columns) - {'category', 'neighbourhood'})].values\n",
    "    X = df_[feature_name].values\n",
    "    \n",
    "    # boolean features: category (+1, -1)\n",
    "    cat_features = np.vstack([list(df_.loc[x, 'category']) for x in df_.index])\n",
    "    \n",
    "    # boolean features: neighbourhood (+1, -1)\n",
    "    neigh_features = np.vstack([list(df_.loc[x, 'neighbourhood']) for x in df_.index])\n",
    "    \n",
    "    return np.hstack([cat_features, neigh_features, X]).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute edge features (transiton / pairwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_edge_features(trajid_list, poi_ix, traj_dict, poi_info):    \n",
    "    feature_names = ['poiCat', 'popularity', 'nVisit', 'avgDuration', 'clusterID']\n",
    "    n_features = len(feature_names)\n",
    "    \n",
    "    # DEBUG: use uniform edge features\n",
    "    #return np.ones((len(poi_ix), len(poi_ix), n_features), dtype=np.float)\n",
    "    #return np.zeros((len(poi_ix), len(poi_ix), n_features), dtype=np.float)\n",
    "    \n",
    "    transmat_cat                        = gen_transmat_cat(trajid_list, traj_dict, poi_info)\n",
    "    transmat_pop,      logbins_pop      = gen_transmat_pop(trajid_list, traj_dict, poi_info)\n",
    "    transmat_visit,    logbins_visit    = gen_transmat_visit(trajid_list, traj_dict, poi_info)\n",
    "    transmat_duration, logbins_duration = gen_transmat_duration(trajid_list, traj_dict, poi_info)\n",
    "    transmat_neighbor, poi_clusters     = gen_transmat_neighbor(trajid_list, traj_dict, poi_info)\n",
    "    \n",
    "    poi_features = pd.DataFrame(data=np.zeros((len(poi_ix), len(feature_names))), \\\n",
    "                                columns=feature_names, index=poi_ix)\n",
    "    poi_features.index.name = 'poiID'\n",
    "    poi_features['poiCat'] = poi_info.loc[poi_ix, 'poiCat']\n",
    "    poi_features['popularity'] = np.digitize(poi_info.loc[poi_ix, 'popularity'], logbins_pop)\n",
    "    poi_features['nVisit'] = np.digitize(poi_info.loc[poi_ix, 'nVisit'], logbins_visit)\n",
    "    poi_features['avgDuration'] = np.digitize(poi_info.loc[poi_ix, 'avgDuration'], logbins_duration)\n",
    "    poi_features['clusterID'] = poi_clusters.loc[poi_ix, 'clusterID']\n",
    "    \n",
    "    edge_features = np.zeros((len(poi_ix), len(poi_ix), n_features), dtype=np.float64)\n",
    "    \n",
    "    for j in range(len(poi_ix)): # NOTE: POI order\n",
    "        pj = poi_ix[j]\n",
    "        cat, pop = poi_features.loc[pj, 'poiCat'], poi_features.loc[pj, 'popularity']\n",
    "        visit, cluster = poi_features.loc[pj, 'nVisit'], poi_features.loc[pj, 'clusterID']\n",
    "        duration = poi_features.loc[pj, 'avgDuration']\n",
    "        \n",
    "        for k in range(len(poi_ix)): # NOTE: POI order\n",
    "            pk = poi_ix[k]\n",
    "            edge_features[j, k, :] = np.log10( np.array(\n",
    "            #edge_features[j, k, :] = np.array(\n",
    "                    [transmat_cat.loc[cat, poi_features.loc[pk, 'poiCat']], \\\n",
    "                     transmat_pop.loc[pop, poi_features.loc[pk, 'popularity']], \\\n",
    "                     transmat_visit.loc[visit, poi_features.loc[pk, 'nVisit']], \\\n",
    "                     transmat_duration.loc[duration, poi_features.loc[pk, 'avgDuration']], \\\n",
    "                     transmat_neighbor.loc[cluster, poi_features.loc[pk, 'clusterID']]] ) )\n",
    "    return edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SSVM:\n",
    "    def __init__(self, inference_fun=do_inference_listViterbi, C=1.0, poi_info=None, debug=False):\n",
    "        assert(C > 0)\n",
    "        self.C = C\n",
    "        self.inference_fun = inference_fun\n",
    "        self.debug = debug\n",
    "        self.trained = False\n",
    "        \n",
    "        if poi_info is None:\n",
    "            self.poi_info = None\n",
    "        else:\n",
    "            self.poi_info = poi_info\n",
    "        \n",
    "        if ABS_SCALER == True:\n",
    "            self.scaler_node = MaxAbsScaler(copy=False)\n",
    "            self.scaler_edge = MaxAbsScaler(copy=False)\n",
    "        else:\n",
    "            self.scaler_node = MinMaxScaler(feature_range=(-1,1), copy=False)\n",
    "            self.scaler_edge = MinMaxScaler(feature_range=(-1,1), copy=False)\n",
    "            #self.scaler = StandardScaler(copy=False)  \n",
    "\n",
    "        \n",
    "    def train(self, trajid_set_train):\n",
    "        if self.poi_info is None:\n",
    "            self.poi_info = calc_poi_info(list(trajid_set_train), traj_all, poi_all)\n",
    "\n",
    "        # build POI_ID <--> POI__INDEX mapping for POIs used to train CRF\n",
    "        # which means only POIs in traj such that len(traj) >= 2 are included\n",
    "        poi_set = {p for tid in trajid_set_train for p in traj_dict[tid] if len(traj_dict[tid]) >= 2}\n",
    "        #poi_set = set()\n",
    "        #for x in trajid_set_train:\n",
    "        #    if len(traj_dict[x]) >= 2:\n",
    "        #        poi_set = poi_set | set(traj_dict[x])\n",
    "        self.poi_ix = sorted(poi_set)\n",
    "        self.poi_id_dict, self.poi_id_rdict = dict(), dict()\n",
    "        for idx, poi in enumerate(self.poi_ix):\n",
    "            self.poi_id_dict[poi] = idx\n",
    "            self.poi_id_rdict[idx] = poi\n",
    "\n",
    "        # generate training data\n",
    "        train_traj_list = [traj_dict[k] for k in trajid_set_train if len(traj_dict[k]) >= 2]\n",
    "        node_features_list = Parallel(n_jobs=N_JOBS)\\\n",
    "                             (delayed(calc_node_features)\\\n",
    "                              (tr[0], len(tr), self.poi_ix, self.poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                               cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST) for tr in train_traj_list)\n",
    "        edge_features = calc_edge_features(list(trajid_set_train), self.poi_ix, traj_dict, self.poi_info)\n",
    "        #print(edge_features)\n",
    "\n",
    "        # feature scaling: node features\n",
    "        # should each example be flattened to one vector before scaling?\n",
    "        self.fdim_node = node_features_list[0].shape\n",
    "        X_node_all = np.vstack(node_features_list)\n",
    "        #print(self.fdim)\n",
    "        #print(X_node_all.shape)\n",
    "        #X_node_all = X_node_all.reshape(len(node_features_list), -1) # flatten every example to a vector\n",
    "        X_node_all = self.scaler_node.fit_transform(X_node_all)\n",
    "        X_node_all = X_node_all.reshape(-1, self.fdim_node[0], self.fdim_node[1])\n",
    "        \n",
    "        # feature scaling: edge features\n",
    "        fdim_edge = edge_features.shape\n",
    "        edge_features = self.scaler_edge.fit_transform(edge_features.reshape(fdim_edge[0]*fdim_edge[1], -1))\n",
    "        self.edge_features = edge_features.reshape(fdim_edge)\n",
    "        #print('---------------------------------------\\n\\n'); print(edge_features)\n",
    "\n",
    "        assert(len(train_traj_list) == X_node_all.shape[0])\n",
    "        X_train = [(X_node_all[k, :, :], \\\n",
    "                    self.edge_features.copy(), \\\n",
    "                    (self.poi_id_dict[train_traj_list[k][0]], len(train_traj_list[k]))) \\\n",
    "                   for k in range(len(train_traj_list))]\n",
    "        y_train = [np.array([self.poi_id_dict[k] for k in tr]) for tr in train_traj_list]\n",
    "        assert(len(X_train) == len(y_train))\n",
    "\n",
    "        # train\n",
    "        sm = MyModel(inference_fun=self.inference_fun)\n",
    "        if self.debug == True: print('C:', self.C)\n",
    "        verbose = 1 if self.debug == True else 0\n",
    "        self.osssvm = OneSlackSSVM(model=sm, C=self.C, n_jobs=N_JOBS, verbose=verbose)\n",
    "        try:\n",
    "            self.osssvm.fit(X_train, y_train, initialize=True)\n",
    "            self.trained = True \n",
    "            print('SSVM training finished.')\n",
    "        #except ValueError:\n",
    "        except:\n",
    "            self.trained = False\n",
    "            sys.stderr.write('SSVM training FAILED.\\n')\n",
    "        return self.trained\n",
    "\n",
    "\n",
    "    def predict(self, startPOI, nPOI):\n",
    "        assert(self.trained == True)\n",
    "        if startPOI not in self.poi_ix: return None\n",
    "        X_node_test = calc_node_features(startPOI, nPOI, self.poi_ix, self.poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                         cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "\n",
    "        # feature scaling\n",
    "        # should each example be flattened to one vector before scaling?\n",
    "        #X_node_test = X_node_test.reshape(1, -1) # flatten test example to a vector\n",
    "        X_node_test = self.scaler_node.transform(X_node_test)\n",
    "        #X_node_test = X_node_test.reshape(self.fdim)\n",
    "\n",
    "        X_test = [(X_node_test, self.edge_features, (self.poi_id_dict[startPOI], nPOI))]\n",
    "        y_hat = self.osssvm.predict(X_test)\n",
    "\n",
    "        return np.array([self.poi_id_rdict[x] for x in y_hat[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross-validation with Monte-Carlo cross-validation as inner loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inference_methods = [do_inference_bruteForce, do_inference_listViterbi]\n",
    "methods_suffix = ['bruteForce', 'listViterbi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method_ix = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recdict_ssvm = dict()\n",
    "cnt = 1\n",
    "keys = sorted(TRAJ_GROUP_DICT.keys())\n",
    "\n",
    "# outer loop to evaluate the test performance by cross validation\n",
    "for i in range(len(keys)):\n",
    "    ps, L = keys[i]\n",
    "    best_C = 1\n",
    "    #best_F1 = 0; best_pF1 = 0\n",
    "    best_Tau = 0\n",
    "    keys_cv = keys[:i] + keys[i+1:]\n",
    "    \n",
    "    # use all training+validation set to compute POI features, \n",
    "    # make sure features do NOT change for training and validation\n",
    "    trajid_set_i = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "    poi_info_i = calc_poi_info(list(trajid_set_i), traj_all, poi_all)\n",
    "    \n",
    "    # tune regularisation constant C\n",
    "    for ssvm_C in C_SET:\n",
    "        print('\\n--------------- try_C: %f ---------------\\n' % ssvm_C); sys.stdout.flush() \n",
    "        F1_ssvm = []; pF1_ssvm = []; Tau_ssvm = []        \n",
    "        \n",
    "        # inner loop to evaluate the performance of a model with a specified C by Monte-Carlo cross validation\n",
    "        for j in range(MC_NITER):\n",
    "            poi_list = []\n",
    "            while True: # make sure the start POI in test set are also in training set\n",
    "                rand_ix = np.arange(len(keys_cv)); np.random.shuffle(rand_ix)\n",
    "                test_ix = rand_ix[:int(MC_PORTION*len(rand_ix))]\n",
    "                assert(len(test_ix) > 0)\n",
    "                trajid_set_train = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "                for j in test_ix: \n",
    "                    trajid_set_train = trajid_set_train - TRAJ_GROUP_DICT[keys_cv[j]]\n",
    "                poi_set = set()\n",
    "                for tid in trajid_set_train: poi_set = poi_set | set(traj_dict[tid])\n",
    "                good_partition = True\n",
    "                for j in test_ix: \n",
    "                    if keys_cv[j][0] not in poi_set: good_partition = False; break\n",
    "                if good_partition == True: \n",
    "                    poi_list = sorted(poi_set)\n",
    "                    break\n",
    "\n",
    "            # train\n",
    "            ssvm = SSVM(inference_fun=inference_methods[method_ix], C=ssvm_C, poi_info=poi_info_i.loc[poi_list].copy())\n",
    "            if ssvm.train(trajid_set_train) == True:            \n",
    "                for j in test_ix: # test\n",
    "                    ps_cv, L_cv = keys_cv[j]\n",
    "                    y_hat = ssvm.predict(ps_cv, L_cv)\n",
    "                    if y_hat is not None:\n",
    "                        F1, pF1, tau = evaluate(y_hat, TRAJ_GROUP_DICT[keys_cv[j]])\n",
    "                        F1_ssvm.append(F1); pF1_ssvm.append(pF1); Tau_ssvm.append(tau)\n",
    "            else: \n",
    "                for j in test_ix:\n",
    "                    F1_ssvm.append(0); pF1_ssvm.append(0); Tau_ssvm.append(0)\n",
    "        \n",
    "        #mean_F1 = np.mean(F1_ssvm); mean_pF1 = np.mean(pF1_ssvm)\n",
    "        mean_Tau = np.mean(Tau_ssvm)\n",
    "        print('mean_Tau: %.3f' % mean_Tau)\n",
    "        if mean_Tau > best_Tau:\n",
    "            best_Tau = mean_Tau\n",
    "            best_C = ssvm_C\n",
    "    print('\\n--------------- %d/%d, Query: (%d, %d), Best_C: %f ---------------\\n' % (cnt, len(keys), ps, L, best_C))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # train model using all examples in training set and measure performance on test set\n",
    "    ssvm = SSVM(inference_fun=inference_methods[method_ix], C=best_C, poi_info=poi_info_i)#, debug=True)\n",
    "    if ssvm.train(trajid_set_i) == True:\n",
    "        y_hat = ssvm.predict(ps, L)\n",
    "        print(cnt, y_hat)\n",
    "        if y_hat is not None:\n",
    "            recdict_ssvm[(ps, L)] = {'PRED': y_hat, 'W': ssvm.osssvm.w, 'C': ssvm.C}\n",
    "        \n",
    "    cnt += 1; #print_progress(cnt, len(keys)); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F1_ssvm = []; pF1_ssvm = []; tau_ssvm = []\n",
    "for key in sorted(recdict_ssvm.keys()):\n",
    "    F1, pF1, tau = evaluate(recdict_ssvm[key]['PRED'], TRAJ_GROUP_DICT[key])\n",
    "    F1_ssvm.append(F1); pF1_ssvm.append(pF1); tau_ssvm.append(tau)\n",
    "print('SSVM: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "      (np.mean(F1_ssvm), np.std(F1_ssvm)/np.sqrt(len(F1_ssvm)), \\\n",
    "       np.mean(pF1_ssvm), np.std(pF1_ssvm)/np.sqrt(len(pF1_ssvm)), \\\n",
    "       np.mean(tau_ssvm), np.std(tau_ssvm)/np.sqrt(len(tau_ssvm))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fssvm = os.path.join(data_dir, 'ssvm-' + methods_suffix[method_ix] + '-' + dat_suffix[dat_ix] + '.pkl')\n",
    "pickle.dump(recdict_ssvm, open(fssvm, 'bw'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
