{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on Generated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to design an experiment to check if multi-user and multi-label (which is what our dataset looks like) is a problem for SSVM.   \n",
    "To chieve this goal,\n",
    "1. a trained SSVM $\\mathcal{M}_0$ (on Glasgow dataset $\\mathcal{D}_0$) is used to generate a single user, single label dataset $\\mathcal{D}_1$. Concretely, we predict a trajectory for every query $(p, l), p \\in \\mathcal{P}, l \\in \\{3,4,5,6,7\\}$ use $\\mathcal{M}_0$, where $\\mathcal{P}$ is from $\\mathcal{D}_0$.\n",
    "1. train a new SSVM $\\mathcal{M}_1$ using features (POI and transition features) computed from $\\mathcal{D}_0$ and labels from $\\mathcal{D}_1$, and check the performance on training set (i.e., $\\mathcal{D}_1$).\n",
    "1. perform leave-one-out cross validation on $\\mathcal{D}_1$. Hyperparameter (i.e., $C$) is determined by trying some numbers when holding one label in $\\mathcal{D}_1$ as test example and using all other labels in $\\mathcal{D}_1$ as training set (POI and transition features are computed from $\\mathcal{D}_0$), then fix the $C$ for all leave-one-out cross validations.\n",
    "1. we noted that POI and transition features are computed from $\\mathcal{D}_0$ and labels are from $\\mathcal{D}_1$, as we can't compute the duration related features (i.e., avgDuration for POI, and log transition probability between discretized duration buckets) on $\\mathcal{D}_1$ as no duration information is generated.\n",
    "1. we try to disable duration related features one-by-one, and perform step $3$ to check whether duration related features help.\n",
    "1. if duration related features don't help, we can turn off them and then compute POI and transition features from $\\mathcal{D}_1$ and use labels in $\\mathcal{D}_1$, then we want to compare the performance of RankSVM and SSVM on $\\mathcal{D}_1$ (using leave-one-out cross validation), if SSVM performans better than RankSVM, it means multi-user and multi-label in our dataset is a problem for SSVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, pickle, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(1234554321)\n",
    "np.random.seed(123456789)\n",
    "cvxopt.base.setseed(123456789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run notebook ```ssvm.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run 'ssvm.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load trained parameters and prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump_variables = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = os.path.join(data_dir, 'ssvm-listViterbi-Glas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssvm_lv = pickle.load(open(fname, 'rb'))  # a dict: query -> {'PRED': trajectory, 'C': ssvm-c, 'W': model_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = (1, 3)\n",
    "W = ssvm_lv[query]['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Wname = 'W.pkl'\n",
    "if dump_variables == True: pickle.dump(W, open(Wname, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W0 = pickle.load(open(Wname, 'rb'))\n",
    "assert(np.allclose(W, W0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trajid_set = set(trajid_set_all) - TRAJ_GROUP_DICT[query]\n",
    "poi_set = set()\n",
    "for tid in trajid_set: \n",
    "    if len(traj_dict[tid]) >= 2:\n",
    "        poi_set = poi_set | set(traj_dict[tid])\n",
    "poi_list = sorted(poi_set)\n",
    "n_states = len(poi_set)\n",
    "n_edge_features = 5\n",
    "n_node_features = (len(W) - n_states * n_states * n_edge_features) // n_states\n",
    "#print(len(W), n_states, n_node_features)\n",
    "unary_params = W[:n_states * n_node_features].reshape(n_states, n_node_features)\n",
    "pw_params = W[n_states * n_node_features:].reshape((n_states, n_states, n_edge_features))    \n",
    "\n",
    "poi_id_dict, poi_id_rdict = dict(), dict()\n",
    "for idx, poi in enumerate(poi_list):\n",
    "    poi_id_dict[poi] = idx\n",
    "    poi_id_rdict[idx] = poi\n",
    "    \n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "many = 'many.pkl'\n",
    "if dump_variables == True: \n",
    "    pickle.dump([sorted(trajid_set), sorted(poi_set), poi_list, poi_id_dict, poi_id_rdict, unary_params, pw_params],\n",
    "                open(many, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the results of previous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[trajid_set0, poi_set0, poi_list0, poi_id_dict0, poi_id_rdict0, unary_params0, pw_params0]=pickle.load(open(many, 'rb'))\n",
    "assert(np.all(np.array(sorted(trajid_set)) == np.array(trajid_set0)))\n",
    "assert(np.all(np.array(sorted(poi_set)) == np.array(poi_set0)))\n",
    "assert(np.all(np.array(poi_list) == np.array(poi_list0)))\n",
    "assert(np.all(np.array(sorted(poi_id_dict.keys())) == np.array(sorted(poi_id_dict0.keys()))))\n",
    "poi_id_v = [poi_id_dict[key] for key in sorted(poi_id_dict.keys())]\n",
    "poi_id_v0 = [poi_id_dict0[key] for key in sorted(poi_id_dict0.keys())]\n",
    "assert(np.all(np.array(poi_id_v) == np.array(poi_id_v0)))\n",
    "assert(np.all(np.array(sorted(poi_id_rdict.keys())) == np.array(sorted(poi_id_rdict0.keys()))))\n",
    "poi_id_rv = [poi_id_rdict[key] for key in sorted(poi_id_rdict.keys())]\n",
    "poi_id_rv0 = [poi_id_rdict0[key] for key in sorted(poi_id_rdict0.keys())]\n",
    "assert(np.all(np.array(poi_id_rv) == np.array(poi_id_rv0)))\n",
    "assert(np.allclose(unary_params, unary_params0))\n",
    "assert(np.allclose(pw_params, pw_params0))\n",
    "\n",
    "print('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute feature scaling parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_info = calc_poi_info(sorted(trajid_set), traj_all, poi_all)\n",
    "\n",
    "traj_list = [traj_dict[k] for k in sorted(trajid_set) if len(traj_dict[k]) >= 2]\n",
    "node_features_list = Parallel(n_jobs=N_JOBS)\\\n",
    "                     (delayed(calc_node_features)\\\n",
    "                      (tr[0], len(tr), poi_list, poi_info.copy(), poi_clusters=POI_CLUSTERS, \\\n",
    "                       cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST) for tr in traj_list)\n",
    "edge_features = calc_edge_features(list(trajid_set), poi_list, traj_dict, poi_info.copy())\n",
    "fdim = node_features_list[0].shape\n",
    "X_node_all = np.vstack(node_features_list)\n",
    "scaler = MaxAbsScaler(copy=False)\n",
    "scaler.fit(X_node_all)\n",
    "\n",
    "# turn off duration\n",
    "#poi_info['avgDuration'] = 0.0\n",
    "edge_features[:, :, 3] = LOG_ZERO\n",
    "\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "node_features0 = calc_node_features(traj_list[0][0], len(traj_list[0]), poi_list, poi_info.copy(), \n",
    "                                    poi_clusters=POI_CLUSTERS, cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "manymore = 'manymore.pkl'\n",
    "if dump_variables == True: \n",
    "    pickle.dump([node_features0, traj_list[0], \n",
    "                 POI_CLUSTERS, POI_CAT_LIST, POI_CLUSTER_LIST, poi_info, traj_list, node_features_list,\n",
    "                 X_node_all, edge_features], open(manymore, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the results of previous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[nf0, tr0, POI_CLUSTERS0, POI_CAT_LIST0, POI_CLUSTER_LIST0, poi_info0, traj_list0, node_features_list0,\n",
    " X_node_all0, edge_features0] = pickle.load(open(manymore, 'rb'))\n",
    "assert(np.all(np.array(traj_list[0]) == np.array(tr0)))\n",
    "assert(POI_CLUSTERS.equals(POI_CLUSTERS0))\n",
    "assert(pd.Series(POI_CAT_LIST).equals(pd.Series(POI_CAT_LIST0)))\n",
    "assert(np.all(np.array(POI_CLUSTER_LIST) == np.array(POI_CLUSTER_LIST0)))\n",
    "assert(poi_info.equals(poi_info0))\n",
    "assert(np.allclose(node_features0, nf0))\n",
    "assert(len(traj_list) == len(traj_list0))\n",
    "for t in range(len(traj_list)): \n",
    "    assert(np.all(np.array(traj_list[t]) == np.array(traj_list0[t])))\n",
    "assert(len(node_features_list) == len(node_features_list0))\n",
    "for t in range(len(node_features_list)):\n",
    "    assert(np.allclose(node_features_list[t], node_features_list0[t]))\n",
    "assert(np.allclose(X_node_all, X_node_all0))\n",
    "assert(np.allclose(edge_features, edge_features0))\n",
    "\n",
    "print('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lengthes = [3, 4, 5]#, 6, 7]\n",
    "fake_labels = []\n",
    "for poi in sorted(poi_list):\n",
    "    for L in lengthes:\n",
    "        X_node_test = calc_node_features(poi, L, poi_list, poi_info.copy(), poi_clusters=POI_CLUSTERS, \\\n",
    "                                         cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "        X_node_test = scaler.transform(X_node_test)  # feature scaling\n",
    "        unary_features = X_node_test\n",
    "        pw_features = edge_features.copy()\n",
    "        y_pred = do_inference_listViterbi(poi_id_dict[poi], L, len(poi_set), \n",
    "                                          unary_params, pw_params, unary_features, pw_features)\n",
    "        fake_labels.append([poi_id_rdict[p] for p in y_pred])\n",
    "\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fakename = 'fake_labels.pkl'\n",
    "if dump_variables == True: pickle.dump(fake_labels, open(fakename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the results of previous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fake_labels0 = pickle.load(open(fakename, 'rb'))\n",
    "assert(len(fake_labels) == len(fake_labels0))\n",
    "for l in range(len(fake_labels)):\n",
    "    fl0 = fake_labels0[l]\n",
    "    fl1 = fake_labels[l]\n",
    "    assert(len(fl0) == len(fl1))\n",
    "    assert(np.all(np.array(fl0) == np.array(fl1)))\n",
    "    \n",
    "print('True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing scaling on the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels = fake_labels.copy()\n",
    "node_features_all = Parallel(n_jobs=N_JOBS)\\\n",
    "                    (delayed(calc_node_features)\\\n",
    "                     (tr[0], len(tr), poi_list, poi_info.copy(), poi_clusters=POI_CLUSTERS, \\\n",
    "                      cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST) for tr in train_labels)\n",
    "fdim_train = node_features_all[0].shape\n",
    "X_node_train = np.vstack(node_features_all)\n",
    "scaler_train = MaxAbsScaler(copy=False)\n",
    "X_node_train = scaler_train.fit_transform(X_node_train)\n",
    "X_node_train = X_node_train.reshape(-1, fdim_train[0], fdim_train[1])\n",
    "assert(len(train_labels) == X_node_train.shape[0])\n",
    "X_train = [(X_node_train[k, :, :], edge_features.copy(), \n",
    "            (poi_id_dict[train_labels[k][0]], len(train_labels[k]))) for k in range(len(train_labels))]\n",
    "y_train = [np.array([poi_id_dict[k] for k in tr]) for tr in train_labels]\n",
    "assert(len(X_train) == len(y_train))\n",
    "\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = 0.001\n",
    "\n",
    "sm = MyModel(inference_fun=do_inference_listViterbi)\n",
    "osssvm = OneSlackSSVM(model=sm, C=C, n_jobs=N_JOBS, verbose=0)\n",
    "try:\n",
    "    osssvm.fit(X_train, y_train, initialize=True)\n",
    "    print('SSVM training finished.')\n",
    "except:\n",
    "    sys.stderr.write('SSVM training FAILED.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(osssvm.objective_curve_, label='dual')\n",
    "plt.plot(osssvm.primal_objective_curve_, label='primal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction on generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = dict()\n",
    "for i in range(len(fake_labels)):\n",
    "    ps_cv, L_cv = fake_labels[i][0], len(fake_labels[i])\n",
    "    X_node_test = calc_node_features(ps_cv, L_cv, poi_list, poi_info.copy(), poi_clusters=POI_CLUSTERS, \n",
    "                                     cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "    X_node_test = scaler_train.transform(X_node_test)\n",
    "    X_test = [(X_node_test, edge_features, (poi_id_dict[ps_cv], L_cv))]\n",
    "    y_hat = osssvm.predict(X_test)\n",
    "    predictions[(ps_cv, L_cv)] = {'PRED': np.array([poi_id_rdict[p] for p in y_hat[0]]), 'REAL':fake_labels[i]}\n",
    "    #print(fake_labels[i], '->', predictions[(ps_cv, L_cv)]['PRED'].tolist())\n",
    "\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F1_ssvm = []; pF1_ssvm = []; tau_ssvm = []\n",
    "for key in sorted(predictions.keys()):\n",
    "    F1 = calc_F1(predictions[key]['REAL'], predictions[key]['PRED'])\n",
    "    pF1 = calc_pairsF1(predictions[key]['REAL'], predictions[key]['PRED'])\n",
    "    tau = calc_kendalltau(predictions[key]['REAL'], predictions[key]['PRED'])\n",
    "    F1_ssvm.append(F1); pF1_ssvm.append(pF1); tau_ssvm.append(tau)\n",
    "print('SSVM: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "      (np.mean(F1_ssvm), np.std(F1_ssvm)/np.sqrt(len(F1_ssvm)), \\\n",
    "       np.mean(pF1_ssvm), np.std(pF1_ssvm)/np.sqrt(len(pF1_ssvm)), \\\n",
    "       np.mean(tau_ssvm), np.std(tau_ssvm)/np.sqrt(len(tau_ssvm))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
