{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trajectory Recommendation - MEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import os, sys, time, pickle\n",
    "import math, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq as hq\n",
    "from scipy.optimize import minimize\n",
    "from scipy.misc import logsumexp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dat_ix``` is required in notebook ```shared.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_ix = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run notebook ```shared.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run 'shared.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_JOBS = 6         # number of parallel jobs\n",
    "MIN_MAX_SCALE = True  # feature scaling, True: MinMaxScaler, False: StandardScaler\n",
    "LOGIT_C = 1\n",
    "run_memm1m = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference using **the List Viterbi algorithm**, which *sequentially* find the (k+1)-th best path/walk given the 1st, 2nd, ..., k-th best paths/walks.\n",
    "\n",
    "Reference papers:\n",
    "- [*Sequentially finding the N-Best List in Hidden Markov Models*](http://www.eng.biu.ac.il/~goldbej/papers/ijcai01.pdf), Dennis Nilsson and Jacob Goldberger, IJCAI 2001.\n",
    "- [*A tutorial on hidden Markov models and selected applications in speech recognition*](http://www.cs.ubc.ca/~murphyk/Bayes/rabiner.pdf), L.R. Rabiner, Proceedings of the IEEE, 1989.\n",
    "\n",
    "Implementation is adapted from the above references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HeapItem:  # an item in heapq (min-heap)\n",
    "    def __init__(self, priority, task):\n",
    "        self.priority = priority\n",
    "        self.task = task\n",
    "        self.string = str(priority) + ': ' + str(task)\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        return self.priority < other.priority\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.string\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_inference_listViterbi(ps, L, N, unary_params, pw_params, unary_features, pw_features, y_true=None, debug=False):\n",
    "    assert(L > 1)\n",
    "    assert(N >= L)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < N)\n",
    "    \n",
    "    Cu = np.zeros(N, dtype=np.float)      # unary_param[p] x unary_features[p]\n",
    "    Cp = np.zeros((N, N), dtype=np.float) # pw_param[pi, pj] x pw_features[pi, pj]\n",
    "    \n",
    "    # a intermediate POI should NOT be the start POI, NO self-loops\n",
    "    for pi in range(N):\n",
    "        Cu[pi] = np.dot(unary_params[pi, :], unary_features[pi, :]) # if pi != ps else -np.inf\n",
    "        for pj in range(N):\n",
    "            Cp[pi, pj] = -np.inf if (pj == ps or pi == pj) else np.dot(pw_params[pi, pj, :], pw_features[pi, pj, :])\n",
    "            \n",
    "    # forward-backward procedure: adapted from the Rabiner paper\n",
    "    Alpha = np.zeros((L, N), dtype=np.float)  # alpha_t(p_i)\n",
    "    Beta  = np.zeros((L, N), dtype=np.float)  # beta_t(p_i)\n",
    "    for t in range(1, L):\n",
    "        for pj in range(N):\n",
    "            Alpha[t, pj] = np.max([Alpha[t-1, pi] + Cp[pi, pj] + Cu[pj] for pi in range(N)])\n",
    "        t1 = L-t\n",
    "        for pi in range(N):\n",
    "            Beta[t1-1, pi] = np.max([Cp[pi, pj] + Cu[pj] + Beta[t1, pj] for pj in range(N)])\n",
    "            \n",
    "    Fu = np.zeros((L, N), dtype=np.float)       # f_t(p)\n",
    "    Fp = np.zeros((L-1, N, N), dtype=np.float)  # f_{t, t+1}(p, p')\n",
    "    for t in range(L):\n",
    "        for p in range(N):\n",
    "            Fu[t, p] = Alpha[t, p] + Beta[t, p]\n",
    "    for t in range(L-1):\n",
    "        for pi in range(N):\n",
    "            for pj in range(N):\n",
    "                Fp[t, pi, pj] = Alpha[t, pi] + Cp[pi, pj] + Cu[pj] + Beta[t+1, pj]\n",
    "                \n",
    "    # identify the best path/walk: adapted from the IJCAI01 paper\n",
    "    y_best = np.ones(L, dtype=np.int) * (-1)\n",
    "    y_best[0] = ps\n",
    "    maxix = np.argmax(Fp[0, ps, :])  # the start POI is specified\n",
    "    y_best[1] = maxix % N\n",
    "    for t in range(2, L): \n",
    "        y_best[t] = np.argmax(Fp[t-1, y_best[t-1], :])\n",
    "        \n",
    "    Q = []  # priority queue (min-heap)\n",
    "    maxIter = np.power(N,L-1) - np.prod([N-kx for kx in range(1,L)]) + 1 #gauranteed to find a path in maxIter iterations\n",
    "    if debug == True: maxIter = np.min([maxIter, 200]); print('#iterations:', maxIter) \n",
    "        \n",
    "    # heap item for the best path/walk\n",
    "    priority, partition_index, exclude_set = -np.max(Fu[L-1, :]), None, set()  # -1 * score as priority\n",
    "    hq.heappush(Q, HeapItem(priority, (y_best, partition_index, exclude_set)))\n",
    "    \n",
    "    histories = set()\n",
    "        \n",
    "    k = 0\n",
    "    while len(Q) > 0 and k < maxIter:\n",
    "        #print('------------------\\n', Q, '\\n------------------')\n",
    "        hitem = hq.heappop(Q)\n",
    "        k_priority, (k_best, k_partition_index, k_exclude_set) = hitem.priority, hitem.task\n",
    "        k += 1\n",
    "        \n",
    "        histories.add(''.join([str(x) + ',' for x in k_best]))\n",
    "        #print(k, len(histories))\n",
    "        \n",
    "        #print('pop:', k_priority, k_best, k_partition_index, k_exclude_set)\n",
    "        if debug == True: \n",
    "            print(k_best, -k_priority)\n",
    "        else:\n",
    "            if len(set(k_best)) == L: return k_best\n",
    "            \n",
    "        \n",
    "        # identify the (k+1)-th best path/walk given the 1st, 2nd, ..., k-th best: adapted from the IJCAI01 paper\n",
    "        partition_index_start = 1\n",
    "        if k_partition_index is not None:\n",
    "            assert(k_partition_index > 0)\n",
    "            assert(k_partition_index < L)\n",
    "            partition_index_start = k_partition_index\n",
    "            \n",
    "        for parix in range(partition_index_start, L):    \n",
    "            new_exclude_set = set({k_best[parix]})\n",
    "            if parix == partition_index_start:\n",
    "                new_exclude_set = new_exclude_set | k_exclude_set\n",
    "            \n",
    "            new_best = np.ones(L, dtype=np.int) * (-1)\n",
    "            for pk in range(parix):\n",
    "                new_best[pk] = k_best[pk]\n",
    "            \n",
    "            candidate_points = [p for p in range(N) if p not in new_exclude_set]\n",
    "            if len(candidate_points) == 0: continue\n",
    "            candidate_maxix = np.argmax([Fp[parix-1, k_best[parix-1], p] for p in candidate_points])\n",
    "            new_best[parix] = candidate_points[candidate_maxix]\n",
    "            \n",
    "            for pk in range(parix+1, L):\n",
    "                new_best[pk] = np.argmax([Fp[pk-1, new_best[pk-1], p] for p in range(N)])\n",
    "            \n",
    "            new_priority = Fp[parix-1, k_best[parix-1], new_best[parix]]\n",
    "            if k_partition_index is not None:\n",
    "                new_priority += (-k_priority) - Fp[parix-1, k_best[parix-1], k_best[parix]]\n",
    "            new_priority *= -1.0  # NOTE: -np.inf - np.inf + np.inf = nan\n",
    "            \n",
    "            #if debug == True and np.isnan(new_priority):\n",
    "            #    print(Fp[parix-1,k_best[parix-1],new_best[parix]], (-k_priority), \\\n",
    "            #          Fp[parix-1,k_best[parix-1],k_best[parix]])\n",
    "            #    print(Fp[parix-1,k_best[parix-1],new_best[parix]] - k_priority - \\\n",
    "            #          Fp[parix-1,k_best[parix-1],k_best[parix]])\n",
    "                \n",
    "            #print(' '*3, 'push:', new_priority, new_best, parix, new_exclude_set)\n",
    "            \n",
    "            hq.heappush(Q, HeapItem(new_priority, (new_best, parix, new_exclude_set)))\n",
    "            #print('------------------\\n', Q, '\\n------------------')\n",
    "    if debug == True: print('#iterations: %d, #distinct_trajectories: %d' % (k, len(histories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#do_inference_listViterbi(ps0, L0, N0, w_u, w_p, f_u, f_p, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(do_inference_listViterbi(ps0, L0, N0, w_u, w_p, f_u, f_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEMM with first order MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute node features (singleton)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_node_features(startPOI, nPOI, poi_ix, poi_info, poi_clusters, cats, clusters):\n",
    "    \"\"\"\n",
    "    Generate feature vectors for all POIs given query (startPOI, nPOI)\n",
    "    \"\"\"\n",
    "    assert(isinstance(cats, list))\n",
    "    assert(isinstance(clusters, list))\n",
    "    \n",
    "    columns = DF_COLUMNS[3:]\n",
    "    poi_distmat = POI_DISTMAT\n",
    "    query_id_dict = QUERY_ID_DICT\n",
    "    key = (p0, trajLen) = (startPOI, nPOI)\n",
    "    assert(key in query_id_dict)\n",
    "    assert(p0 in poi_info.index)\n",
    "    \n",
    "    # DEBUG: use uniform node features\n",
    "    nrows = len(poi_ix)\n",
    "    ncols = len(columns) + len(cats) + len(clusters) - 2\n",
    "    #return np.ones((nrows, ncols), dtype=np.float)\n",
    "    #return np.zeros((nrows, ncols), dtype=np.float)\n",
    "    \n",
    "    poi_list = poi_ix\n",
    "    df_ = pd.DataFrame(index=np.arange(len(poi_list)), columns=columns)\n",
    "        \n",
    "    for i in range(df_.index.shape[0]):\n",
    "        poi = poi_list[i]\n",
    "        lon, lat = poi_info.loc[poi, 'poiLon'], poi_info.loc[poi, 'poiLat']\n",
    "        pop, nvisit = poi_info.loc[poi, 'popularity'], poi_info.loc[poi, 'nVisit']\n",
    "        cat, cluster = poi_info.loc[poi, 'poiCat'], poi_clusters.loc[poi, 'clusterID']\n",
    "        duration = poi_info.loc[poi, 'avgDuration']\n",
    "        idx = df_.index[i]\n",
    "        df_.set_value(idx, 'category', tuple((cat == np.array(cats)).astype(np.int) * 2 - 1))\n",
    "        df_.set_value(idx, 'neighbourhood', tuple((cluster == np.array(clusters)).astype(np.int) * 2 - 1))\n",
    "        df_.loc[idx, 'popularity'] = LOG_SMALL if pop < 1 else np.log10(pop)\n",
    "        df_.loc[idx, 'nVisit'] = LOG_SMALL if nvisit < 1 else np.log10(nvisit)\n",
    "        df_.loc[idx, 'avgDuration'] = LOG_SMALL if duration < 1 else np.log10(duration)\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_all.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi, p0]\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'sameNeighbourhoodStart'] = 1 if cluster == poi_clusters.loc[p0, 'clusterID'] else -1\n",
    "    \n",
    "    # features other than category and neighbourhood\n",
    "    X = df_[list(set(df_.columns) - {'category', 'neighbourhood'})].values  \n",
    "    \n",
    "    # boolean features: category (+1, -1)\n",
    "    cat_features = np.vstack([list(df_.loc[x, 'category']) for x in df_.index])\n",
    "    \n",
    "    # boolean features: neighbourhood (+1, -1)\n",
    "    neigh_features = np.vstack([list(df_.loc[x, 'neighbourhood']) for x in df_.index])\n",
    "    \n",
    "    return np.hstack([X, cat_features, neigh_features]).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute edge features (transiton / pairwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_edge_features(trajid_list, poi_ix, traj_dict, poi_info):    \n",
    "    feature_names = ['poiCat', 'popularity', 'nVisit', 'avgDuration', 'clusterID']\n",
    "    n_features = len(feature_names)\n",
    "    \n",
    "    # DEBUG: use uniform edge features\n",
    "    #return np.ones((len(poi_ix), len(poi_ix), n_features), dtype=np.float)\n",
    "    #return np.zeros((len(poi_ix), len(poi_ix), n_features), dtype=np.float)\n",
    "    \n",
    "    transmat_cat                        = gen_transmat_cat(trajid_list, traj_dict, poi_info)\n",
    "    transmat_pop,      logbins_pop      = gen_transmat_pop(trajid_list, traj_dict, poi_info)\n",
    "    transmat_visit,    logbins_visit    = gen_transmat_visit(trajid_list, traj_dict, poi_info)\n",
    "    transmat_duration, logbins_duration = gen_transmat_duration(trajid_list, traj_dict, poi_info)\n",
    "    transmat_neighbor, poi_clusters     = gen_transmat_neighbor(trajid_list, traj_dict, poi_info)\n",
    "    \n",
    "    poi_features = pd.DataFrame(data=np.zeros((len(poi_ix), len(feature_names))), \\\n",
    "                                columns=feature_names, index=poi_ix)\n",
    "    poi_features.index.name = 'poiID'\n",
    "    poi_features['poiCat'] = poi_info.loc[poi_ix, 'poiCat']\n",
    "    poi_features['popularity'] = np.digitize(poi_info.loc[poi_ix, 'popularity'], logbins_pop)\n",
    "    poi_features['nVisit'] = np.digitize(poi_info.loc[poi_ix, 'nVisit'], logbins_visit)\n",
    "    poi_features['avgDuration'] = np.digitize(poi_info.loc[poi_ix, 'avgDuration'], logbins_duration)\n",
    "    poi_features['clusterID'] = poi_clusters.loc[poi_ix, 'clusterID']\n",
    "    \n",
    "    edge_features = np.zeros((len(poi_ix), len(poi_ix), n_features), dtype=np.float64)\n",
    "    \n",
    "    for j in range(len(poi_ix)): # NOTE: POI order\n",
    "        pj = poi_ix[j]\n",
    "        cat, pop = poi_features.loc[pj, 'poiCat'], poi_features.loc[pj, 'popularity']\n",
    "        visit, cluster = poi_features.loc[pj, 'nVisit'], poi_features.loc[pj, 'clusterID']\n",
    "        duration = poi_features.loc[pj, 'avgDuration']\n",
    "        \n",
    "        for k in range(len(poi_ix)): # NOTE: POI order\n",
    "            pk = poi_ix[k]\n",
    "            edge_features[j, k, :] = np.log10( np.array(\n",
    "                    [transmat_cat.loc[cat, poi_features.loc[pk, 'poiCat']], \\\n",
    "                     transmat_pop.loc[pop, poi_features.loc[pk, 'popularity']], \\\n",
    "                     transmat_visit.loc[visit, poi_features.loc[pk, 'nVisit']], \\\n",
    "                     transmat_duration.loc[duration, poi_features.loc[pk, 'avgDuration']], \\\n",
    "                     transmat_neighbor.loc[cluster, poi_features.loc[pk, 'clusterID']]] ) )\n",
    "    return edge_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function for MEMM with first order MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "from scipy.misc import logsumexp\n",
    "cimport numpy as np\n",
    "\n",
    "cpdef cost_MEMM_1MC(w, X_node, X_edge, list Y, float C, long M):\n",
    "    \"\"\"\n",
    "    w - parameter vector\n",
    "    X_node - feature matrix of POIs for all training examples, (N x M) x n_node_features\n",
    "    X_edge - transition feature matrix of POIs, M x M x n_edge_features\n",
    "    Y - labels/trajectories for all training examples\n",
    "    C - regularisation constant\n",
    "    M - total number of POIs\n",
    "    \"\"\"\n",
    "    #print('entering cost_MEMM')\n",
    "    assert(C > 0)\n",
    "    assert(M > 0)\n",
    "    cdef long N, D, i, j, pj, pk, pl\n",
    "    N = int(np.shape(X_node)[0]/M)\n",
    "    D = np.shape(X_node)[1] * 2 + np.shape(X_edge)[2]\n",
    "    assert(D == np.shape(w)[0])\n",
    "    assert(N == len(Y))\n",
    "    \n",
    "    cdef double result\n",
    "    result = 0.0\n",
    "    for i in range(N):\n",
    "        for j in range(1, np.shape(Y[i])[0]):\n",
    "            pj = Y[i][j-1]  # index of feature vector for POI p_{j-1}\n",
    "            pk = Y[i][j]\n",
    "            result -= np.dot(w, np.hstack([X_node[i*M + pj], X_edge[pj, pk], X_node[i*M + pk]]))\n",
    "            result += logsumexp([np.dot(w, np.hstack([X_node[i*M + pj], X_edge[pj, pl], X_node[i*M + pl]])) \\\n",
    "                                 for pl in range(M)])\n",
    "    #print('exit cost_MEMM')\n",
    "    return 0.5 * np.dot(w, w) + result * C / N  # note the normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of cost function for MEMM with first order MC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "cpdef grad_MEMM_1MC(w, X_node, X_edge, list Y, float C, long M):\n",
    "    \"\"\"\n",
    "    w - parameter vector\n",
    "    X_node - feature matrix of POIs for all training examples, (N x M) x n_node_features\n",
    "    X_edge - transition feature matrix of POIs, M x M x n_edge_features\n",
    "    Y - labels/trajectories for all training examples\n",
    "    C - regularisation constant\n",
    "    M - total number of POIs\n",
    "    \"\"\"\n",
    "    #print('entering grad_MEMM')\n",
    "    assert(C > 0)\n",
    "    assert(M > 0)\n",
    "    cdef long N, D, i, j, pj, pk, pl\n",
    "    N = int(np.shape(X_node)[0]/M)\n",
    "    D = np.shape(X_node)[1] * 2 + np.shape(X_edge)[2]\n",
    "    assert(D == np.shape(w)[0])\n",
    "    assert(N == len(Y))\n",
    "    \n",
    "    cdef float denorminator\n",
    "    grad = np.zeros(D, dtype=np.float)\n",
    "    for i in range(N):\n",
    "        for j in range(1, np.shape(Y[i])[0]):\n",
    "            pj = Y[i][j-1]  # index of feature vector for POI p_{j-1}\n",
    "            pk = Y[i][j]\n",
    "            grad -= np.hstack([X_node[i*M + pj], X_edge[pj, pk], X_node[i*M + pk]])\n",
    "            denorminator = 0.0\n",
    "            numerator = np.zeros(D, dtype=np.float)\n",
    "            for pl in range(M):\n",
    "                feature = np.hstack([X_node[i*M + pj], X_edge[pj, pl], X_node[i*M + pl]])\n",
    "                term = np.exp(np.dot(w, feature))\n",
    "                numerator = numerator + term * feature\n",
    "                denorminator += term\n",
    "            grad = grad + numerator / denorminator\n",
    "    #print('exit grad_MEMM')\n",
    "    return w + grad * C / N  # note the normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference for MEMM with first order MC using the Viterbi algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_MEMM_1MC(ps, L, M, w, X_node, X_edge):\n",
    "    assert(L > 1)\n",
    "    assert(M >= L)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < M)\n",
    "    assert(w.shape[0] == X_node.shape[1]*2 + X_edge.shape[2])\n",
    "    \n",
    "    A = np.zeros((L-1, M), dtype=np.float)     # scores matrix\n",
    "    B = np.ones((L-1, M), dtype=np.int) * (-1) # backtracking pointers\n",
    "    \n",
    "    for p in range(M): # ps--p\n",
    "        A[0, p] = np.dot(w, np.hstack([X_node[ps, :], X_edge[ps, p], X_node[p, :]])) if ps != p else -np.inf\n",
    "        B[0, p] = ps\n",
    "\n",
    "    for t in range(0, L-2): # ps~~p1--p\n",
    "        for p in range(M):\n",
    "            scores = [np.dot(w, np.hstack([X_node[p1,:], X_edge[p1,p], X_node[p,:]])) if p1 not in {p,ps} else -np.inf \\\n",
    "                      for p1 in range(M)]\n",
    "            maxix = np.argmax(scores)\n",
    "            A[t+1, p] = scores[maxix]\n",
    "            B[t+1, p] = maxix\n",
    "\n",
    "    y_hat = [np.argmax(A[L-2, :])]\n",
    "    p, t = y_hat[-1], L-2\n",
    "    while t >= 0:\n",
    "        y_hat.append(B[t, p])\n",
    "        p, t = y_hat[-1], t-1\n",
    "    y_hat.reverse()\n",
    "\n",
    "    return np.asarray(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_memm1m == True:\n",
    "    recdict_memm1m = dict()\n",
    "    cnt = 1\n",
    "    keys = sorted(TRAJ_GROUP_DICT.keys())\n",
    "    for i in range(len(keys)):\n",
    "        t0 = time.time()\n",
    "        ps, L = keys[i]\n",
    "        assert(L > 1) # length > 1\n",
    "        \n",
    "        # training set\n",
    "        trajid_set_train = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "        poi_info = calc_poi_info(list(trajid_set_train), traj_all, poi_all)\n",
    "        assert(L <= poi_info.shape[0])\n",
    "                \n",
    "        # build POI_ID <--> POI__INDEX mapping for POIs used to train CRF\n",
    "        # which means only POIs in traj such that len(traj) >= 2 are included\n",
    "        poi_set = set()\n",
    "        for x in trajid_set_train:\n",
    "            if len(traj_dict[x]) >= 2:\n",
    "                poi_set = poi_set | set(traj_dict[x])\n",
    "        poi_ix = sorted(poi_set)\n",
    "        poi_id_dict, poi_id_rdict = dict(), dict()\n",
    "        for idx, poi in enumerate(poi_ix):\n",
    "            poi_id_dict[poi] = idx\n",
    "            poi_id_rdict[idx] = poi\n",
    "            \n",
    "        # start should be in training set\n",
    "        if ps not in poi_set: continue\n",
    "            \n",
    "        print('query #%d: start=%d, length=%d ->' % (cnt, ps, L)); cnt += 1; sys.stdout.flush()\n",
    "        \n",
    "        # generate training data\n",
    "        train_traj_list = [traj_dict[x] for x in trajid_set_train if len(traj_dict[x]) >= 2]\n",
    "        node_features_list = Parallel(n_jobs=N_JOBS)\\\n",
    "                             (delayed(calc_node_features)\\\n",
    "                              (tr[0], len(tr), poi_ix, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                               cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST) for tr in train_traj_list)\n",
    "        edge_features = calc_edge_features(list(trajid_set_train), poi_ix, traj_dict, poi_info)       \n",
    "        assert(len(train_traj_list) == len(node_features_list))\n",
    "        X_node = np.vstack(node_features_list)\n",
    "        X_edge = edge_features\n",
    "        y_train = [np.array([poi_id_dict[x] for x in tr]) for tr in train_traj_list]\n",
    "        \n",
    "        # feature scaling\n",
    "        if MIN_MAX_SCALE == True:\n",
    "            scaler = MinMaxScaler(feature_range=(-1,1), copy=False)\n",
    "        else:\n",
    "            scaler = StandardScaler(copy=False)\n",
    "        X_node = scaler.fit_transform(X_node)\n",
    "                        \n",
    "        # train and test\n",
    "        w = np.random.rand(X_node.shape[1] * 2 + X_edge.shape[2])  # initial guess\n",
    "        opt_method = 'BFGS' # 'Newton-CG'\n",
    "        opt = minimize(cost_MEMM_1MC, w, args=(X_node, X_edge, y_train, LOGIT_C, len(poi_ix)), \\\n",
    "                       method=opt_method, jac=grad_MEMM_1MC, options={'disp': True})\n",
    "        if opt.success == True:\n",
    "            w = opt.x\n",
    "            \n",
    "            # generate test data\n",
    "            X_node_test = calc_node_features(ps, L, poi_ix, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                             cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "            X_node_test = scaler.transform(X_node_test)\n",
    "        \n",
    "            # prediction (Viterbi)\n",
    "            y_hat = inference_MEMM_1MC(poi_id_dict[ps], L, len(poi_ix), w, X_node_test, X_edge)\n",
    "            \n",
    "            recdict_memm1m[(ps, L)] = [poi_id_rdict[x] for x in y_hat]\n",
    "            print(' '*10, recdict_memm1m[(ps, L)])\n",
    "            \n",
    "        else:\n",
    "            sys.stderr.write('Optimisation failed\\n')\n",
    "            continue\n",
    "        if cnt == 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_memm1m == True:\n",
    "    F1_memm1m = []; pF1_memm1m = []; Tau_memm1m = []\n",
    "    for key in sorted(recdict_memm1m.keys()):\n",
    "        F1, pF1, tau = evaluate(recdict_memm1m[key], TRAJ_GROUP_DICT[key])\n",
    "        F1_memm1m.append(F1); pF1_memm1m.append(pF1); Tau_memm1m.append(tau)\n",
    "    print('MEMM_1MC: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_memm1m), np.std(F1_memm1m)/np.sqrt(len(F1_memm1m)), \\\n",
    "           np.mean(pF1_memm1m), np.std(pF1_memm1m)/np.sqrt(len(pF1_memm1m)), \\\n",
    "           np.mean(Tau_memm1m), np.std(Tau_memm1m)/np.sqrt(len(Tau_memm1m))))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
