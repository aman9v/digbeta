{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_dir = 'data/data-new'\n",
    "#dat_suffix = ['Osak', 'Glas', 'Edin', 'Toro', 'Melb']\n",
    "dat_name = ['Osaka', 'Glasgow', 'Edinburge', 'Toronto', 'Melbourne']\n",
    "C_SET = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000]  # regularisation parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "algo = ['rand', 'pop', 'linreg', 'logreg', 'rank', 'logpwr', 'tranDP', 'tranILP', 'combDP', 'combILP', \\\n",
    "        'ssvm-greedy', 'ssvm-viterbi', 'ssvm-listViterbi', 'ssvm-ILP', 'memm']\n",
    "algo_name = ['Random', 'Popularity', 'POILocationPrediction', 'POIOccurrencePrediction', 'RankSVM', 'RankLogistic', \\\n",
    "             'Markov', 'MarkovPath', 'Rank+Markov', 'Rank+MarkovPath', \\\n",
    "             'SSVM-Greedy', 'SSVM-Viterbi', 'SSVM-ListViterbi', 'SSVM-ILP', 'MEMM']\n",
    "metric_name = ['F$_1$', 'pairs-F$_1$', '$\\\\tau$']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dat_ix``` is required in notebook ```shared.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_ix = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run notebook ```shared.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run 'shared.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_fnames(dat_ix):\n",
    "    fnames = []\n",
    "    for a in algo:\n",
    "        fnames.append(os.path.join(data_dir, a + '-' + dat_suffix[dat_ix] + '.pkl'))\n",
    "    return fnames        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fnames = build_fnames(dat_ix)\n",
    "fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_metrics(fnames):\n",
    "    assert(len(fnames) == len(algo))\n",
    "    recdicts = []\n",
    "    for f in fnames:\n",
    "        recdicts.append(pkl.load(open(f, 'rb')))\n",
    "    \n",
    "    keys = set(recdicts[0].keys())\n",
    "    #print(len(keys))\n",
    "    for d in recdicts[1:]:\n",
    "        keys = keys & set(d.keys())\n",
    "        #print(len(d.keys()))\n",
    "    print('#Records:', len(keys))\n",
    "    keys = sorted(keys)\n",
    "    \n",
    "    metrics = np.zeros((len(algo), 3, len(keys)), dtype=np.float)\n",
    "    Cs = -1 * np.ones((len(algo), len(keys)), dtype=np.float)\n",
    "    \n",
    "    for j in range(len(recdicts)):\n",
    "        d = recdicts[j]\n",
    "        for k in range(len(keys)):\n",
    "            q = keys[k]\n",
    "            F1, pF1, tau = evaluate(d[q]['PRED'], TRAJ_GROUP_DICT[q])\n",
    "            if 'C' in d[q]: Cs[j, k] = d[q]['C']\n",
    "            metrics[j, 0, k] = F1\n",
    "            metrics[j, 1, k] = pF1\n",
    "            metrics[j, 2, k] = tau\n",
    "    return metrics, keys, Cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics, keys, Cs = calc_metrics(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the values of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric_ix = 0  # [F1, pairs-F1, Tau]\n",
    "\n",
    "plt.figure(figsize=[15, 5])\n",
    "X = np.arange(metrics.shape[2])\n",
    "plt.plot(X, metrics[4, metric_ix, :], c='r', ls='--', marker='^', markeredgewidth=0) # RankSVM\n",
    "plt.plot(X, metrics[10, metric_ix, :], c='g', ls='--', marker='v', markeredgewidth=0) # SSVMListViterbi\n",
    "plt.xticks(np.arange(metrics.shape[2]), [str(q) for q in keys], fontsize=10, rotation=50, horizontalalignment='right')\n",
    "plt.xlim(-1, metrics.shape[2])\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.xlabel('Query')\n",
    "plt.ylabel(metric_name[metric_ix])\n",
    "plt.title('Values of evaluation metric ' + metric_name[metric_ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot values of hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 5])\n",
    "X = np.arange(Cs.shape[1])\n",
    "plt.plot(X, Cs[4, :], c='r', ls='--', marker='^', markeredgewidth=0) # RankSVM\n",
    "plt.xticks(np.arange(Cs.shape[1]), [str(q) for q in keys], fontsize=10, rotation=50, horizontalalignment='right')\n",
    "plt.xlim(-1, Cs.shape[1])\n",
    "plt.ylim(0.001, 10000)\n",
    "plt.plot([-1, Cs.shape[1]], [C_SET[0],  C_SET[0]],  c='b', ls='-')\n",
    "plt.plot([-1, Cs.shape[1]], [C_SET[-1], C_SET[-1]], c='b', ls='-')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Query')\n",
    "plt.ylabel('C')\n",
    "plt.title('Values of hyper-parameter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate LaTeX table for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_latex_h(mean, stderr, title, label):\n",
    "    assert(mean.shape == stderr.shape)\n",
    "    assert(mean.shape == (len(algo), 3))\n",
    "    \n",
    "    max_1st = np.zeros(len(metric_name), dtype=np.int)\n",
    "    max_2nd = np.zeros(len(metric_name), dtype=np.int)\n",
    "    \n",
    "    for j in range(mean.shape[1]):\n",
    "        max_2nd[j], max_1st[j] = np.argsort(mean[:, j])[-2:]\n",
    "    \n",
    "    strs = []\n",
    "    strs.append('\\\\begin{table*}[t]\\n')\n",
    "    strs.append('\\\\caption{' + title + '}\\n')\n",
    "    strs.append('\\\\label{' + label + '}\\n')\n",
    "    strs.append('\\\\centering\\n')\n",
    "    strs.append('\\\\begin{tabular}{l|' + (mean.shape[1])*'c' + '} \\\\hline\\n')\n",
    "    for j in range(mean.shape[1]):\n",
    "        strs.append(' & ' + metric_name[j])\n",
    "    strs.append(' \\\\\\\\ \\\\hline\\n')\n",
    "    for i in range(mean.shape[0]):\n",
    "        strs.append(algo_name[i] + ' ')\n",
    "        for j in range(mean.shape[1]):\n",
    "            strs.append('& $')\n",
    "            if i == max_1st[j]: strs.append('\\\\mathbf{')\n",
    "            if i == max_2nd[j]: strs.append('\\\\mathit{')\n",
    "            strs.append('%.3f' % mean[i, j] + '\\\\pm' + '%.3f' % stderr[i, j])\n",
    "            if i in [max_1st[j], max_2nd[j]]: strs.append('}')\n",
    "            strs.append('$ ')\n",
    "        strs.append('\\\\\\\\\\n')\n",
    "    strs.append('\\\\hline\\n')\n",
    "    strs.append('\\\\end{tabular}\\n')\n",
    "    strs.append('\\\\end{table*}\\n')\n",
    "    return ''.join(strs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean   = np.zeros((len(algo), 3), dtype=np.float)\n",
    "stderr = np.zeros((len(algo), 3), dtype=np.float)\n",
    "for i in range(len(algo)):\n",
    "        mean[i, 0] = np.mean(metrics[i, 0, :]); stderr[i, 0] = np.std(metrics[i, 0, :]) / np.sqrt(metrics.shape[2])\n",
    "        mean[i, 1] = np.mean(metrics[i, 1, :]); stderr[i, 1] = np.std(metrics[i, 1, :]) / np.sqrt(metrics.shape[2])\n",
    "        mean[i, 2] = np.mean(metrics[i, 2, :]); stderr[i, 2] = np.std(metrics[i, 2, :]) / np.sqrt(metrics.shape[2])\n",
    "strs = gen_latex_h(mean, stderr, 'Performance', 'tab:performance')\n",
    "print(strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate LaTeX table for each evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_metric_mean(metrics_list, metric_ix):\n",
    "    assert(len(metrics_list) == len(dat_name))\n",
    "    assert(0 <= metric_ix < len(metric_name))\n",
    "    assert(type(metric_ix) == int)\n",
    "    mean   = np.zeros((len(algo), len(dat_name)), dtype=np.float)\n",
    "    stderr = np.zeros((len(algo), len(dat_name)), dtype=np.float)\n",
    "    for i in range(len(algo)):\n",
    "        for j in range(len(dat_name)):\n",
    "            mean[i, j]   = np.mean(metrics_list[j][i, metric_ix, :])\n",
    "            stderr[i, j] = np.std(metrics_list[j][i, metric_ix, :]) / np.sqrt(metrics_list[j].shape[2])\n",
    "    return mean, stderr   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_latex_v(mean, stderr, title, label):\n",
    "    assert(mean.shape == stderr.shape)\n",
    "    assert(mean.shape == (len(algo), len(dat_name)))\n",
    "    \n",
    "    max_1st = np.zeros(len(dat_name), dtype=np.int)\n",
    "    max_2nd = np.zeros(len(dat_name), dtype=np.int)\n",
    "    \n",
    "    for j in range(mean.shape[1]):\n",
    "        max_2nd[j], max_1st[j] = np.argsort(mean[:, j])[-2:]\n",
    "    \n",
    "    strs = []\n",
    "    strs.append('\\\\begin{table*}[t]\\n')\n",
    "    strs.append('\\\\caption{' + title + '}\\n')\n",
    "    strs.append('\\\\label{' + label + '}\\n')\n",
    "    strs.append('\\\\centering\\n')\n",
    "    strs.append('\\\\begin{tabular}{l|' + (mean.shape[1])*'c' + '} \\\\hline\\n')\n",
    "    for j in range(mean.shape[1]):\n",
    "        strs.append(' & ' + dat_name[j])\n",
    "    strs.append(' \\\\\\\\ \\\\hline\\n')\n",
    "    for i in range(mean.shape[0]):\n",
    "        strs.append(algo_name[i] + ' ')\n",
    "        for j in range(mean.shape[1]):\n",
    "            strs.append('& $')\n",
    "            if i == max_1st[j]: strs.append('\\\\mathbf{')\n",
    "            if i == max_2nd[j]: strs.append('\\\\mathit{')\n",
    "            strs.append('%.3f' % mean[i, j] + '\\\\pm' + '%.3f' % stderr[i, j])\n",
    "            if i in [max_1st[j], max_2nd[j]]: strs.append('}')\n",
    "            strs.append('$ ')\n",
    "        strs.append('\\\\\\\\\\n')\n",
    "    strs.append('\\\\hline\\n')\n",
    "    strs.append('\\\\end{tabular}\\n')\n",
    "    strs.append('\\\\end{table*}\\n')\n",
    "    return ''.join(strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_stdout = sys.stdout  # save the device for notebook output\n",
    "nb_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics_list = []\n",
    "sys.stdout = open('/dev/stdout', 'w')  # redirect the output of %run to terminal\n",
    "for dat_ix in range(len(dat_name)):\n",
    "    fnames = build_fnames(dat_ix)\n",
    "    %run 'shared.ipynb'\n",
    "    metrics, keys, Cs = calc_metrics(fnames)\n",
    "    metrics_list.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.stdout = nb_stdout  # restore the output to notebook\n",
    "sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric_ix = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean, stderr = calc_metric_mean(metrics_list, metric_ix)\n",
    "title = ['F$_1$ score on points', 'F$_1$ score on pairs', 'Kendall\\'s $\\\\tau$'][metric_ix]\n",
    "label = 'tab:' + ['f1', 'pf1', 'tau'][metric_ix]\n",
    "print(gen_latex_v(mean, stderr, title, label))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
