{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('src/src_cluster')\n",
    "from shared import TrajData, evaluate, do_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_dir = 'data/data-new'\n",
    "#dat_suffix = ['Osak', 'Glas', 'Edin', 'Toro', 'Melb']\n",
    "dat_name = ['Osaka', 'Glasgow']#, 'Edinburgh', 'Toronto', 'Melbourne']\n",
    "C_SET = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000, 3000]  # regularisation parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#algo = ['rand', 'pop', 'linreg', 'logreg', 'rank', 'logpwr', 'tranDP', 'tranILP', 'combDP', 'combILP', \\\n",
    "#        'ssvm-greedy', 'ssvm-viterbi', 'ssvm-listViterbi', 'ssvm-ILP', 'memm']\n",
    "#algo_name = ['Random', 'Popularity', 'POILocationPrediction', 'POIOccurrencePrediction', 'RankSVM', 'RankLogistic', \\\n",
    "#             'Markov', 'MarkovPath', 'Rank+Markov', 'Rank+MarkovPath', \\\n",
    "#             'SSVM-Greedy', 'SSVM-Viterbi', 'SSVM-ListViterbi', 'SSVM-ILP', 'MEMM']\n",
    "#metric_name = ['F$_1$', 'pairs-F$_1$', '$\\\\tau$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "algo = ['rand', 'pop', 'rank', 'ssvm-B10', 'ssvm-A10', 'ssvm-D10', 'ssvm-C10']\n",
    "#algo = ['rand', 'pop', 'rank', 'ssvm-B', 'ssvm-A', 'ssvm-D', 'ssvm-C']\n",
    "algo_name = ['\\\\textsc{Random}', '\\\\textsc{Popularity}', '\\\\textsc{PoiRank}', \n",
    "             '\\\\textsc{SP}', '\\\\textsc{SPpath}', '\\\\textsc{SR}', '\\\\textsc{SRpath}']\n",
    "metric_name = ['F$_1$', 'pairs-F$_1$', '$\\\\tau$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skip_indices = [(4, 2), (4, 3), (4, 4), (6, 2), (6, 3), (6, 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of the number of ground truths for queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%script false\n",
    "fig = plt.figure(figsize=[10, 3])\n",
    "#for dat_ix in range(len(dat_name)):\n",
    "#indices = [2, 1, 4, 0, 3]\n",
    "indices = [0, 1]\n",
    "for j in range(len(indices)):\n",
    "    dat_ix = indices[j]\n",
    "    dat_obj = TrajData(dat_ix)\n",
    "    #ax = plt.subplot(int('15' + str(dat_ix+1)))\n",
    "    ax = plt.subplot(int('12' + str(j+1)))\n",
    "    ax.set_xlabel('#Ground Truths')\n",
    "    ax.set_ylabel('#Queries')\n",
    "    ax.set_yscale('log')\n",
    "    #ax.set_ylim([0.1, 1000])\n",
    "    ax.set_ylim([0.5, 100])\n",
    "    ax.set_title(dat_name[dat_ix])\n",
    "    Y = [len(dat_obj.TRAJID_GROUP_DICT[q]) for q in dat_obj.TRAJID_GROUP_DICT]\n",
    "    pd.Series(Y).hist(bins=10, ax=ax)\n",
    "    fig.savefig('hist.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple dataset stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_ix = 1\n",
    "dat_obj = TrajData(dat_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrajs = np.sum([len(dat_obj.TRAJID_GROUP_DICT[q]) for q in dat_obj.TRAJID_GROUP_DICT.keys()])\n",
    "print('#Traj:', ntrajs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pois = {p for q in dat_obj.TRAJID_GROUP_DICT.keys() for tid in dat_obj.TRAJID_GROUP_DICT[q] \\\n",
    "        for p in dat_obj.traj_dict[tid]}\n",
    "print('#POIs:', len(pois))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users = {dat_obj.traj_user.loc[tid, 'userID'] for q in dat_obj.TRAJID_GROUP_DICT.keys() \\\n",
    "         for tid in dat_obj.TRAJID_GROUP_DICT[q]}\n",
    "print('#Users:', len(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('#Queries:', len(dat_obj.TRAJID_GROUP_DICT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lentotal = np.sum([len(dat_obj.traj_dict[tid]) for q in dat_obj.TRAJID_GROUP_DICT.keys() \\\n",
    "                   for tid in dat_obj.TRAJID_GROUP_DICT[q]])\n",
    "print('AvgLengh: %.1f' % (lentotal/ntrajs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "plt.figure(figsize=[15, 5])\n",
    "keys = sorted(recdict.keys())\n",
    "X = np.arange(len(keys))\n",
    "Y1 = [recdict[q]['C'] for q in keys]\n",
    "Y2 = [recdict1[q]['C'] for q in keys]\n",
    "Y3 = [recdict0[q]['C'] for q in keys]\n",
    "#plt.plot(X, Y1, c='r', ls='--', marker='^', markeredgewidth=0, label='NEW-NOLOG')\n",
    "plt.plot(X, Y2, c='g', ls='--', marker='v', markeredgewidth=0, label='NEW-LOG')\n",
    "plt.plot(X, Y3, c='b', ls='--', marker='o', markeredgewidth=0, label='OLD-LOG')\n",
    "plt.xticks(np.arange(len(keys)), [str(q) for q in keys], fontsize=10, rotation=50, horizontalalignment='right')\n",
    "plt.xlim(-1, len(keys))\n",
    "plt.ylim(0.001, 10000)\n",
    "plt.plot([-1, len(keys)], [C_SET[0],  C_SET[0]],  c='b', ls='-')\n",
    "plt.plot([-1, len(keys)], [C_SET[-1], C_SET[-1]], c='b', ls='-')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Query')\n",
    "plt.ylabel('C')\n",
    "plt.title('Values of hyper-parameter $C$')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_fnames(dat_obj, dat_ix):\n",
    "    fnames = []\n",
    "    for a in algo:\n",
    "        fnames.append(os.path.join(dat_obj.data_dir, a + '-' + dat_obj.dat_suffix[dat_ix] + '.pkl'))\n",
    "    return fnames        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fnames = build_fnames(dat_obj, dat_ix)\n",
    "#fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_metrics(dat_obj, fnames):\n",
    "    assert(len(fnames) == len(algo))\n",
    "    recdicts = []\n",
    "    for f in fnames:\n",
    "        recdicts.append(pkl.load(open(f, 'rb')))\n",
    "    \n",
    "    queries = set(recdicts[0].keys())\n",
    "    #print(len(keys))\n",
    "    for d in recdicts[1:]:\n",
    "        queries = queries & set(d.keys())\n",
    "        #print(len(d.keys()))\n",
    "    print('#Records:', len(queries))\n",
    "    queries = sorted(queries)\n",
    "    \n",
    "    metrics = np.zeros((len(algo), 3, len(queries)), dtype=np.float)\n",
    "    Cs = -1 * np.ones((len(algo), len(queries)), dtype=np.float)\n",
    "    \n",
    "    for i in range(len(recdicts)):\n",
    "        d = {k:recdicts[i][k] for k in queries}\n",
    "        F1_list, pF1_list, Tau_list = do_evaluation(dat_obj, d, debug=False)\n",
    "        assert(len(F1_list) == len(pF1_list) == len(Tau_list) == len(queries))\n",
    "        metrics[i, 0, :] = F1_list\n",
    "        metrics[i, 1, :] = pF1_list\n",
    "        metrics[i, 2, :] = Tau_list\n",
    "        \n",
    "        for k in range(len(queries)):\n",
    "            q = queries[k]\n",
    "            if 'C' in d[q]: Cs[i, k] = d[q]['C']\n",
    "    return metrics, queries, Cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#metrics, keys, Cs = calc_metrics(dat_obj, fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the values of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "metric_ix = 0  # [F1, pairs-F1, Tau]\n",
    "\n",
    "plt.figure(figsize=[15, 5])\n",
    "X = np.arange(metrics.shape[2])\n",
    "plt.plot(X, metrics[4, metric_ix, :], c='r', ls='--', marker='^', markeredgewidth=0, label='RankSVM')\n",
    "plt.plot(X, metrics[12, metric_ix, :], c='g', ls='--', marker='v', markeredgewidth=0, label='SSVM-ListViterbi')\n",
    "plt.xticks(np.arange(metrics.shape[2]), [str(q) for q in keys], fontsize=10, rotation=50, horizontalalignment='right')\n",
    "plt.xlim(-1, metrics.shape[2])\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.xlabel('Query')\n",
    "plt.ylabel(metric_name[metric_ix])\n",
    "plt.title('Values of evaluation metric ' + metric_name[metric_ix])\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot values of hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "plt.figure(figsize=[15, 5])\n",
    "X = np.arange(Cs.shape[1])\n",
    "plt.plot(X, Cs[4, :], c='r', ls='--', marker='^', markeredgewidth=0, label='RankSVM')\n",
    "plt.plot(X, Cs[12, :], c='g', ls='--', marker='v', markeredgewidth=0, label='SSVM-ListViterbi')\n",
    "plt.xticks(np.arange(Cs.shape[1]), [str(q) for q in keys], fontsize=10, rotation=50, horizontalalignment='right')\n",
    "plt.xlim(-1, Cs.shape[1])\n",
    "plt.ylim(0.001, 10000)\n",
    "plt.plot([-1, Cs.shape[1]], [C_SET[0],  C_SET[0]],  c='b', ls='-')\n",
    "plt.plot([-1, Cs.shape[1]], [C_SET[-1], C_SET[-1]], c='b', ls='-')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Query')\n",
    "plt.ylabel('C')\n",
    "plt.title('Values of hyper-parameter $C$')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate LaTeX table for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_latex_h(mean, stderr, title, label):\n",
    "    assert(mean.shape == stderr.shape)\n",
    "    assert(mean.shape == (len(algo), 3))\n",
    "    \n",
    "    max_1st = np.zeros(len(metric_name), dtype=np.int)\n",
    "    max_2nd = np.zeros(len(metric_name), dtype=np.int)\n",
    "    \n",
    "    for j in range(mean.shape[1]):\n",
    "        max_2nd[j], max_1st[j] = np.argsort(mean[:, j])[-2:]\n",
    "    \n",
    "    strs = []\n",
    "    strs.append('\\\\begin{table*}[t]\\n')\n",
    "    strs.append('\\\\caption{' + title + '}\\n')\n",
    "    strs.append('\\\\label{' + label + '}\\n')\n",
    "    strs.append('\\\\centering\\n')\n",
    "    strs.append('\\\\begin{tabular}{l|' + (mean.shape[1])*'c' + '} \\\\hline\\n')\n",
    "    for j in range(mean.shape[1]):\n",
    "        strs.append(' & ' + metric_name[j])\n",
    "    strs.append(' \\\\\\\\ \\\\hline\\n')\n",
    "    for i in range(mean.shape[0]):\n",
    "        strs.append(algo_name[i] + ' ')\n",
    "        for j in range(mean.shape[1]):\n",
    "            strs.append('& $')\n",
    "            if i == max_1st[j]: strs.append('\\\\mathbf{')\n",
    "            if i == max_2nd[j]: strs.append('\\\\mathit{')\n",
    "            strs.append('%.3f' % mean[i, j] + '\\\\pm' + '%.3f' % stderr[i, j])\n",
    "            if i in [max_1st[j], max_2nd[j]]: strs.append('}')\n",
    "            strs.append('$ ')\n",
    "        strs.append('\\\\\\\\\\n')\n",
    "    strs.append('\\\\hline\\n')\n",
    "    strs.append('\\\\end{tabular}\\n')\n",
    "    strs.append('\\\\end{table*}\\n')\n",
    "    return ''.join(strs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "mean   = np.zeros((len(algo), 3), dtype=np.float)\n",
    "stderr = np.zeros((len(algo), 3), dtype=np.float)\n",
    "for i in range(len(algo)):\n",
    "        mean[i, 0] = np.mean(metrics[i, 0, :]); stderr[i, 0] = np.std(metrics[i, 0, :]) / np.sqrt(metrics.shape[2])\n",
    "        mean[i, 1] = np.mean(metrics[i, 1, :]); stderr[i, 1] = np.std(metrics[i, 1, :]) / np.sqrt(metrics.shape[2])\n",
    "        mean[i, 2] = np.mean(metrics[i, 2, :]); stderr[i, 2] = np.std(metrics[i, 2, :]) / np.sqrt(metrics.shape[2])\n",
    "strs = gen_latex_h(mean, stderr, 'Performance', 'tab:performance')\n",
    "print(strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate LaTeX table for each evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_metric_mean(metrics_list, metric_ix):\n",
    "    assert(len(metrics_list) == len(dat_name))\n",
    "    assert(type(metric_ix) == int)\n",
    "    assert(0 <= metric_ix < len(metric_name))\n",
    "    mean   = np.zeros((len(algo), len(dat_name)), dtype=np.float)\n",
    "    stderr = np.zeros((len(algo), len(dat_name)), dtype=np.float)\n",
    "    for i in range(len(algo)):\n",
    "        for j in range(len(dat_name)):\n",
    "            mean[i, j]   = np.mean(metrics_list[j][i, metric_ix, :])\n",
    "            stderr[i, j] = np.std(metrics_list[j][i, metric_ix, :]) / np.sqrt(metrics_list[j].shape[2])\n",
    "    return mean, stderr   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_metric_diff(metrics_list, metric_ix, baseline_ix=2):\n",
    "    assert(len(metrics_list) == len(dat_name))\n",
    "    assert(type(metric_ix) == int)\n",
    "    assert(0 <= metric_ix < len(metric_name))\n",
    "    assert(type(baseline_ix) == int)\n",
    "    assert(0 <= baseline_ix < len(algo))\n",
    "    diffmetrics_list = []\n",
    "    for dat_ix in range(len(metrics_list)):\n",
    "        diffmetrics = metrics_list[dat_ix].copy()\n",
    "        for i in range(diffmetrics.shape[0]):\n",
    "            diffmetrics[i, 0, :] -= metrics_list[dat_ix][baseline_ix, 0, :]\n",
    "            diffmetrics[i, 1, :] -= metrics_list[dat_ix][baseline_ix, 1, :]\n",
    "            diffmetrics[i, 2, :] -= metrics_list[dat_ix][baseline_ix, 2, :]\n",
    "        diffmetrics_list.append(diffmetrics)\n",
    "    \n",
    "    diffmean   = np.zeros((len(algo), len(dat_name)), dtype=np.float)\n",
    "    diffstderr = np.zeros((len(algo), len(dat_name)), dtype=np.float)\n",
    "    for i in range(len(algo)):\n",
    "        for j in range(len(dat_name)):\n",
    "            diffmean[i, j]   = np.mean(diffmetrics_list[j][i, metric_ix, :])\n",
    "            diffstderr[i, j] = np.std(diffmetrics_list[j][i, metric_ix, :]) / np.sqrt(diffmetrics_list[j].shape[2])\n",
    "    return diffmean, diffstderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_latex_v(mean, stderr, title, label):\n",
    "    assert(mean.shape == stderr.shape)\n",
    "    assert(mean.shape == (len(algo), len(dat_name)))\n",
    "    \n",
    "    max_1st = np.zeros(len(dat_name), dtype=np.int)\n",
    "    max_2nd = np.zeros(len(dat_name), dtype=np.int)\n",
    "    \n",
    "    for j in range(mean.shape[1]):\n",
    "        max_2nd[j], max_1st[j] = np.argsort(mean[:, j])[-2:]\n",
    "    \n",
    "    strs = []\n",
    "    strs.append('\\\\begin{table*}[t]\\n')\n",
    "    strs.append('\\\\caption{' + title + '}\\n')\n",
    "    strs.append('\\\\label{' + label + '}\\n')\n",
    "    strs.append('\\\\centering\\n')\n",
    "    strs.append('\\\\begin{tabular}{l|' + (mean.shape[1])*'c' + '} \\\\hline\\n')\n",
    "    for j in range(mean.shape[1]):\n",
    "        strs.append(' & ' + dat_name[j])\n",
    "    strs.append(' \\\\\\\\ \\\\hline\\n')\n",
    "    for i in range(mean.shape[0]):\n",
    "        strs.append(algo_name[i] + ' ')\n",
    "        for j in range(mean.shape[1]):\n",
    "            strs.append('& $')\n",
    "            if i == max_1st[j]: strs.append('\\\\mathbf{')\n",
    "            if i == max_2nd[j]: strs.append('\\\\mathit{')\n",
    "            if (i, j) in skip_indices: strs.append('-')\n",
    "            else: strs.append('%.3f' % mean[i, j] + '\\\\pm' + '%.3f' % stderr[i, j])\n",
    "            if i in [max_1st[j], max_2nd[j]]: strs.append('}')\n",
    "            strs.append('$ ')\n",
    "        strs.append('\\\\\\\\\\n')\n",
    "    strs.append('\\\\hline\\n')\n",
    "    strs.append('\\\\end{tabular}\\n')\n",
    "    strs.append('\\\\end{table*}\\n')\n",
    "    return ''.join(strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_latex_m(mean_list, stderr_list, header_list, title, label):\n",
    "    assert(len(mean_list) == len(stderr_list) == len(header_list))\n",
    "    max_1st_list = []\n",
    "    max_2nd_list = []\n",
    "    for mean, stderr in zip(mean_list, stderr_list):\n",
    "        assert(mean.shape == stderr.shape)\n",
    "        assert(mean.shape == (len(algo), len(dat_name)))\n",
    "        max_1st = np.zeros(len(dat_name), dtype=np.int)\n",
    "        max_2nd = np.zeros(len(dat_name), dtype=np.int)\n",
    "        for j in range(mean.shape[1]):\n",
    "            max_2nd[j], max_1st[j] = np.argsort(mean[:, j])[-2:]\n",
    "        max_1st_list.append(max_1st)\n",
    "        max_2nd_list.append(max_2nd)\n",
    "    \n",
    "    strs = []\n",
    "    strs.append('\\\\begin{table*}[t]\\n')\n",
    "    strs.append('\\\\caption{' + title + '}\\n')\n",
    "    strs.append('\\\\label{' + label + '}\\n')\n",
    "    strs.append('\\\\centering\\n')\n",
    "    strs.append('\\\\begin{tabular}{l|')\n",
    "    for mean in mean_list:\n",
    "        strs.append('|' + (mean.shape[1])*'c')\n",
    "    strs.append('} \\\\hline\\n')\n",
    "    for header in header_list:\n",
    "        strs.append('\\\\multicolumn{%d}{c}{%s} \\\\cline{2-%d}\\n' % (len(dat_name),header,1+len(header_list)*len(dat_name)))\n",
    "    for j in range(len(dat_name))\n",
    "        strs.append(' & ' + dat_name[j])\n",
    "    strs.append(' \\\\\\\\ \\\\hline\\n')\n",
    "    for i in range(mean.shape[0]):\n",
    "        strs.append(algo_name[i] + ' ')\n",
    "        for j in range(mean.shape[1]):\n",
    "            strs.append('& $')\n",
    "            if i == max_1st[j]: strs.append('\\\\mathbf{')\n",
    "            if i == max_2nd[j]: strs.append('\\\\mathit{')\n",
    "            if (i, j) in skip_indices: strs.append('-')\n",
    "            else: strs.append('%.3f' % mean[i, j] + '\\\\pm' + '%.3f' % stderr[i, j])\n",
    "            if i in [max_1st[j], max_2nd[j]]: strs.append('}')\n",
    "            strs.append('$ ')\n",
    "        strs.append('\\\\\\\\\\n')\n",
    "    strs.append('\\\\hline\\n')\n",
    "    strs.append('\\\\end{tabular}\\n')\n",
    "    strs.append('\\\\end{table*}\\n')\n",
    "    return ''.join(strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nb_stdout = sys.stdout  # save the device for notebook output\n",
    "#sys.stdout = open('/dev/stdout', 'w')  # redirect the output of %run to terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics_list = []\n",
    "for dat_ix in range(len(dat_name)):\n",
    "    dat_obj = TrajData(dat_ix)\n",
    "    fnames = build_fnames(dat_obj, dat_ix)\n",
    "    metrics, keys, Cs = calc_metrics(dat_obj, fnames)\n",
    "    metrics_list.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sys.stdout = nb_stdout  # restore the output to notebook\n",
    "#sys.stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics and difference of evaluation metrics between baseline and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_ix = 2\n",
    "baseline_algo = algo_name[baseline_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = ['F$_1$ score on points', 'F$_1$ score on pairs', 'Kendall\\'s $\\\\tau$']\n",
    "labels = ['tab:f1', 'tab:pf1', 'tab:tau']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles_diff = ['Difference of F$_1$ score on points from ' + baseline_algo, \n",
    "               'Difference of F$_1$ score on pairs from ' + baseline_algo, \n",
    "               'Difference of Kendall\\'s $\\\\tau$ from ' + baseline_algo]\n",
    "labels_diff = ['tab:df1', 'tab:dpf1', 'tab:dtau']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for metric_ix in range(len(metric_name)):\n",
    "    mean, stderr = calc_metric_mean(metrics_list, metric_ix)\n",
    "    print(gen_latex_v(mean, stderr, titles[metric_ix], labels[metric_ix]))\n",
    "    #diffmean, diffstderr = calc_metric_diff(metrics_list, metric_ix, baseline_ix=baseline_ix)\n",
    "    #print(gen_latex_v(diffmean, diffstderr, titles_diff[metric_ix], labels_diff[metric_ix]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
