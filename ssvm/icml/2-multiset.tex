%!TEX root = main.tex
\section{Recommending sequences}
\label{sec:recseq}

We consider the problem of given an input query $\x$ -- a user, a location, or some items of interest -- 
recommend one or more sequences $\y$ according to a learned score function $f(\x,\y)$. 
For this work we assume the output is a sequence of $l$ points, or $\y=y_{1:l}$. 
Having this assumption does not limit the generality of the approach, 
as inferring $\y$ of other structure can be achieved by using the corresponding inference and loss-augmented inference algorithms~\cite{joachims2009predicting}.  %LX - this sentence can be cut or merged above
There are two notable differences between the problem of interest -- recommending sequences, and what is being solved 
in structured prediction and recommender systems. 

In structured recommendation, the goal is to learn from 
$(\x\pb{i}, \{ \y\pb{ij} \}_{j=1:n^i})$, $i=1:n$, or a set of output sequences for each input $\x\pb{i}$. 
This setting generalises from structured prediction and recommendation problems in the following ways. 
In a typical structured prediction setting, the goal is to learn from a collection of $n$
input vector and output sequence tuples $(\x\pb{i}, \y\pb{i})$, $i=1:n$. Here, 
for each distinct input $\x\pb{i}$ there is usually one unique output sequence $\y\pb{i}$. 
In a typical recommender systems, however, the typical setting is to learn from 
tuples $(\x\pb{i}, \{ y\pb{ij} \}_{j=1:n^i})$, $i=1:n$. That is to say, for each input $\x\pb{i}$ (\eg users)
have not one, but a set of correct output values $\{ y\pb{ij} \}_{j=1:n^i}$ (\eg movies). 

Such a setting brings unique challenges for both inference and learning. 
For recommender systems, making a prediction involves enumerating all {\em non-structured} items $y$ in order to compute $\argmax_y f(\x,y)$, whereas for structured prediction problems $\argmax_\y f(\x,\y)$ is harder since it is impossible to efficiently enumerate $\y$. For model learning, structured prediction approaches do not have a standard way to take into account multiple output sequences $\{ \y\pb{ij} \}_{j=1:n^i}$ for each input $\x\pb{i}$ yet. 

In the rest of this section, we will first review the background of structured prediction problems (Sec~\ref{ssec:ssvm}), then present a model for structured recommendation (Sec~\ref{ssec:sr}), followed by algorithms for its learning (Sec~\ref{ssec:SRtrain}) and inference (Sec~\ref{ssec:SRinf}).

% In many practical
% problems we may observe more than one label for the same set of features, which violates
% the implicit assumptions of many learning algorithms. In this work we explicitly consider
% all observed labels of a particular example to be useful for training, that is we use
% the multiset of ground truths in training.
% In particular we focus on the structured prediction case,
% where the output of the classifier is from a large set $\mathcal{Y}$ with internal structure.
% An example of this is when $y\in\mathcal{Y}$ is a sequence of binary values.
% Given an example $x_i$ there may be multiple label sequences $y_{ij}$, where $j=1,...,J$.

\eat{
Suggested order:
\begin{enumerate}
  \item structured SVM
  \item multiset SSVM
  \item list Viterbi for multiple ground truths
\end{enumerate}

Then focus on trajectory
\begin{enumerate}
  \item Trajectory recommendation
  \item ILP for subtour elimination
  \item 2 uses of list Viterbi
  \begin{itemize}
    \item multiple ground truths
    \item subtour elimination
  \end{itemize}
\end{enumerate}
}

\subsection{Structured SVM}
\label{ssec:ssvm}

In structured prediction, the output of classifier given feature vector $\x$ is
\begin{equation*}
\y^* = \argmax_{\y \in \mathcal{Y}}~ f(\x, \y),
\end{equation*}
%where $\mathcal{Y}_\mathbf{x}$ is the set of all possible trajectories with POIs in $\mathcal{P}$ and satisfying query $\mathbf{x}$,
where $f(\mathbf{x}, \mathbf{y})$ is a function that scores the compatibility between features $\mathbf{x}$ and a specific label $\mathbf{y}$,
in the case of structured SVM (SSVM), the compatibility function $f(\mathbf{x}, \mathbf{y})$ for structured SVM is this linear form,
\begin{equation*}
f(\mathbf{x}, \mathbf{y}) = \mathbf{w}^\top \Psi(\mathbf{x}, \mathbf{y}),
\end{equation*}
where $\Psi(\mathbf{x}, \mathbf{y})$ is a \emph{joint feature map} 
that captures features extracted from both $\mathbf{x}$ and label $\mathbf{y}$.

The design of joint feature $\Psi(\cdot,\cdot)$ is problem specific, 
for many problems, we can assume the joint feature be decomposed into singleton and pairwise interactions, i.e.,
\begin{equation*}
\label{eq:jointfeature}
\mathbf{w}^\top \Psi(\mathbf{x}, \mathbf{y}) 
= \sum_{j=2}^{| \mathbf{y} |} 
  \left( \mathbf{w}_j^\top \Psi_j(\mathbf{x}, y_j) + 
  \mathbf{w}_{j-1,j}^\top \Psi_{j-1, j}(\mathbf{x}, y_{j-1}, y_j) \right),
\end{equation*}
where $\Psi_j$ is a feature vector of singleton $y_j$ 
and $\Psi_{j-1,j}$ is a pairwise feature vector that captures the interactions between $y_{j-1}$ and POI $y_j$.

To learn the parameters, we train the structured SVM by optimising a quadratic program (QP),
\begin{equation}
\label{eq:nslack}
\begin{aligned}
\min_{\mathbf{w}, \, \bm{\xi} \ge 0} ~& \frac{1}{2} \mathbf{w}^\top \mathbf{w} + \frac{C}{N} \sum_{i=1}^N \xi_i \\
s.t.~ ~& \langle \mathbf{w}, \, \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) - \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) \rangle \ge 
       \Delta(\mathbf{y}^{(i)}, \bar{\mathbf{y}}) - \xi_i, \, \forall i,
\end{aligned}
\end{equation}
where $\bar{\mathbf{y}} \in \mathcal{Y}$ and $\Delta(\mathbf{y}, \bar{\mathbf{y}})$ is a discrepancy function that measures the loss 
for predicting $\bar{\mathbf{y}}$ given ground truth $\mathbf{y}$, 
and slack variable $\xi_i$ is the \emph{hinge loss} for the prediction of the $i$-th example~\cite{tsochantaridis2005large},
\begin{equation*}
\xi_i = \max \left( 0, \,
        \max_{\bar{\mathbf{y}} \in \mathcal{Y}} 
        \left\{ \Delta(\mathbf{y}^{(i)}, \bar{\mathbf{y}}^{(i)}) + \mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) \right\} -
        \mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) \right).
\end{equation*}
%This formulation is called "$n$-slack" as we have one slack variable for each example in training set.

We can rewrite the constraint in problem (\ref{eq:nslack}) as
\begin{equation}
\label{eq:ssvminf}
\mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) + \xi_i \ge
          \max_{\bar{\mathbf{y}} \in \mathcal{Y}}
          \left\{\mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) + \Delta(\mathbf{y}^{(i)}, \bar{\mathbf{y}}) \right\},
\end{equation}
where the right hand side is known as the \emph{loss-augmented inference}.

To solve problem (\ref{eq:nslack}), one option is simply enumerating all constraints, and feeding the problem into a standard QP solver.
However, this approach is impractical as there is a constraint for every possible label $\bar{\mathbf{y}}$.
Instead, we use a cutting-plane algorithm which repeatedly solves QP (\ref{eq:nslack}) 
w.r.t. different set of constraints~\cite{joachims2009predicting}.
In each iteration, a new constraint is formed by solving the loss-augmented inference, 
which helps shrink the feasible region of the problem.


\subsection{SSVM for recommendation}
\label{ssec:sr}

\TODO{talk about ranking objectives}

If we observed more than one labels for a particular set of features, 
the classic SSVM described in Section~\ref{sec:ssvm} can be generalised to capture the multiple ground truths setting,
in particular, given feature vector $\mathbf{x}^{(i)}$ and the corresponding set of ground truths $\{\mathbf{y}^{(ij)}\}_{j=1}^{n_i}$ 
where $n_i$ is the number of labels for $\mathbf{x}_i$,
we can train a multiset SSVM by optimising a QP similar to (\ref{eq:nslack}),
\begin{equation}
\label{eq:nslack_ml}
\begin{aligned}
\min_{\mathbf{w}, \, \bm{\xi} \ge 0} ~& \frac{1}{2} \mathbf{w}^\top \mathbf{w} + \frac{C}{N} \sum_{i,j} \xi_{ij} \\
s.t.~ ~& \langle \mathbf{w}, \, \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(ij)}) - \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) \rangle \ge 
         \Delta(\mathbf{y}^{(ij)}, \bar{\mathbf{y}}) - \xi_{ij}, \, \forall j, \, \forall i.
\end{aligned}
\end{equation}
where $N = \sum_i n_i$ and $\bar{\mathbf{y}} \in \mathcal{Y} \setminus \{\mathbf{y}^{(ij)}\}_{j=1}^{n_i}$.



\subsection{Training SR}
\label{sec:SRtrain}
% list Viterbi for multiple ground truths

\TODO{talk about training and what cutting plane does}

Similar to the loss-augmented inference described in Section~\ref{sec:ssvm}, 
we can rewrite the constraints in problem (\ref{eq:nslack_ml}) into
\begin{equation*}
\mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(ij)}) + \xi_{ij} \ge 
\max_{\bar{\mathbf{y}}} \left( \Delta(\mathbf{y}^{(ij)}, \bar{\mathbf{y}}) + \mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) \right),
\, \forall j.
\end{equation*} 
Normally, the maximisation at the right side of the above inequality can not be solved efficiently due to the constraint that 
$\bar{\mathbf{y}}$ is in $\mathcal{Y}$ but should not be in the set of observed labels $\{\mathbf{y}^{(ij)}\}_{j=1}^{n_i}$.
However, we can pretend that $\bar{\mathbf{y}}$ can be any label in $\mathcal{Y}$ and do the unconstrained optimisation,
which can sometimes be solved efficiently, then we filtering out the optimal solution if it has been observed, 
i.e., in $\{\mathbf{y}^{(ij)}\}_{j=1}^{n_i}$. 
In addition to train multiset SSVM, this technique can be further used to deal with other constraints such as sub-tour elimination 
as described in Section~\ref{sec:subtour}.

\TODO{talk about multiple truths and what it means in prediction}

\subsection{Inference for structured recommendation}
\label{sec:SRinf}

\TODO{recommending lists: talk about listViterbi for inference}
