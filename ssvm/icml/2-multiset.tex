%!TEX root = main.tex
\section{Recommending sequences}
\label{sec:multiset}

We consider the problem of supervised learning with multiple ground truths. In many practical
problems we may observe more than one label for the same set of features, which violates
the implicit assumptions of many learning algorithms. In this work we explicitly consider
all observed labels of a particular example to be useful for training, that is we use
the multiset of ground truths in training.
In particular we focus on the structured prediction case,
where the output of the classifier is from a large set $\mathcal{Y}$ with internal structure.
An example of this is when $y\in\mathcal{Y}$ is a sequence of binary values.
Given an example $x_i$ there may be multiple label sequences $y_{ij}$, where $j=1,...,J$.

\eat{
Suggested order:
\begin{enumerate}
  \item structured SVM
  \item multiset SSVM
  \item list Viterbi for multiple ground truths
\end{enumerate}

Then focus on trajectory
\begin{enumerate}
  \item Trajectory recommendation
  \item ILP for subtour elimination
  \item 2 uses of list Viterbi
  \begin{itemize}
    \item multiple ground truths
    \item subtour elimination
  \end{itemize}
\end{enumerate}
}

\subsection{Structured SVM}
\label{sec:ssvm}

In structured prediction, the output of classifier given feature vector $\mathbf{x}$ is
\begin{equation*}
\mathbf{y}^* = \argmax_{\mathbf{y} \in \mathcal{Y}}~ f(\mathbf{x}, \mathbf{y}),
\end{equation*}
%where $\mathcal{Y}_\mathbf{x}$ is the set of all possible trajectories with POIs in $\mathcal{P}$ and satisfying query $\mathbf{x}$,
where $f(\mathbf{x}, \mathbf{y})$ is a function that scores the compatibility between features $\mathbf{x}$ and a specific label $\mathbf{y}$,
in the case of structured SVM (SSVM), the compatibility function $f(\mathbf{x}, \mathbf{y})$ for structured SVM is this linear form,
\begin{equation*}
f(\mathbf{x}, \mathbf{y}) = \mathbf{w}^\top \Psi(\mathbf{x}, \mathbf{y}),
\end{equation*}
where $\Psi(\mathbf{x}, \mathbf{y})$ is a \emph{joint feature map} 
that captures features extracted from both $\mathbf{x}$ and label $\mathbf{y}$.

The design of joint feature $\Psi(\cdot,\cdot)$ is problem specific, 
for many problems, we can assume the joint feature be decomposed into singleton and pairwise interactions, i.e.,
\begin{equation*}
\label{eq:jointfeature}
\mathbf{w}^\top \Psi(\mathbf{x}, \mathbf{y}) 
= \sum_{j=2}^{| \mathbf{y} |} 
  \left( \mathbf{w}_j^\top \Psi_j(\mathbf{x}, y_j) + 
  \mathbf{w}_{j-1,j}^\top \Psi_{j-1, j}(\mathbf{x}, y_{j-1}, y_j) \right),
\end{equation*}
where $\Psi_j$ is a feature vector of singleton $y_j$ 
and $\Psi_{j-1,j}$ is a pairwise feature vector that captures the interactions between $y_{j-1}$ and POI $y_j$.

To learn the parameters, we train the structured SVM by optimising a quadratic program (QP),
\begin{equation}
\label{eq:nslack}
\begin{aligned}
\min_{\mathbf{w}, \, \bm{\xi} \ge 0} ~& \frac{1}{2} \mathbf{w}^\top \mathbf{w} + \frac{C}{N} \sum_{i=1}^N \xi_i \\
s.t.~ ~& \langle \mathbf{w}, \, \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) - \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) \rangle \ge 
       \Delta(\mathbf{y}^{(i)}, \bar{\mathbf{y}}) - \xi_i, \, \forall i,
\end{aligned}
\end{equation}
where $\bar{\mathbf{y}} \in \mathcal{Y}$ and $\Delta(\mathbf{y}, \bar{\mathbf{y}})$ is a discrepancy function that measures the loss 
for predicting $\bar{\mathbf{y}}$ given ground truth $\mathbf{y}$, 
and slack variable $\xi_i$ is the \emph{hinge loss} for the prediction of the $i$-th example~\cite{tsochantaridis2005large},
\begin{equation*}
\xi_i = \max \left( 0, \,
        \max_{\bar{\mathbf{y}} \in \mathcal{Y}} 
        \left\{ \Delta(\mathbf{y}^{(i)}, \bar{\mathbf{y}}^{(i)}) + \mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) \right\} -
        \mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) \right).
\end{equation*}
%This formulation is called "$n$-slack" as we have one slack variable for each example in training set.

We can rewrite the constraint in problem (\ref{eq:nslack}) as
\begin{equation}
\label{eq:ssvminf}
\mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) + \xi_i \ge
          \max_{\bar{\mathbf{y}} \in \mathcal{Y}}
          \left\{\mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) + \Delta(\mathbf{y}^{(i)}, \bar{\mathbf{y}}) \right\},
\end{equation}
where the right hand side is known as the \emph{loss-augmented inference}.

To solve problem (\ref{eq:nslack}), one option is simply enumerating all constraints, and feeding the problem into a standard QP solver.
However, this approach is impractical as there is a constraint for every possible label $\bar{\mathbf{y}}$.
Instead, we use a cutting-plane algorithm which repeatedly solves QP (\ref{eq:nslack}) 
w.r.t. different set of constraints~\cite{joachims2009predicting}.
In each iteration, a new constraint is formed by solving the loss-augmented inference, 
which helps shrink the feasible region of the problem.


\subsection{Multiset SSVM}
\label{sec:ssvm-ms}

If we observed more than one labels for a particular set of features, 
the classic SSVM described in Section~\ref{sec:ssvm} can be generalised to capture the multiple ground truths setting,
in particular, given feature vector $\mathbf{x}^{(i)}$ and the corresponding set of ground truths $\{\mathbf{y}^{(ij)}\}_{j=1}^{n_i}$ 
where $n_i$ is the number of labels for $\mathbf{x}_i$,
we can train a multiset SSVM by optimising a QP similar to (\ref{eq:nslack}),
\begin{equation}
\label{eq:nslack_ml}
\begin{aligned}
\min_{\mathbf{w}, \, \bm{\xi} \ge 0} ~& \frac{1}{2} \mathbf{w}^\top \mathbf{w} + \frac{C}{N} \sum_{i,j} \xi_{ij} \\
s.t.~ ~& \langle \mathbf{w}, \, \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(ij)}) - \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) \rangle \ge 
         \Delta(\mathbf{y}^{(ij)}, \bar{\mathbf{y}}) - \xi_{ij}, \, \forall j, \, \forall i.
\end{aligned}
\end{equation}
where $N = \sum_i n_i$ and $\bar{\mathbf{y}} \in \mathcal{Y} \setminus \{\mathbf{y}^{(ij)}\}_{j=1}^{n_i}$.



\subsection{Training Multiset SSVM}
\label{sec:train-ssvm-ms}
% list Viterbi for multiple ground truths

Similar to the loss-augmented inference described in Section~\ref{sec:ssvm}, 
we can rewrite the constraints in problem (\ref{eq:nslack_ml}) into
\begin{equation*}
\mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(ij)}) + \xi_{ij} \ge 
\max_{\bar{\mathbf{y}}} \left( \Delta(\mathbf{y}^{(ij)}, \bar{\mathbf{y}}) + \mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) \right),
\, \forall j.
\end{equation*} 
Normally, the maximisation at the right side of the above inequality can not be solved efficiently due to the constraint that 
$\bar{\mathbf{y}}$ is in $\mathcal{Y}$ but should not be in the set of observed labels $\{\mathbf{y}^{(ij)}\}_{j=1}^{n_i}$.
However, we can pretend that $\bar{\mathbf{y}}$ can be any label in $\mathcal{Y}$ and do the unconstrained optimisation,
which can sometimes be solved efficiently, then we filtering out the optimal solution if it has been observed, 
i.e., in $\{\mathbf{y}^{(ij)}\}_{j=1}^{n_i}$. 
In addition to train multiset SSVM, this technique can be further used to deal with other constraints such as sub-tour elimination 
as described in Section~\ref{sec:subtour}.
