\section{Experiment}
\label{sec:experiment}

\subsection{Dataset and Features}
\label{sec:dataset}

% experiment protocol: Nested cross-validation with Monte-Carlo cross-validation for the inner loop
We experiment methods developed in Section~\ref{sec:trajrec} on trajectories extracted from Flickr photos~\cite{thomee2016yfcc100m}.
The statistics of datasets are shown in Table~\ref{tab:data} and 
the histograms of the number of ground truths for queries are shown in Figure~\ref{fig:hist}.

% dataset stats
\begin{table}[t]
\caption{Statistics of trajectory dataset}
\label{tab:data}
\centering
\setlength{\tabcolsep}{4pt} % tweak the space between columns
\small
\begin{tabular}{l*{5}{r}} \hline
\textbf{Dataset} & \textbf{\#Photos} & \textbf{\#Visits} & \textbf{\#Traj.} & \textbf{\#Users} & \textbf{\#Queries} \\ \hline
Edinburgh & 82,060 & 33,944 & 5,028 & 1,454 & 147 \\
Glasgow & 29,019 & 11,434 & 2,227 & 601 & 64 \\
Melbourne & 94,142 & 23,995 & 5,106 & 1,000 & 280 \\
Osaka & 392,420 & 7,747 & 1,115 & 450 & 47 \\
Toronto & 157,505 & 39,419 & 6,057 & 1,395 & 99 \\
\hline
\end{tabular}
\end{table}


% histogram of #ground truth
\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{hist.pdf}
	\caption{Histogram of the number of ground truth}
	\label{fig:hist}
\end{figure*}


% leave-one-out evaluation (with query aggregation)
As described in Section~\ref{sec:queryagg}, 
we first group trajectories according to queries that they conform to,
then evaluate the performance of each algorithm using leave-one-out cross validation,
where we hold each query and its associated trajectories for test and all other trajectories for training.
% model selection (Monte Carlo CV) (with query aggregation): 90/10 random split for 5 times
Furthermore, 
the hyper-parameter (regularisation constant $C$) is tuned using the Monte Carlo cross validation~\cite{burman1989comparative} on training set.

% evaluation metric: kendall's Tau (mention F1, pF1)
To evaluate the performance of each algorithm variant, 
we compute the Kendall's $\tau$ (version $b$)~\cite{kendall1945,agresti2010analysis} to measure the quality of recommendation.
In particular,
we define the rank of POIs $\mathcal{P}$ given trajectory $\mathbf{y} = (y_1,\dots,y_K)$ as
\begin{align*} 
r_\mathbf{y} &= (r_1,\dots,r_j,\dots,r_{|\mathcal{P}|}), \\
r_j &= \sum_{k=1}^K (| \mathcal{P} | - k + 1)  \llb p_j = y_k \rrb, ~ j = 1, \dots, | \mathcal{P} |.
\end{align*}
Unlike the F$_1$ sore on points~\cite{ijcai15} or the F$_1$ score on pairs~\cite{cikm16paper}, 
which only cares about either the set of correctly recommended POIs or the set of correctly predicted POI pairs,
this metric taking both factors into account.

In addition, we take the maximum of all pairs,
i.e.,
\begin{equation*}
\tau_b^{(i)} = 
\max_{(\mathbf{y}, \hat{\mathbf{y}}) \in \{\mathbf{y}^{(ij)}\}_{j=1}^{N_i} \times \{\hat{\mathbf{y}}^{(ij)}\}_{j=1}^k} 
\tau_b(r_\mathbf{y}, r_{\hat{\mathbf{y}}}),
\end{equation*}
where $\{\mathbf{y}^{(ij)}\}_{j=1}^{N_i}$ are the ground truths for query $\mathbf{x}^{(i)}$ and
$\{\hat{\mathbf{y}}^{(ij)}\}_{j=1}^k$ are the top-$k$ recommendations.



\subsection{Experimental Results}
\label{sec:result}

\input{tab_experiment}

% experimental results
The performance of three baselines and four variants based on structured prediction on two datasets are shown in Table~\ref{tab:result}.
The \textsc{Random} baseline simply keep recommending a POI sampled uniformly at random from the whole set of POIs (without replacement),
until we get all the specified number of POIs.
On the other hand, \textsc{Popularity} is slightly smarter which considers the popularity of each POI, 
and recommend the top-$k$ most popular places.
\textsc{PoiRank}~\cite{cikm16paper} is a generalisation of \textsc{Popularity} which consider a number of POI features in addition to its popularity.

We can see from the empirical results that all variants of structured prediction achieve very good performance.
In particular, \textsc{SR} always performs better than \textsc{SP}, 
which suggests that explicitly modelling multiple ground truths helps recommendation.

Furthermore, we observed that sub-tour elimination in training improves the ordering of recommended trajectories, 
as indicated by the F$_1$ score on pairs, 
however, this advantage will not take effect if the multiple ground truths is not modelled explicitly,
which further enforces its importance.
