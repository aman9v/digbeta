%!TEX root = main.tex

% sub-tour elimination
\subsection{Sequence decoding for the SR model}
\label{ssec:subtour}

The SR model require two algorithmic components for inference and learning that is missing from the SSVM algorithms. 

For inference, SR needs to predict a {\em path}, \ie a sequence whose elements are distinct from each other. 
As described in Section~\ref{sec:intro}, this is desirable since users traversing a sequence (of locations or music)
would not want to see repeated entries. 
Given desired sequence length $l$ among $L$ possible points, dynamic programming~\cite{tsochantaridis2005large} 
will generate the length-$l$ sequence with the best score, i.e. $\y^* = \argmax_\y f(\x,\y)$. 
One may then use the well-known 
Christofes algorithm~\cite{christofides1976} on $y^*$ to eliminate loops in the sequence. 
%is known to generate a solutions within a factor of 3/2 of the optimal solution for traveling salesman problems. 
However, the resulting sequence will have less than the desired number of points, and the resulting score won't be optimal. 

For learning an SR model, loss-augmented inference needs to be done by excluding multiple known sequences. 
As described in Section~\ref{ssec:sr}, this involves %we would want to maximize the loss-augmented objective function 
$\max_{\bar\y} \left\{ \w^\top \Psi(\x^{(i)}, \bar\y) + \Delta(\y^{(ij)}, \bar{\y}) \right\}$
where the domain of candidate sequences excludes the known sequences for query $\x^{(i)}$, \ie $\bar{\y} \in \mathcal{Y} \setminus \{\y^{(ij)}\}_{j=1}^{n_i}$. \TODO{should this be in Sec 3.2?}
Note that the dynamic programming~\cite{tsochantaridis2005large} algorithm use back-tracking to find the best sequence, 
and cannot easily exclude known sequences. 

%Both the loss-augmented inference and prediction inference for structured SVM (Section~\ref{sec:ssvm}) cannot be done efficiently 
%if the no duplicates constraints are required, moreover, we would like to recommend more than one trajectories given a query.

The rest of this section describes two algorithms, each intuitively aimed to address one of the two requirements above. 
Both can be applied in novel contexts of the SR model. 
We will also describe practical choices about which algorithm to use when. 


\subsubsection{Traversing points with integer linear programming}
% ILP for subtour elimination
Inference in the SR model requires finding the best path that traverses exactly $l$ of $L$ candidate points. 
This is can be seen as a variant of the traveling salesman problem (TSP), or the best of $C_L^l$ TSPs.
Such a point traversal problem can be solved by incorporating 
sub-tour elimination constraints of the travelling salesman problem (TSP).
In particular, the following integer linear program (ILP) formulation~\cite{ijcai15,cikm16paper} 
will solve SR inference for trajectory $\y$ of length $l$. 
\resizebox{.99\columnwidth}{!}{
  \begin{minipage}{\linewidth}
\begin{alignat}{5}
\max_{\bu,\bv} ~& \sum_{k=1}^l \w_k^\top \Psi_k(\x, y_k) \sum_{j=1}^l u_{jk} \nonumber\\
                & + \sum_{j=1}^l \sum_{k=1}^l u_{jk} \w_{jk}^\top \Psi_{j, k}(\x, y_j, y_k) \\
s.t. ~& u_{jk}, ~z_j \in \{0, 1\}, ~u_{jj}=0, ~z_1=0, ~v_j \in \mathbf{Z},~ \\
  & p_j \in \mathcal{P}, ~\forall j, k = 1,\cdots,M   \label{eq:cons1} \\
  & \sum_{k=2}^M u_{1k} = 1, ~\sum_{j=2}^M u_{j1} = 0  \label{eq:cons2} \\
  & \sum_{j=1}^M u_{jl} = z_l + \sum_{k=2}^M u_{lk} \le 1,   ~\forall l=2,\cdots,M                    \label{eq:cons3} \\
  & \sum_{j=1}^M \sum_{k=1}^M u_{jk} = L-1,                                                           \label{eq:cons4} \\
  & v_j - v_k + 1 \le (M-1) (1-u_{jk}),                     \forall j,k=2,\cdots,M                    \label{eq:cons5}
\end{alignat}
\end{minipage}
}
Here $u_{jk}$ are binary decision variables that determines whether the transition from $y_j$ to $y_k$ is in the resulting trajectory,
$z_j$ is a binary decision variable that determines whether $p_j$ is the last POI in trajectory.
$L$ is the number of POIs in trajectory.
For brevity, we arrange the POIs such that $p_1 = s$.
Firstly, the desired trajectory should start from $s$ (Constraint~\ref{eq:cons2}).
In addition, any POI could be visited at most once (Constraint~\ref{eq:cons3}).
Moreover, only $L-1$ transitions between POIs are permitted (Constraint~\ref{eq:cons4}),
i.e., the number of POI visits should be exactly $L$ (including $s$).
The last constraint, where $v_i$ is an auxiliary variable,
enforces that only a single sequence of POIs without sub-tours is permitted in the trajectory.

If we employ the above ILP to do loss-augmented inference, one restriction is that the loss should be a linear function of $u_{jk}$,
e.g., $\Delta(\mathbf{y}, \bar{\mathbf{y}}) = 1 - \sum_{j=1}^M \sum_{k=1}^M u_{j, y_k}$ if we define the loss as the number of mispredicted POIs,
where $\mathbf{y}$ is the ground truth and $\bar{\mathbf{y}}$ is the trajectory corresponding to the optimal solution of this ILP.

\subsubsection{Finding $k$-best sequences}
% 2 uses of list Viterbi: 1) multiple ground truths; 2) subtour elimination
To achieve this, we resort to a variant of the list Viterbi algorithm~\cite{nilsson2001sequentially,seshadri1994list}
which sequentially find the $k$-th best (scored) trajectory given the best, $2$nd best, \dots, $(k-1)$-th best (scored) trajectories,
as described in Algorithm~\ref{alg:listviterbi}.

\TODO{Briefly discuss which one to use, when}