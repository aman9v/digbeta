% !TEX root = ./main.tex

\section{Trajectory Recommendation as a Structured Prediction Problem}
\label{sec:trajrec}

Travel recommendation problems involve a set of points-of-interest (POIs) $\mathcal{P}$ in a city.
The \emph{trajectory recommendation} problem is: given a \emph{trajectory query} $\mathbf{x} = (s, K)$,
comprising a start POI $s \in \mathcal{P}$ and trip length\footnote{Instead of specifying the number of desired POIs, we can constrain the trajectory with a total time budget $T$.
In this case, the number of POIs $K$ can be treated as a \emph{hidden} variable, with additional constraint $\sum_{k=1}^K t_k \le T$
where $t_k$ is the time spent at POI $y_k$.} $K > 1$ (including the start location $s$),
we want to recommend a sequence of POIs $\mathbf{y}^*$ that maximises some notion of utility.
That is, for a suitable function $f(\cdot,\cdot)$, we wish to find
\begin{equation*}
\mathbf{y}^* = \argmax_{\mathbf{y} \in \mathcal{Y}}~f(\mathbf{x}, \mathbf{y}),
\end{equation*}
where $\mathcal{Y}$ is the set of all possible trajectories with POIs in $\mathcal{P}$ that conform to the constraints imposed by the query $\mathbf{x}$.
In particular,
$\mathbf{y} = (y_1 = s,~ y_2, \dots, y_K)$ is a trajectory with $K$ POIs, which has no sub-tours i.e. $y_j \ne y_k$ if $j \ne k$.

% AKM: not sure these are suitable here
%
% This problem is related to automatic playlist generation,
% where we recommend a sequence of songs given a specified song (a.k.a. the seed) and the number of new songs.
% Formally, given a library of songs and a query $\mathbf{x} = (s, K)$, where $s$ is the seed and $K$ is the number of songs in playlist,
% we produce a list with $K$ songs (without duplication) by maximising the likelihood~\cite{chen2012playlist},
% \begin{equation*}
% %\max_{(y_1,\dots,y_K)} \prod_{k=2}^K \mathbb{P}(y_{k-1} \given y_k),~ y_1 = s ~\text{and}~ y_j \ne y_k,~ j \ne k.
% \mathbf{y}^* = \argmax_{\mathbf{y} \in \mathcal{P}_\mathbf{x}}~ \mathbb{P}(\mathbf{y} \given \mathbf{x}),~ \mathbf{y} = (y_1=s,\dots,y_K)
% ~\text{and}~ y_j \ne y_k ~\text{if}~ j \ne k.
% \end{equation*}

% Another similar problem is choosing a small set of photos from a large photo library and compiling them into a slideshow or movie.

To learn a suitable $f$ for trajectory recommendation,
we are provided as input a training set
%$(\x\pb{i}, \{ \y\pb{ij} \}_{j=1:n^i})$, $i=1:n$,
$\{ ( \x\pb{i}, \{ \y\pb{ij} \}_{j=1}^{n^i} ) \}_{i=1}^{n}$.
Here, each $\x\pb{i}$ is a distinct query,
while the corresponding $\{ \y\pb{ij} \}_{j=1}^{n^i}$ is the set of trajectories observed for that query.
Note that we expect most queries to have several distinct trajectories;
minimally,
for example,
there may two nearby POIs that are visited in interchangeable order by different travellers.

Comparing the above to the discussion in the previous section, it is clear that
we can cast trajectory recommendation as a special case of the structured recommendation problem.
Consequently, we may approach it using structured prediction methods such as the SSVM,
as well as the extensions proposed to account for multiple ground truths and eliminate loops during training.

% AKM: repetitive
%
% standard SSVM
% We can learn a recommender by training a SSVM on the set of observed trajectories $\{\mathbf{x}^{(i')}, \mathbf{y}^{(i')}\}_{i'=1}^{N'}$,
% However, we ignore the fact that for the same query, we normally observed more than one trajectory,
% we would like to exploit this fact to better modelling the observed trajectories.

% AKM: repetitive
%
% \subsection{Query Aggregation}
% \label{sec:query}

% To modelling the fact that a given query has multiple observed trajectories, 
% we firstly group trajectories according to queries, in other words,
% we now have a dataset $\{\mathbf{x}^{(i)}, \{\mathbf{y}^{(ij)}\}_{j=1}^{N_i}\}_{i=1}^N$
% with $N$ queries and queries $\mathbf{x}^{(i)}$ has $N_i$ trajectories observed.


% \subsection{Recommendation with Multiset SSVM}
% \label{sec:trajrec-ssvm}

% We can learn to recommend trajectories by training a multiset SSVM described in Section~\ref{sec:ssvm-ms}

% multiset SSVM

% multiset SSVM: training, prediction

% sub-tour elimination
\section{Sub-tour Elimination}
\label{sec:subtour}

Both the loss-augmented inference and prediction inference for structured SVM (Section~\ref{sec:ssvm}) cannot be done efficiently 
if the no duplicates constraints are required, moreover, we would like to recommend more than one trajectories given a query.
To achieve this, we resort to a variant of the list Viterbi algorithm~\cite{nilsson2001sequentially,seshadri1994list}
which sequentially find the $k$-th best (scored) trajectory given the best, $2$nd best, \dots, $(k-1)$-th best (scored) trajectories,
as described in Algorithm~\ref{alg:listviterbi}.


% ILP for subtour elimination
The no duplicates constraint can be achieved by adapting the sub-tour elimination constraints of the travelling salesman problem (TSP),
in particular, we solve the following integer linear program (ILP),
\begin{alignat}{5}
& \max_{u,v} ~&& \sum_{k=1}^M \mathbf{w}_k^\top \phi_k(\mathbf{x}, p_k) \sum_{j=1}^M u_{jk} + 
                 \sum_{j=1}^M \sum_{k=1}^M u_{jk} \mathbf{w}_{jk}^\top \phi_{j, k}(\mathbf{x}, p_j, p_k) \\
& s.t. ~~ ~&& u_{jk}, ~z_j \in \{0, 1\}, ~u_{jj}=0, ~z_1=0, ~v_j \in \mathbf{Z},~ p_j \in \mathcal{P}, ~\forall j, k = 1,\cdots,M   \label{eq:cons1} \\
&          && \sum_{k=2}^M u_{1k} = 1, ~\sum_{j=2}^M u_{j1} = 0  \label{eq:cons2} \\
&          && \sum_{j=1}^M u_{jl} = z_l + \sum_{k=2}^M u_{lk} \le 1,   ~\forall l=2,\cdots,M                    \label{eq:cons3} \\
&          && \sum_{j=1}^M \sum_{k=1}^M u_{jk} = L-1,                                                           \label{eq:cons4} \\
&          && v_j - v_k + 1 \le (M-1) (1-u_{jk}),                     \forall j,k=2,\cdots,M                    \label{eq:cons5}
\end{alignat}
where $u_{jk}$ is a binary decision variable that determines whether the transition from $p_j$ to $p_k$ is in the resulting trajectory,
$z_j$ is a binary decision variable that determines whether $p_j$ is the last POI in trajectory.
$L$ is the number of POIs in trajectory.
For brevity, we arrange the POIs such that $p_1 = s$.
Firstly, the desired trajectory should start from $s$ (Constraint~\ref{eq:cons2}).
In addition, any POI could be visited at most once (Constraint~\ref{eq:cons3}).
Moreover, only $L-1$ transitions between POIs are permitted (Constraint~\ref{eq:cons4}),
i.e., the number of POI visits should be exactly $L$ (including $s$).
The last constraint, where $v_i$ is an auxiliary variable,
enforces that only a single sequence of POIs without sub-tours is permitted in the trajectory.

If we employ the above ILP to do loss-augmented inference, one restriction is that the loss should be a linear function of $u_{jk}$,
e.g., $\Delta(\mathbf{y}, \bar{\mathbf{y}}) = 1 - \sum_{j=1}^M \sum_{k=1}^M u_{j, y_k}$ if we define the loss as the number of mispredicted POIs,
where $\mathbf{y}$ is the ground truth and $\bar{\mathbf{y}}$ is the trajectory corresponding to the optimal solution of this ILP.


% 2 uses of list Viterbi: 1) multiple ground truths; 2) subtour elimination
