%!TEX root = main.tex
\section{Recommending sequences}
\label{sec:recseq}

We consider the problem of given an input query $\x$ -- a user, a location, or some items of interest -- 
recommend one or more sequences $\y$ according to a learned score function $f(\x,\y)$. 
For this work we assume the output $\y$ is a sequence of $l$ points, or $y_{1:l}$. 
Having this assumption does not limit the generality of the approach, 
as inferring $\y$ of other structure can be achieved by using the corresponding inference and loss-augmented inference algorithms~\cite{joachims2009predicting}.  %LX - this sentence can be cut or merged above
There are two notable differences between the problem of interest -- recommending sequences, and what is being solved 
in structured prediction and recommender systems. 

In structured recommendation, the goal is to learn from 
$(\x\pb{i}, \{ \y\pb{ij} \}_{j=1:n^i})$, $i=1:n$, or a set of output sequences for each input $\x\pb{i}$. 
This setting generalises from structured prediction and recommendation problems in the following ways. 
In a typical structured prediction setting, the goal is to learn from a collection of $n$
input vector and output sequence tuples $(\x\pb{i}, \y\pb{i})$, $i=1:n$. Here, 
for each distinct input $\x\pb{i}$ there is usually one unique output sequence $\y\pb{i}$. 
In a typical recommender systems, however, the typical setting is to learn from 
tuples $(\x\pb{i}, \{ y\pb{ij} \}_{j=1:n^i})$, $i=1:n$. That is to say, for each input $\x\pb{i}$ (\eg users)
have not one, but a set of correct output values $\{ y\pb{ij} \}_{j=1:n^i}$ (\eg movies). 

Such a setting brings unique challenges for both inference and learning. 
For recommender systems, making a prediction involves enumerating all {\em non-structured} items $y$ in order to compute $\argmax_y f(\x,y)$, whereas for structured prediction problems $\argmax_\y f(\x,\y)$ is harder since it is impossible to efficiently enumerate $\y$. For model learning, structured prediction approaches do not have a standard way to take into account multiple output sequences $\{ \y\pb{ij} \}_{j=1:n^i}$ for each input $\x\pb{i}$ yet. 

In the rest of this section, we will first review the background of structured prediction problems (Sec~\ref{ssec:ssvm}), then present a model for structured recommendation (Sec~\ref{ssec:sr}), followed by algorithms for its learning (Sec~\ref{ssec:SRtrain}) and inference (Sec~\ref{ssec:SRinf}).

% In many practical
% problems we may observe more than one label for the same set of features, which violates
% the implicit assumptions of many learning algorithms. In this work we explicitly consider
% all observed labels of a particular example to be useful for training, that is we use
% the multiset of ground truths in training.
% In particular we focus on the structured prediction case,
% where the output of the classifier is from a large set $\mathcal{Y}$ with internal structure.
% An example of this is when $y\in\mathcal{Y}$ is a sequence of binary values.
% Given an example $x_i$ there may be multiple label sequences $y_{ij}$, where $j=1,...,J$.

\eat{
Suggested order:
\begin{enumerate}
  \item structured SVM
  \item multiset SSVM
  \item list Viterbi for multiple ground truths
\end{enumerate}

Then focus on trajectory
\begin{enumerate}
  \item Trajectory recommendation
  \item ILP for subtour elimination
  \item 2 uses of list Viterbi
  \begin{itemize}
    \item multiple ground truths
    \item subtour elimination
  \end{itemize}
\end{enumerate}
}

\subsection{Preliminary: Structured SVM}
\label{ssec:ssvm}

%In structured prediction, the output of classifier given feature vector $\x$ is
%\begin{equation*}
%\y^* = \argmax_{\y \in \mathcal{Y}}~ f(\x, \y),
%\end{equation*}
%where $\mathcal{Y}_\mathbf{x}$ is the set of all possible trajectories with POIs in $\mathcal{P}$ and satisfying query $\mathbf{x}$,
One well known model for structured prediction is the Structured Support Vector Machines (SSVM)~\cite{joachims2009predicting,tsochantaridis2005large}. 
Here score function $f(\x, \y)$ take a linear form:
%is a function that scores the compatibility between features $\mathbf{x}$ and a specific label $\mathbf{y}$,
%in the case of structured SVM (SSVM), the compatibility function $f(\mathbf{x}, \mathbf{y})$ for structured SVM is this linear form,
\begin{equation*}
f(\x, \y) = \w^\top \Psi(\x, \y),
\end{equation*}
where $\w$ is a weight vector, and $\Psi(\mathbf{x}, \mathbf{y})$ is a \emph{joint feature map} 
between the input $\x$ and label sequence $\y$.

The design of the feature map $\Psi(\cdot,\cdot)$ is problem specific. 
For many problems, we can assume that it is a vector whose elements represent unary 
 terms for each element in the label $y_{1:l}$ and pairwise interactions between the labels. 
 For sequence data, in particular, we also assume that the pair-wise interactions are between 
 adjacent elements, i.e. $y_j$ and $y_{j+1}$ for $j=1:l-1$. 
 Subsequently, the score function $f(\x,\y)$ decomposes into a sum of 
 each of these unary and pairwise features with the corresponding feature weight.  
\begin{align}
\label{eq:jointfeature}
f(\x, \y) =  %\w^\top \Psi(\x,\y) 
\sum_{j=1}^l \w_j^\top \Psi_j(\x, y_j)  
  + \sum_{j=1}^{l-1} \w_{j,j+1}^\top \Psi_{j,j+1}(\x, y_{j}, y_{j+1}) %\nonumber
\end{align}
here $\Psi_j$ is a feature map between the input and one output label element $y_j$, with a corresponding weight vector $\w_j$
and $\Psi_{j,j+1}$ is a pairwise feature vector that captures the interactions between consecutive labels $y_j$ $y_{j-1}$, 
with a corresponding weight vector $\w_{j,j+1}$.

%To learn the parameters, we train the structured SVM by optimising a quadratic program (QP),
The learning for structured SVM solves the following optimisation problem.
\begin{align}
&\min_{\x, \, \bm{\xi} \ge 0} \frac{1}{2} \x^\top \x + \frac{C}{n} \sum_{i=1}^n \xi_i \label{eq:nslack}\\
&s.t.~  \forall i, \nonumber\\
&\w^\top \Psi(\x^{(i)}, \y^{(i)}) + \xi_i \ge
          \max_{\bar{\y} \in \mathcal{Y}}
          \left\{\w^\top \Psi(\x^{(i)}, \bar{\y}) + \Delta(\y^{(i)}, \bar{\y}) \right\} \nonumber
%\langle \mathbf{w}, \, \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) - \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) \rangle \ge 
%       \Delta(\mathbf{y}^{(i)}, \bar{\mathbf{y}}) - \xi_i, 
\end{align}

where $\bar\y$ is a candidate sequence,  % \in \mathcal{Y} -- LX: what is cal Y anyway?
and $\Delta(\y, \bar\y)$ is the loss function between $\bar\y$ and the ground truth $\y$. 
Here the right hand side is known as the \emph{loss-augmented inference}. 
Loss-augmented inference can be efficiently done if loss function $\Delta(,)$ is also decomposable 
with respect to individual and pairs of label elements in a way similar to Equation~\eqref{eq:jointfeature}.
%%LX: we don't need the def of \xi_i below?
%and slack variable $\xi_i$ is the \emph{hinge loss} for the prediction of the $i$-th example~\cite{tsochantaridis2005large},
%\begin{equation*}
%\xi_i = \max \left( 0, \,
%        \max_{\bar{\mathbf{y}} \in \mathcal{Y}} 
%        \left\{ \Delta(\mathbf{y}^{(i)}, \bar{\mathbf{y}}^{(i)}) + \mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) %\right\} -
%        \mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) \right).
%\end{equation*}
%This formulation is called "$n$-slack" as we have one slack variable for each example in training set.

%%LX: merge above, no need to write out the constraint twice
% We can rewrite the constraint in problem (\ref{eq:nslack}) as
% \begin{equation}
% \label{eq:ssvminf}
% \mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) + \xi_i \ge
%           \max_{\bar{\mathbf{y}} \in \mathcal{Y}}
%           \left\{\mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) + \Delta(\mathbf{y}^{(i)}, \bar{\mathbf{y}}) \right\},
% \end{equation}


To solve problem (\ref{eq:nslack}), one option is simply enumerating all constraints, and feeding the problem into a standard solver.
However, this approach is impractical as there is a constraint for every possible label $\bar{\mathbf{y}}$.
Instead, we use a cutting-plane algorithm which repeatedly solves the quadratic program (\ref{eq:nslack}) 
with a growing set of constraints~\cite{joachims2009predicting}.
In each iteration, a new constraint is formed by solving the loss-augmented inference, 
which helps shrink the feasible region of the problem.


\subsection{SSVM for recommendation}
\label{ssec:sr}

\TODO{talk about ranking objectives}

If we observed more than one labels for a particular set of features, 
the classic SSVM described in Section~\ref{ssec:ssvm} can be generalised to capture the multiple ground truths setting,
in particular, given feature vector $\mathbf{x}^{(i)}$ and the corresponding set of ground truths $\{\mathbf{y}^{(ij)}\}_{j=1}^{n_i}$ 
where $n_i$ is the number of labels for $\mathbf{x}_i$,
we can train a multiset SSVM by optimising a QP similar to (\ref{eq:nslack}),
\begin{equation}
\label{eq:nslack_ml}
\begin{aligned}
\min_{\mathbf{w}, \, \bm{\xi} \ge 0} ~& \frac{1}{2} \mathbf{w}^\top \mathbf{w} + \frac{C}{N} \sum_{i,j} \xi_{ij} \\
s.t.~ ~& \langle \mathbf{w}, \, \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(ij)}) - \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) \rangle \ge 
         \Delta(\mathbf{y}^{(ij)}, \bar{\mathbf{y}}) - \xi_{ij}, \, \forall j, \, \forall i.
\end{aligned}
\end{equation}
where $N = \sum_i n_i$ and $\bar{\mathbf{y}} \in \mathcal{Y} \setminus \{\mathbf{y}^{(ij)}\}_{j=1}^{n_i}$.



\subsection{Training SR}
\label{ssec:SRtrain}
% list Viterbi for multiple ground truths

\TODO{talk about training and what cutting plane does}

Similar to the loss-augmented inference described in Section~\ref{ssec:ssvm}, 
we can rewrite the constraints in problem (\ref{eq:nslack_ml}) into
\begin{equation*}
\mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(ij)}) + \xi_{ij} \ge 
\max_{\bar{\mathbf{y}}} \left( \Delta(\mathbf{y}^{(ij)}, \bar{\mathbf{y}}) + \mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) \right),
\, \forall j.
\end{equation*} 
Normally, the maximisation at the right side of the above inequality can not be solved efficiently due to the constraint that 
$\bar{\mathbf{y}}$ is in $\mathcal{Y}$ but should not be in the set of observed labels $\{\mathbf{y}^{(ij)}\}_{j=1}^{n_i}$.
However, we can pretend that $\bar{\mathbf{y}}$ can be any label in $\mathcal{Y}$ and do the unconstrained optimisation,
which can sometimes be solved efficiently, then we filtering out the optimal solution if it has been observed, 
i.e., in $\{\mathbf{y}^{(ij)}\}_{j=1}^{n_i}$. 
In addition to train multiset SSVM, this technique can be further used to deal with other constraints such as sub-tour elimination 
as described in Section~\ref{sec:subtour}.

\TODO{talk about multiple truths and what it means in prediction}

\subsection{Inference for structured recommendation}
\label{ssec:SRinf}

\TODO{recommending lists: talk about listViterbi for inference}
