%!TEX root = main.tex
%\section{Recommending sequences}
\section{The sequence recommendation problem}
\label{sec:recseq}

Consider the following abstract
\emph{structured recommendation} problem:
given an input query $\x \in \mathcal{X}$ -- representing for example a user, a location, or some ``seed'' item --
we wish to recommend one or more \emph{structured outputs} $\y \in \mathcal{Y}$ according to a learned \emph{score function} $f(\x,\y)$. 
To learn a suitable $f$,
we are provided as input a training set
%$(\x\pb{i}, \{ \y\pb{ij} \}_{j=1:n^i})$, $i=1:n$,
$\{ ( \x\pb{i}, \{ \y\pb{ij} \}_{j=1}^{n^i} ) \}_{i=1}^{n}$,
comprising a collection of inputs $\x\pb{i}$ with an associated \emph{set} of output structures $\{ \y\pb{ij} \}$.
As an example, this might represent a collection of users in a city, along with a set of trajectories that the user has visited.

For this work, we assume the output $\y$ is a \emph{sequence} of $l$ points, denoted $y_{1:l}$
where each $y_i$ belongs to some fixed set (e.g.\ places of interest in a city).
We call the resulting specialisation the \emph{sequence recommendation} problem,
and this shall be our primary interest in this paper.
The assumption that $\y$ is a sequence does not limit the generality of our approach, 
as inferring $\y$ of other structure can be achieved using corresponding inference and loss-augmented inference algorithms~\cite{joachims2009predicting}.  %LX - this sentence can be cut or merged above

There are notable differences between the sequence recommendation problem and %what is being solved in
the standard problems considered in structured prediction and recommender systems. 
%This setting generalises from structured prediction and recommendation problems in the following ways. 
These differences bring unique challenges for both inference and learning. 
In a typical structured prediction setting, the goal is to learn from a collection of $n$
input vector and output sequence tuples %$(\x\pb{i}, \y\pb{i})$, $i=1:n$.
$\{ (\x\pb{i}, \y\pb{i}) \}_{i = 1}^n$. Here, 
for each distinct input $\x\pb{i}$ there is usually one \emph{unique} output sequence $\y\pb{i}$. 
In a typical sequence recommendation problem, however, we expect that %learn from 
%tuples $(\x\pb{i}, \{ y\pb{ij} \}_{j=1:n^i})$, $i=1:n$. That is to say,
for each input $\x\pb{i}$ (\eg users),
there %is %have not one, but a set of
are multiple associated outputs %$\{ y\pb{ij} \}_{j=1:n^i}$ (\eg movies). 
$\{ \y\pb{ij} \}_{j=1}^{n^i}$ (\eg trajectories they have visited).
Indeed, the existence of multiple outputs is the basis on which even non-structured recommendation systems are built, as one looks to exploit signal embedded in the aggregate information.
For model learning, structured prediction approaches do not have a standard way to take into account multiple output sequences %$\{ \y\pb{ij} \}_{j=1:n^i}$
for each input %$\x\pb{i}$
yet. 

On the other hand, for typical recommender systems problems, one assumes that the outputs are non-structured (\eg real-valued ratings for movies).
Thus, making a prediction involves enumerating all {\em non-structured} items $y$ in order to compute $\argmax_y f(\x,y)$.
For structured recommendation problems, computing $\argmax_\y f(\x,\y)$ is harder since it is often impossible to efficiently enumerate $\y$ (\eg all possible trajectories in a city).

In the rest of this section, we will first review the background of structured prediction problems (Sec~\ref{ssec:ssvm}), then present a model for structured recommendation (Sec~\ref{ssec:sr}), followed by algorithms for its learning (Sec~\ref{ssec:SRtrain}) and inference (Sec~\ref{ssec:SRinf}).

% In many practical
% problems we may observe more than one label for the same set of features, which violates
% the implicit assumptions of many learning algorithms. In this work we explicitly consider
% all observed labels of a particular example to be useful for training, that is we use
% the multiset of ground truths in training.
% In particular we focus on the structured prediction case,
% where the output of the classifier is from a large set $\mathcal{Y}$ with internal structure.
% An example of this is when $y\in\mathcal{Y}$ is a sequence of binary values.
% Given an example $x_i$ there may be multiple label sequences $y_{ij}$, where $j=1,...,J$.

\eat{
Suggested order:
\begin{enumerate}
  \item structured SVM
  \item multiset SSVM
  \item list Viterbi for multiple ground truths
\end{enumerate}

Then focus on trajectory
\begin{enumerate}
  \item Trajectory recommendation
  \item ILP for subtour elimination
  \item 2 uses of list Viterbi
  \begin{itemize}
    \item multiple ground truths
    \item subtour elimination
  \end{itemize}
\end{enumerate}
}

\subsection{Preliminaries: structured SVMs}
\label{ssec:ssvm}

%In structured prediction, the output of classifier given feature vector $\x$ is
%\begin{equation*}
%\y^* = \argmax_{\y \in \mathcal{Y}}~ f(\x, \y),
%\end{equation*}
%where $\mathcal{Y}_\mathbf{x}$ is the set of all possible trajectories with POIs in $\mathcal{P}$ and satisfying query $\mathbf{x}$,
One well known model for structured prediction is the Structured Support Vector Machines (SSVM)~\cite{joachims2009predicting,tsochantaridis2005large}. 
This comprises three essential ingredients.

\emph{Score function}. In SSVMs, we specify that the score function $f(\x, \y)$ takes a linear form:
%is a function that scores the compatibility between features $\mathbf{x}$ and a specific label $\mathbf{y}$,
%in the case of structured SVM (SSVM), the compatibility function $f(\mathbf{x}, \mathbf{y})$ for structured SVM is this linear form,
\begin{equation*}
f(\x, \y) = \w^\top \Psi(\x, \y),
\end{equation*}
where $\w$ is a weight vector, and $\Psi(\mathbf{x}, \mathbf{y})$ is a \emph{joint feature map} 
between the input $\x$ and label sequence $\y$.
The design of the feature map $\Psi(\cdot,\cdot)$ is problem specific. 
For many problems, we can assume that it is a vector whose elements represent unary 
 terms for each element in the label $y_{1:l}$, and pairwise interactions between the labels. 
 For sequence data, in particular, we also assume that the pair-wise interactions are between 
 adjacent elements, i.e. $y_j$ and $y_{j+1}$ for $j=1:l-1$. 
 Subsequently, the score function $f(\x,\y)$ decomposes into a sum of 
 each of these unary and pairwise features with the corresponding feature weight:
\begin{equation}
\label{eq:jointfeature}
\resizebox{0.99\linewidth}{!}{$\displaystyle
f(\x, \y) =  %\w^\top \Psi(\x,\y) 
\sum_{j=1}^l \w_j^\top \Psi_j(\x, y_j)  
  + \sum_{j=1}^{l-1} \w_{j,j+1}^\top \Psi_{j,j+1}(\x, y_{j}, y_{j+1}). %\nonumber
  $}
\end{equation}
Here, $\Psi_j$ is a feature map between the input and one output label element $y_j$, with a corresponding weight vector $\w_j$
and $\Psi_{j,j+1}$ is a pairwise feature vector that captures the interactions between consecutive labels $y_j$ $y_{j-1}$, 
with a corresponding weight vector $\w_{j,j+1}$.

%To learn the parameters, we train the structured SVM by optimising a quadratic program (QP),
\emph{Objective function}. 
To learn a suitable set of weights $\w$, SSVMs solve the following optimisation problem during training:

\resizebox{.99\linewidth}{!}{
  \begin{minipage}{\linewidth}
\begin{align}
&\min_{\x, \, \bm{\xi} \ge 0} \frac{1}{2} \x^\top \x + \frac{C}{n} \sum_{i=1}^n \xi_i \label{eq:nslack}\\
&s.t.~  \forall i, \nonumber\\
&\w^\top \Psi(\x^{(i)}, \y^{(i)}) + \xi_i \ge
          \max_{\bar{\y} \in \mathcal{Y}}
          \left\{\w^\top \Psi(\x^{(i)}, \bar{\y}) + \Delta(\y^{(i)}, \bar{\y}) \right\}. \nonumber
%\langle \mathbf{w}, \, \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) - \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) \rangle \ge 
%       \Delta(\mathbf{y}^{(i)}, \bar{\mathbf{y}}) - \xi_i, 
\end{align}
\end{minipage}
}

Here, $\bar\y$ is a candidate sequence,  % \in \mathcal{Y} -- LX: what is cal Y anyway?
and $\Delta(\y, \bar\y)$ is the loss function between $\bar\y$ and the ground truth $\y$. 
Here the right hand side is known as the \emph{loss-augmented inference}. 
Loss-augmented inference can be efficiently done if loss function $\Delta(,)$ is also decomposable 
with respect to individual and pairs of label elements in a way similar to Equation~\eqref{eq:jointfeature}.
%%LX: we don't need the def of \xi_i below?
%and slack variable $\xi_i$ is the \emph{hinge loss} for the prediction of the $i$-th example~\cite{tsochantaridis2005large},
%\begin{equation*}
%\xi_i = \max \left( 0, \,
%        \max_{\bar{\mathbf{y}} \in \mathcal{Y}} 
%        \left\{ \Delta(\mathbf{y}^{(i)}, \bar{\mathbf{y}}^{(i)}) + \mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) %\right\} -
%        \mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) \right).
%\end{equation*}
%This formulation is called "$n$-slack" as we have one slack variable for each example in training set.

%%LX: merge above, no need to write out the constraint twice
% We can rewrite the constraint in problem (\ref{eq:nslack}) as
% \begin{equation}
% \label{eq:ssvminf}
% \mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) + \xi_i \ge
%           \max_{\bar{\mathbf{y}} \in \mathcal{Y}}
%           \left\{\mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) + \Delta(\mathbf{y}^{(i)}, \bar{\mathbf{y}}) \right\},
% \end{equation}


To solve problem (\ref{eq:nslack}), one option is to simply enumerate all constraints, and feed the problem into a standard solver.
However, this approach is impractical as there is a constraint for every possible label $\bar{\mathbf{y}}$.
Instead, we use a cutting-plane algorithm which repeatedly solves the quadratic program (\ref{eq:nslack}) 
with a growing set of constraints~\cite{joachims2009predicting}.
In each iteration, a new constraint is formed by solving the loss-augmented inference, 
which helps shrink the feasible region of the problem.


\subsection{SSVM for recommendation: the SR model}
\label{ssec:sr}

Recall that the structured recommendation problem
involves observing \emph{multiple} ground truth output sequences for each input.
%If we observed more than one labels for a particular set of features, 
The classic SSVM described in Section~\ref{ssec:ssvm} can be easily generalised to capture this setting:
given feature vector $\mathbf{x}^{(i)}$ and the corresponding set of ground truths $\{\mathbf{y}^{(ij)}\}_{j=1}^{n_i}$
where $n_i$ is the number of labels for $\mathbf{x}_i$,
we can train a \emph{structured recommendation} (\emph{SR}) model by optimising a quadratic program similar to (\ref{eq:nslack}),
\begin{equation}
\label{eq:nslack_ml}
\resizebox{0.99\linewidth}{!}{$
\begin{aligned}
\min_{\mathbf{w}, \, \bm{\xi} \ge 0} ~& \frac{1}{2} \mathbf{w}^\top \mathbf{w} + \frac{C}{N} \sum_{i,j} \xi_{ij} \\
s.t.~ ~& \langle \mathbf{w}, \, \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(ij)}) - \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) \rangle \ge 
         \Delta(\mathbf{y}^{(ij)}, \bar{\mathbf{y}}) - \xi_{ij}, \, \forall j, \, \forall i.
\end{aligned}
$}
\end{equation}
where $N = \sum_i n_i$ and $\bar{\y} \in \mathcal{Y} \setminus \{\y^{(ij)}\}_{j=1}^{n_i}$.
Compared to (\ref{eq:nslack}), the key distinction is that the above
explicitly aggregates all the ground truth labels for each input when generating the constraints;
in this way, we do not have apparently contradictory constraints where
two ground truth sequences are each required to have larger score than the other.

one-slack formulation:
\begin{equation}
\label{eq:1slack_ml}
\resizebox{0.99\linewidth}{!}{$
\begin{aligned}
\min_{\w, \, \xi \ge 0} ~& \frac{1}{2} \w^\top \w + C \xi \\
s.t.~ ~& \frac{1}{N} \sum_{i,j} \langle \w, \, \Psi(\x^{(i)}, \y^{(ij)}) - \Psi(\x^{(i)}, \bar{\y}^{(i)}) \rangle \ge 
         \frac{1}{N} \sum_{i,j} \Delta(\y^{(ij)}, \bar{\y}^{(i)}) - \xi, 
\end{aligned}
$}
\end{equation}
where $\bar{\y}^{(i)} \in \mathcal{Y} \setminus \{\y^{ij}\}_{j=1}^{N_i}$.
\TODO{explain how 1-slack connects to n-slack and this is what's implemented}

\TODO{maybe - talk about this actually being a ranking objective for positive-only training sequences re: recommendations }

% sub-tour elimination
\subsection{Sequence decoding for the SR model}
\label{ssec:subtour}

The SR model require two algorithmic components for inference and learning that is missing from the SSVM algorithms. 

The first is being able to predict a {\em path}, \ie sequences whose elements are distinct from each other. 
As described in Section~\ref{sec:intro}, this is desirable since users traversing a sequence (of locations or music)
would not want to see repeated entries. 
Given desired sequence length $l$ among $L$ possible points, dynamic programming~\cite{tsochantaridis2005large} 
will generate the length-$l$ sequence with the best score, i.e. $\y^* = \argmax_\y f(\x,\y)$. 
One may then use the well-known 
Christofes algorithm~\cite{christofides1976} on $y^*$ to eliminate loops in the sequence. 
%is known to generate a solutions within a factor of 3/2 of the optimal solution for traveling salesman problems. 
However, the resulting sequence will have less than the desired number of points, and the resulting score won't be optimal. 

The second is being able to do loss-augmented inference by excluding multiple known sequences. 
As described in Section~\ref{ssec:sr}, this involves %we would want to maximize the loss-augmented objective function 
$\max_{\bar\y} \left\{ \w^\top \Psi(\x^{(i)}, \bar\y) + \Delta(\y^{(ij)}, \bar{\y}) \right\}$
where the domain of candidate sequences excludes the known sequences for query $\x^{(i)}$, \ie $\bar{\y} \in \mathcal{Y} \setminus \{\y^{(ij)}\}_{j=1}^{n_i}$. \TODO{should this be in Sec 3.2?}
Note that the dynamic programming~\cite{tsochantaridis2005large} algorithm use back-tracking to find the best sequence, 
and does not easily support excluding known sequences. 

%Both the loss-augmented inference and prediction inference for structured SVM (Section~\ref{sec:ssvm}) cannot be done efficiently 
%if the no duplicates constraints are required, moreover, we would like to recommend more than one trajectories given a query.

The rest of this section describes two algorithms, each intuitively aimed to address one of the two requirements above. 
But it turns how that applied in novel contexts of the SR model, 
either algorithm can satisfy the needs of both inference and learning. 
We will also describe practical choices about which algorithm to use when. 


\subsubsection{Traversing points with integer linear programming}
% ILP for subtour elimination
The no duplicates constraint can be achieved by adapting the sub-tour elimination constraints of the travelling salesman problem (TSP),
in particular, we solve the following integer linear program (ILP),
\begin{alignat}{5}
& \max_{u,v} ~&& \sum_{k=1}^M \mathbf{w}_k^\top \phi_k(\mathbf{x}, p_k) \sum_{j=1}^M u_{jk} + 
                 \sum_{j=1}^M \sum_{k=1}^M u_{jk} \mathbf{w}_{jk}^\top \phi_{j, k}(\mathbf{x}, p_j, p_k) \\
& s.t. ~~ ~&& u_{jk}, ~z_j \in \{0, 1\}, ~u_{jj}=0, ~z_1=0, ~v_j \in \mathbf{Z},~ p_j \in \mathcal{P}, ~\forall j, k = 1,\cdots,M   \label{eq:cons1} \\
&          && \sum_{k=2}^M u_{1k} = 1, ~\sum_{j=2}^M u_{j1} = 0  \label{eq:cons2} \\
&          && \sum_{j=1}^M u_{jl} = z_l + \sum_{k=2}^M u_{lk} \le 1,   ~\forall l=2,\cdots,M                    \label{eq:cons3} \\
&          && \sum_{j=1}^M \sum_{k=1}^M u_{jk} = L-1,                                                           \label{eq:cons4} \\
&          && v_j - v_k + 1 \le (M-1) (1-u_{jk}),                     \forall j,k=2,\cdots,M                    \label{eq:cons5}
\end{alignat}
where $u_{jk}$ is a binary decision variable that determines whether the transition from $p_j$ to $p_k$ is in the resulting trajectory,
$z_j$ is a binary decision variable that determines whether $p_j$ is the last POI in trajectory.
$L$ is the number of POIs in trajectory.
For brevity, we arrange the POIs such that $p_1 = s$.
Firstly, the desired trajectory should start from $s$ (Constraint~\ref{eq:cons2}).
In addition, any POI could be visited at most once (Constraint~\ref{eq:cons3}).
Moreover, only $L-1$ transitions between POIs are permitted (Constraint~\ref{eq:cons4}),
i.e., the number of POI visits should be exactly $L$ (including $s$).
The last constraint, where $v_i$ is an auxiliary variable,
enforces that only a single sequence of POIs without sub-tours is permitted in the trajectory.

If we employ the above ILP to do loss-augmented inference, one restriction is that the loss should be a linear function of $u_{jk}$,
e.g., $\Delta(\mathbf{y}, \bar{\mathbf{y}}) = 1 - \sum_{j=1}^M \sum_{k=1}^M u_{j, y_k}$ if we define the loss as the number of mispredicted POIs,
where $\mathbf{y}$ is the ground truth and $\bar{\mathbf{y}}$ is the trajectory corresponding to the optimal solution of this ILP.

\subsubsection{Finding $k$-best sequences}
% 2 uses of list Viterbi: 1) multiple ground truths; 2) subtour elimination
To achieve this, we resort to a variant of the list Viterbi algorithm~\cite{nilsson2001sequentially,seshadri1994list}
which sequentially find the $k$-th best (scored) trajectory given the best, $2$nd best, \dots, $(k-1)$-th best (scored) trajectories,
as described in Algorithm~\ref{alg:listviterbi}.

\TODO{Briefly discuss which one to use, when}

\subsection{Inference for structured recommendation}
\label{ssec:SRinf}

\TODO{recommending lists: talk about listViterbi for inference}


\subsection{Training the SR model}
\label{ssec:SRtrain}
% list Viterbi for multiple ground truths

\TODO{talk about training and what cutting plane does}

Similar to the loss-augmented inference described in Section~\ref{ssec:ssvm}, 
we can rewrite the constraints in problem (\ref{eq:nslack_ml}) into
\begin{equation*}
\resizebox{0.99\linewidth}{!}
{$
\mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \mathbf{y}^{(ij)}) + \xi_{ij} \ge 
\max_{\bar{\mathbf{y}}} \left( \Delta(\mathbf{y}^{(ij)}, \bar{\mathbf{y}}) + \mathbf{w}^\top \Psi(\mathbf{x}^{(i)}, \bar{\mathbf{y}}) \right),
\, \forall j.
$}
\end{equation*} 
Normally, the maximisation at the right side of the above inequality cannot be solved efficiently due to the constraint that 
$\bar{\mathbf{y}}$ is in $\mathcal{Y}$,
but should not be in the set of observed labels $\{\mathbf{y}^{(ij)}\}_{j=1}^{n_i}$.
However, we can pretend that $\bar{\mathbf{y}}$ can be any label in $\mathcal{Y}$ and do the unconstrained optimisation,
which can sometimes be solved efficiently, then we filter out the optimal solution if it has been observed, 
i.e., in $\{\mathbf{y}^{(ij)}\}_{j=1}^{n_i}$. 
In addition to train multiset SSVM, this technique can be further used to deal with other constraints such as sub-tour elimination 
as described in Section~\ref{sec:subtour}.

\TODO{talk about multiple truths and what it means in prediction}





