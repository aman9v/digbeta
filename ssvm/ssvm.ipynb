{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trajectory Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import os, sys, time, pickle, tempfile\n",
    "import math, random, itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import kron\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from pystruct.models import StructuredModel\n",
    "from pystruct.learners import OneSlackSSVM\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import cython\n",
    "import pulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(1234567890)\n",
    "np.random.seed(1234567890)\n",
    "LOG_SMALL = -10\n",
    "LOG_ZERO = -1000\n",
    "ranksvm_dir = '$HOME/work/ranksvm'  # directory that contains rankSVM binaries: train, predict, svm-scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/data-new'\n",
    "dat_suffix = ['Osak', 'Glas', 'Edin', 'Toro', 'Melb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_ix = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALPHA_SET = [0.1, 0.3, 0.5, 0.7, 0.9]  # trade-off parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BIN_CLUSTER = 5  # discritization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANKSVM_C = 10     # RankSVM regularisation constant\n",
    "LOGIT_C = 1        # Logistic regression regularisation constant, smaller values specify stronger regularisation\n",
    "SSVM_C = 1         # Structured SVM regularisation constant\n",
    "N_JOBS = 4         # number of parallel jobs\n",
    "USE_GUROBI = False # whether to use GUROBI as ILP solver\n",
    "DO_NORM = True     # whether normalise features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method switches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_ssvm = True\n",
    "run_logreg = False\n",
    "run_linreg = False\n",
    "run_logpwr = False\n",
    "run_rank = False\n",
    "run_tran = False\n",
    "run_comb = False\n",
    "run_rand = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate results filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_fname(dat_ix):\n",
    "    assert(0 <= dat_ix < len(dat_suffix))\n",
    "    \n",
    "    suffix = dat_suffix[dat_ix] + '.pkl'\n",
    "    \n",
    "    fssvm = os.path.join(data_dir, 'ssvm-' + suffix)\n",
    "    flogreg = os.path.join(data_dir, 'logreg-' + suffix)\n",
    "    flinreg = os.path.join(data_dir, 'linreg-' + suffix)\n",
    "    logpwr = os.path.join(data_dir, 'logpwr-' + suffix)\n",
    "    frank = os.path.join(data_dir, 'rank-' + suffix)\n",
    "    ftran = os.path.join(data_dir, 'tran-' + suffix)\n",
    "    fcomb = os.path.join(data_dir, 'comb-' + suffix)\n",
    "    frand = os.path.join(data_dir, 'rand-' + suffix)\n",
    "    return fssvm, flogreg, flinreg, logpwr, frank, ftran, fcomb, frand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frecdict_ssvm, frecdict_logreg, frecdict_linreg, frecdict_logpwr, \\\n",
    "frecdict_rank, frecdict_tran, frecdict_comb, frecdict_rand = gen_fname(dat_ix)\n",
    "print(frecdict_ssvm)\n",
    "print(frecdict_logreg)\n",
    "print(frecdict_linreg)\n",
    "print(frecdict_logpwr)\n",
    "print(frecdict_rank)\n",
    "print(frecdict_tran)\n",
    "print(frecdict_comb)\n",
    "print(frecdict_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpoi = os.path.join(data_dir, 'poi-' + dat_suffix[dat_ix] + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_all = pd.read_csv(fpoi)\n",
    "poi_all.set_index('poiID', inplace=True)\n",
    "poi_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ftraj = os.path.join(data_dir, 'traj-' + dat_suffix[dat_ix] + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traj_all = pd.read_csv(ftraj)\n",
    "traj_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_user = traj_all['userID'].unique().shape[0]\n",
    "num_poi = traj_all['poiID'].unique().shape[0]\n",
    "num_traj = traj_all['trajID'].unique().shape[0]\n",
    "pd.DataFrame({'#user': num_user, '#poi': num_poi, '#traj': num_traj, '#traj/user': num_traj/num_user}, \\\n",
    "             index=[str(dat_suffix[dat_ix])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of the number of POIs in trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ax = traj_all['trajLen'].hist(bins=20)\n",
    "#ax.set_yscale('log')\n",
    "#ax.set_xlabel('#POIs in trajectory'); ax.set_ylabel('#Trajectories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of POI visit duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ax = traj_all['poiDuration'].hist(bins=20)\n",
    "#ax.set_xscale('log')\n",
    "#ax.set_yscale('log')\n",
    "#ax.set_xlabel('POI visit duration (sec)'); ax.set_ylabel('#POI visits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print computing progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_progress(cnt, total):\n",
    "    \"\"\"Display a progress bar\"\"\"\n",
    "    assert(cnt > 0 and total > 0 and cnt <= total)\n",
    "    length = 80\n",
    "    ratio = cnt / total\n",
    "    n = int(length * ratio)\n",
    "    sys.stdout.write('\\r[%-80s] %d%%' % ('-'*n, int(ratio*100)))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract trajectory, i.e., a list of POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_traj(tid, traj_all):\n",
    "    traj = traj_all[traj_all['trajID'] == tid].copy()\n",
    "    traj.sort_values(by=['startTime'], ascending=True, inplace=True)\n",
    "    return traj['poiID'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute POI properties, e.g., popularity, total number of visit, average visit duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_info(trajid_list, traj_all, poi_all):\n",
    "    assert(len(trajid_list) > 0)\n",
    "    poi_info = traj_all[traj_all['trajID'] == trajid_list[0]][['poiID', 'poiDuration']].copy() \n",
    "    for i in range(1, len(trajid_list)):\n",
    "        traj = traj_all[traj_all['trajID'] == trajid_list[i]][['poiID', 'poiDuration']]\n",
    "        poi_info = poi_info.append(traj, ignore_index=True)\n",
    "    \n",
    "    poi_info = poi_info.groupby('poiID').agg([np.mean, np.size])\n",
    "    poi_info.columns = poi_info.columns.droplevel()\n",
    "    poi_info.reset_index(inplace=True)\n",
    "    poi_info.rename(columns={'mean':'avgDuration', 'size':'nVisit'}, inplace=True)\n",
    "    poi_info.set_index('poiID', inplace=True) \n",
    "    poi_info['poiCat'] = poi_all.loc[poi_info.index, 'poiCat']\n",
    "    poi_info['poiLon'] = poi_all.loc[poi_info.index, 'poiLon']\n",
    "    poi_info['poiLat'] = poi_all.loc[poi_info.index, 'poiLat']\n",
    "    \n",
    "    # POI popularity: the number of distinct users that visited the POI\n",
    "    pop_df = traj_all[traj_all['trajID'].isin(trajid_list)][['poiID', 'userID']].copy()\n",
    "    pop_df = pop_df.groupby('poiID').agg(pd.Series.nunique)\n",
    "    pop_df.rename(columns={'userID':'nunique'}, inplace=True)\n",
    "    poi_info['popularity'] = pop_df.loc[poi_info.index, 'nunique']\n",
    "    \n",
    "    return poi_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the F1 score for recommended trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_F1(traj_act, traj_rec, noloop=False):\n",
    "    '''Compute recall, precision and F1 for recommended trajectories'''\n",
    "    assert(isinstance(noloop, bool))\n",
    "    assert(len(traj_act) > 0)\n",
    "    assert(len(traj_rec) > 0)\n",
    "    \n",
    "    if noloop == True:\n",
    "        intersize = len(set(traj_act) & set(traj_rec))\n",
    "    else:\n",
    "        match_tags = np.zeros(len(traj_act), dtype=np.bool)\n",
    "        for poi in traj_rec:\n",
    "            for j in range(len(traj_act)):\n",
    "                if match_tags[j] == False and poi == traj_act[j]:\n",
    "                    match_tags[j] = True\n",
    "                    break\n",
    "        intersize = np.nonzero(match_tags)[0].shape[0]\n",
    "        \n",
    "    recall = intersize / len(traj_act)\n",
    "    precision = intersize / len(traj_rec)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the pairs-F1 score for recommended trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "cpdef float calc_pairsF1(y, y_hat):\n",
    "    assert(len(y) > 0)\n",
    "    assert(len(y) == len(set(y))) # no loops in y\n",
    "    cdef int n, nr, n0, n0r, nc, poi1, poi2, i, j\n",
    "    n = len(y)\n",
    "    nr = len(y_hat)\n",
    "    n0 = int(n*(n-1) / 2)\n",
    "    n0r = int(nr*(nr-1) / 2)\n",
    "    \n",
    "    # y determines the correct visiting order\n",
    "    order_dict = dict()\n",
    "    for i in range(n):\n",
    "        order_dict[y[i]] = i\n",
    "        \n",
    "    nc = 0\n",
    "    for i in range(nr):\n",
    "        poi1 = y_hat[i]\n",
    "        for j in range(i+1, nr):\n",
    "            poi2 = y_hat[j]\n",
    "            if poi1 in order_dict and poi2 in order_dict and poi1 != poi2:\n",
    "                if order_dict[poi1] < order_dict[poi2]: nc += 1\n",
    "\n",
    "    cdef float precision, recall, F1\n",
    "    precision = (1.0 * nc) / (1.0 * n0r)\n",
    "    recall = (1.0 * nc) / (1.0 * n0)\n",
    "    if nc == 0:\n",
    "        F1 = 0\n",
    "    else:\n",
    "        F1 = 2. * precision * recall / (precision + recall)\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute distance between two POIs using [Haversine formula](http://en.wikipedia.org/wiki/Great-circle_distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_dist_vec(longitudes1, latitudes1, longitudes2, latitudes2):\n",
    "    \"\"\"Calculate the distance (unit: km) between two places on earth, vectorised\"\"\"\n",
    "    # convert degrees to radians\n",
    "    lng1 = np.radians(longitudes1)\n",
    "    lat1 = np.radians(latitudes1)\n",
    "    lng2 = np.radians(longitudes2)\n",
    "    lat2 = np.radians(latitudes2)\n",
    "    radius = 6371.0088 # mean earth radius, en.wikipedia.org/wiki/Earth_radius#Mean_radius\n",
    "\n",
    "    # The haversine formula, en.wikipedia.org/wiki/Great-circle_distance\n",
    "    dlng = np.fabs(lng1 - lng2)\n",
    "    dlat = np.fabs(lat1 - lat2)\n",
    "    dist =  2 * radius * np.arcsin( np.sqrt( \n",
    "                (np.sin(0.5*dlat))**2 + np.cos(lat1) * np.cos(lat2) * (np.sin(0.5*dlng))**2 ))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance between POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POI_DISTMAT = pd.DataFrame(data=np.zeros((poi_all.shape[0], poi_all.shape[0]), dtype=np.float), \\\n",
    "                           index=poi_all.index, columns=poi_all.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ix in poi_all.index:\n",
    "    POI_DISTMAT.loc[ix] = calc_dist_vec(poi_all.loc[ix, 'poiLon'], \\\n",
    "                                        poi_all.loc[ix, 'poiLat'], \\\n",
    "                                        poi_all['poiLon'], \\\n",
    "                                        poi_all['poiLat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trajid_set_all = sorted(traj_all['trajID'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_info_all = calc_poi_info(trajid_set_all, traj_all, poi_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary maps every trajectory ID to the actual trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traj_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for trajid in trajid_set_all:\n",
    "    traj = extract_traj(trajid, traj_all)\n",
    "    assert(trajid not in traj_dict)\n",
    "    traj_dict[trajid] = traj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a *query* (in IR terminology) using tuple (start POI, end POI, #POI) ~~user ID.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_ID_DICT = dict()  # (start, length) --> qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = [(traj_dict[x][0], len(traj_dict[x])) \\\n",
    "        for x in sorted(traj_dict.keys()) if len(traj_dict[x]) > 1]\n",
    "cnt = 0\n",
    "for key in keys:\n",
    "    if key not in QUERY_ID_DICT:   # (start, length) --> qid\n",
    "        QUERY_ID_DICT[key] = cnt\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('#traj in total:', len(trajid_set_all))\n",
    "print('#traj (length >= 2):', traj_all[traj_all['trajLen'] >= 2]['trajID'].unique().shape[0])\n",
    "print('#query tuple:', len(QUERY_ID_DICT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. POI Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 POI Features for Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI Features used for ranking, given query (`startPOI`, `endPOI`, `nPOI`):\n",
    "1. `category`: one-hot encoding of POI category, encode `True` as `1` and `False` as `-1`\n",
    "1. `neighbourhood`: one-hot encoding of POI cluster, encode `True` as `1` and `False` as `-1`\n",
    "1. `popularity`: log of POI popularity, i.e., the number of distinct users that visited the POI\n",
    "1. `nVisit`: log of the total number of visit by all users\n",
    "1. `avgDuration`: log of average POI visit duration\n",
    "1. `trajLen`: trajectory length, i.e., the number of POIs `nPOI` in trajectory, copy from query\n",
    "1. `sameCatStart`: 1 if POI category is the same as that of `startPOI`, -1 otherwise\n",
    "1. `sameCatEnd`: 1 if POI category is the same as that of `endPOI`, -1 otherwise\n",
    "1. `distStart`: distance (haversine formula) from `startPOI`\n",
    "1. `distEnd`: distance from `endPOI`\n",
    "1. `diffPopStart`: difference in POI popularity from `startPOI` (NO LOG as it could be negative)\n",
    "1. `diffPopEnd`: difference in POI popularity from `endPOI`\n",
    "1. `diffNVisitStart`: difference in the total number of visit from `startPOI`\n",
    "1. `diffNVisitEnd`: difference in the total number of visit from `endPOI`\n",
    "1. `diffDurationStart`: difference in average POI visit duration from the actual duration spent at `startPOI`\n",
    "1. `diffDurationEnd`: difference in average POI visit duration from the actual duration spent at `endPOI`\n",
    "1. `sameNeighbourhoodStart`: 1 if POI resides in the same cluster as that of `startPOI`, -1 otherwise\n",
    "1. `sameNeighbourhoodEnd`: 1 if POI resides in the same cluster as that of `endPOI`, -1 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DF_COLUMNS = ['poiID', 'label', 'queryID', 'category', 'neighbourhood', 'popularity', 'nVisit', 'avgDuration', \\\n",
    "              'trajLen', 'sameCatStart', 'distStart', 'diffPopStart', 'diffNVisitStart', 'diffDurationStart', \\\n",
    "              'sameNeighbourhoodStart']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data are generated as follows:\n",
    "1. each input tuple $(\\text{startPOI}, \\text{#POI})$ form a `query` (in IR terminology).\n",
    "1. the label of a specific POI is the number of presence of that POI in the set of trajectories grouped by a specific `query`, excluding the presence as $\\text{startPOI}$. (the label of all absence POIs w.r.t. that `query` got a label `0`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of training data matrix is `#(qid, poi)` by `#feature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_train_subdf(poi_id, query_id_set, poi_info, poi_clusters, cats, clusters, query_id_rdict):\n",
    "    assert(isinstance(cats, list))\n",
    "    assert(isinstance(clusters, list))\n",
    "    \n",
    "    columns = DF_COLUMNS\n",
    "    poi_distmat = POI_DISTMAT\n",
    "    df_ = pd.DataFrame(index=np.arange(len(query_id_set)), columns=columns)\n",
    "    \n",
    "    pop, nvisit = poi_info.loc[poi_id, 'popularity'], poi_info.loc[poi_id, 'nVisit']\n",
    "    cat, cluster = poi_info.loc[poi_id, 'poiCat'], poi_clusters.loc[poi_id, 'clusterID'] \n",
    "    duration = poi_info.loc[poi_id, 'avgDuration']\n",
    "    \n",
    "    for j in range(len(query_id_set)):\n",
    "        qid = query_id_set[j]\n",
    "        assert(qid in query_id_rdict) # qid --> (start, end, length)\n",
    "        (p0, trajLen) = query_id_rdict[qid]\n",
    "        idx = df_.index[j]\n",
    "        df_.loc[idx, 'poiID'] = poi_id\n",
    "        df_.loc[idx, 'queryID'] = qid\n",
    "        df_.set_value(idx, 'category', tuple((cat == np.array(cats)).astype(np.int) * 2 - 1))\n",
    "        df_.set_value(idx, 'neighbourhood', tuple((cluster == np.array(clusters)).astype(np.int) * 2 - 1))\n",
    "        df_.loc[idx, 'popularity'] = LOG_SMALL if pop < 1 else np.log10(pop)\n",
    "        df_.loc[idx, 'nVisit'] = LOG_SMALL if nvisit < 1 else np.log10(nvisit)\n",
    "        df_.loc[idx, 'avgDuration'] = LOG_SMALL if duration < 1 else np.log10(duration)\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_info.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi_id, p0]\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'sameNeighbourhoodStart'] = 1 if cluster == poi_clusters.loc[p0, 'clusterID'] else -1\n",
    "        \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_train_df(trajid_list, traj_dict, poi_info, poi_clusters, cats, clusters, n_jobs=-1):    \n",
    "    columns = DF_COLUMNS\n",
    "    poi_distmat = POI_DISTMAT\n",
    "    query_id_dict = QUERY_ID_DICT\n",
    "    train_trajs = [traj_dict[x] for x in trajid_list if len(traj_dict[x]) >= 2]\n",
    "    \n",
    "    qid_set = sorted(set([query_id_dict[(t[0], len(t))] for t in train_trajs]))\n",
    "    poi_set = set()\n",
    "    for tr in train_trajs:\n",
    "        poi_set = poi_set | set(tr)\n",
    "    \n",
    "    query_id_rdict = dict()\n",
    "    for k, v in query_id_dict.items(): \n",
    "        query_id_rdict[v] = k  # qid --> (start, length)\n",
    "    \n",
    "    train_df_list = Parallel(n_jobs=n_jobs)\\\n",
    "                            (delayed(gen_train_subdf)(poi, qid_set, poi_info, poi_clusters,cats,clusters,query_id_rdict) \n",
    "                             for poi in poi_set)\n",
    "                        \n",
    "    assert(len(train_df_list) > 0)\n",
    "    df_ = train_df_list[0]\n",
    "    for j in range(1, len(train_df_list)):\n",
    "        df_ = df_.append(train_df_list[j], ignore_index=True)            \n",
    "        \n",
    "    # set label\n",
    "    df_.set_index(['queryID', 'poiID'], inplace=True)\n",
    "    df_['label'] = 0\n",
    "    for t in train_trajs:\n",
    "        qid = query_id_dict[(t[0], len(t))]\n",
    "        for poi in t[1:]:  # do NOT count if the POI is startPOI/endPOI\n",
    "            df_.loc[(qid, poi), 'label'] += 1\n",
    "\n",
    "    df_.reset_index(inplace=True)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Test DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data are generated the same way as training data, except that the labels of testing data (unknown) could be arbitrary values as suggested in [libsvm FAQ](http://www.csie.ntu.edu.tw/~cjlin/libsvm/faq.html#f431).\n",
    "The reported accuracy (by `svm-predict` command) is meaningless as it is calculated based on these labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of training data matrix is `#poi` by `#feature` with one specific `query`, i.e. tuple $(\\text{startPOI}, \\text{#POI})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_test_df(startPOI, nPOI, poi_info, poi_clusters, cats, clusters):\n",
    "    assert(isinstance(cats, list))\n",
    "    assert(isinstance(clusters, list))\n",
    "    \n",
    "    columns = DF_COLUMNS\n",
    "    poi_distmat = POI_DISTMAT\n",
    "    query_id_dict = QUERY_ID_DICT\n",
    "    key = (p0, trajLen) = (startPOI, nPOI)\n",
    "    assert(key in query_id_dict)\n",
    "    assert(p0 in poi_info.index)\n",
    "    \n",
    "    df_ = pd.DataFrame(index=np.arange(poi_info.shape[0]), columns=columns)\n",
    "    poi_list = sorted(poi_info.index)\n",
    "    \n",
    "    qid = query_id_dict[key]\n",
    "    df_['queryID'] = qid\n",
    "    df_['label'] = np.random.rand(df_.shape[0]) # label for test data is arbitrary according to libsvm FAQ\n",
    "\n",
    "    for i in range(df_.index.shape[0]):\n",
    "        poi = poi_list[i]\n",
    "        lon, lat = poi_info.loc[poi, 'poiLon'], poi_info.loc[poi, 'poiLat']\n",
    "        pop, nvisit = poi_info.loc[poi, 'popularity'], poi_info.loc[poi, 'nVisit']\n",
    "        cat, cluster = poi_info.loc[poi, 'poiCat'], poi_clusters.loc[poi, 'clusterID']\n",
    "        duration = poi_info.loc[poi, 'avgDuration']\n",
    "        idx = df_.index[i]\n",
    "        df_.loc[idx, 'poiID'] = poi\n",
    "        df_.set_value(idx, 'category', tuple((cat == np.array(cats)).astype(np.int) * 2 - 1))\n",
    "        df_.set_value(idx, 'neighbourhood', tuple((cluster == np.array(clusters)).astype(np.int) * 2 - 1))\n",
    "        df_.loc[idx, 'popularity'] = LOG_SMALL if pop < 1 else np.log10(pop)\n",
    "        df_.loc[idx, 'nVisit'] = LOG_SMALL if nvisit < 1 else np.log10(nvisit)\n",
    "        df_.loc[idx, 'avgDuration'] = LOG_SMALL if duration < 1 else np.log10(duration)\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_all.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi, p0]\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'sameNeighbourhoodStart'] = 1 if cluster == poi_clusters.loc[p0, 'clusterID'] else -1\n",
    "        \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a string for a training/test data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data_str(df_, df_columns=DF_COLUMNS):\n",
    "    for col in df_columns:\n",
    "        assert(col in df_.columns)\n",
    "        \n",
    "    lines = []\n",
    "    for idx in df_.index:\n",
    "        slist = [str(df_.loc[idx, 'label'])]\n",
    "        slist.append(' qid:')\n",
    "        slist.append(str(int(df_.loc[idx, 'queryID'])))\n",
    "        fid = 1\n",
    "        for j in range(3, len(df_columns)):\n",
    "            values_ = df_.get_value(idx, df_columns[j])\n",
    "            values_ = values_ if isinstance(values_, tuple) else [values_]\n",
    "            for v in values_:\n",
    "                slist.append(' ')\n",
    "                slist.append(str(fid)); fid += 1\n",
    "                slist.append(':')\n",
    "                slist.append(str(v))\n",
    "        slist.append('\\n')\n",
    "        lines.append(''.join(slist))\n",
    "    return ''.join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Ranking POIs using rankSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the [rankSVM implementation](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#large_scale_ranksvm) could be [liblinear-ranksvm](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ranksvm/liblinear-ranksvm-2.1.zip) or [libsvm-ranksvm](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ranksvm/libsvm-ranksvm-3.20.zip), please read `README.ranksvm` in the zip file for installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [softmax function](https://en.wikipedia.org/wiki/Softmax_function) to convert ranking scores to a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x1 = x.copy()\n",
    "    x1 -= np.max(x1)  # numerically more stable, REF: http://cs231n.github.io/linear-classify/#softmax\n",
    "    expx = np.exp(x1)\n",
    "    return expx / np.sum(expx, axis=0) # column-wise sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a python wrapper of the `svm-train` or `train` and `svm-predict` or `predict` commands of rankSVM with ranking probabilities $P(p_i \\lvert (p_s, p_e, len))$ computed using [softmax function](https://en.wikipedia.org/wiki/Softmax_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python wrapper of rankSVM\n",
    "class RankSVM:\n",
    "    def __init__(self, bin_dir, useLinear=True, debug=False):\n",
    "        dir_ = !echo $bin_dir  # deal with environmental variables in path\n",
    "        assert(os.path.exists(dir_[0]))\n",
    "        self.bin_dir = dir_[0]\n",
    "        \n",
    "        self.bin_train = 'svm-train'\n",
    "        self.bin_predict = 'svm-predict'\n",
    "        if useLinear:\n",
    "            self.bin_train = 'train'\n",
    "            self.bin_predict = 'predict'\n",
    "        \n",
    "        assert(isinstance(debug, bool))\n",
    "        self.debug = debug\n",
    "        \n",
    "        # create named tmp files for model and feature scaling parameters\n",
    "        self.fmodel = None\n",
    "        self.fscale = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fmodel = fd.name\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            self.fscale = fd.name\n",
    "        \n",
    "        if self.debug:\n",
    "            print('model file:', self.fmodel)\n",
    "            print('feature scaling parameter file:', self.fscale)\n",
    "    \n",
    "    \n",
    "    def __del__(self):\n",
    "        # remove tmp files\n",
    "        if self.debug == False:\n",
    "            if self.fmodel is not None and os.path.exists(self.fmodel):\n",
    "                os.unlink(self.fmodel)\n",
    "            if self.fscale is not None and os.path.exists(self.fscale):\n",
    "                os.unlink(self.fscale)\n",
    "\n",
    "    \n",
    "    def train(self, train_df, cost=1):\n",
    "        # cost is parameter C in SVM\n",
    "        # write train data to file\n",
    "        ftrain = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain = fd.name\n",
    "            datastr = gen_data_str(train_df)\n",
    "            fd.write(datastr)\n",
    "        \n",
    "        # feature scaling\n",
    "        ftrain_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftrain_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -s $self.fscale $ftrain > $ftrain_scaled\n",
    "        \n",
    "        if self.debug:\n",
    "            print('cost:', cost)\n",
    "            print('train data file:', ftrain)\n",
    "            print('feature scaled train data file:', ftrain_scaled)\n",
    "        \n",
    "        # train rank svm and generate model file, if the model file exists, rewrite it\n",
    "        result = !$self.bin_dir/$self.bin_train -c $cost $ftrain_scaled $self.fmodel\n",
    "        if self.debug:\n",
    "            print('Training finished.')\n",
    "            for i in range(len(result)): print(result[i])\n",
    "\n",
    "        # remove train data file\n",
    "        if self.debug == False:\n",
    "            os.unlink(ftrain)\n",
    "            os.unlink(ftrain_scaled)        \n",
    "\n",
    "    \n",
    "    def predict(self, test_df):\n",
    "        # predict ranking scores for the given feature matrix\n",
    "        if self.fmodel is None or not os.path.exists(self.fmodel):\n",
    "            print('Model should be trained before prediction')\n",
    "            return\n",
    "        \n",
    "        # write test data to file\n",
    "        ftest = None\n",
    "        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as fd: \n",
    "            ftest = fd.name\n",
    "            datastr = gen_data_str(test_df)\n",
    "            fd.write(datastr)\n",
    "                \n",
    "        # feature scaling\n",
    "        ftest_scaled = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            ftest_scaled = fd.name\n",
    "        result = !$self.bin_dir/svm-scale -r $self.fscale $ftest > $ftest_scaled\n",
    "            \n",
    "        # generate prediction file\n",
    "        fpredict = None\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as fd: \n",
    "            fpredict = fd.name\n",
    "            \n",
    "        if self.debug:\n",
    "            print('test data file:', ftest)\n",
    "            print('feature scaled test data file:', ftest_scaled)\n",
    "            print('predict result file:', fpredict)\n",
    "            \n",
    "        # predict using trained model and write prediction to file\n",
    "        result = !$self.bin_dir/$self.bin_predict $ftest_scaled $self.fmodel $fpredict\n",
    "        if self.debug:\n",
    "            print('Predict result: %-30s  %s' % (result[0], result[1]))\n",
    "        \n",
    "        # generate prediction DataFrame from prediction file\n",
    "        poi_rank_df = pd.read_csv(fpredict, header=None)\n",
    "        poi_rank_df.rename(columns={0:'rank'}, inplace=True)\n",
    "        poi_rank_df['poiID'] = test_df['poiID'].astype(np.int)\n",
    "        poi_rank_df.set_index('poiID', inplace=True)\n",
    "        poi_rank_df['probability'] = softmax(poi_rank_df['rank'])\n",
    "        \n",
    "        # remove test file and prediction file\n",
    "        if self.debug == False:\n",
    "            os.unlink(ftest)\n",
    "            os.unlink(ftest_scaled)\n",
    "            os.unlink(fpredict)\n",
    "\n",
    "        return poi_rank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Factorised Transition Probabilities between POIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate a transition matrix for each feature of POI, transition probabilities between different POIs are obtrained by the Kronecker product of the individual transition matrix corresponding to each feature (with normalisation and a few constraints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 POI Features for Factorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI features used to factorise transition matrix of Markov Chain with POI features (vector) as states:\n",
    "- Category of POI\n",
    "- Popularity of POI (discritize with uniform log-scale bins, #bins <=5 )\n",
    "- The number of POI visits (discritize with uniform log-scale bins, #bins <=5 )\n",
    "- The average visit duration of POI (discritise with uniform log-scale bins, #bins <= 5)\n",
    "- The neighborhood relationship between POIs (clustering POI(lat, lon) using k-means, #clusters <= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the number of transition first, then normalise each row while taking care of zero by adding each cell a number $k=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalise_transmat(transmat_cnt):\n",
    "    transmat = transmat_cnt.copy()\n",
    "    assert(isinstance(transmat, pd.DataFrame))\n",
    "    for row in range(transmat.index.shape[0]):\n",
    "        rowsum = np.sum(transmat.iloc[row] + 1)\n",
    "        assert(rowsum > 0)\n",
    "        transmat.iloc[row] = (transmat.iloc[row] + 1) / rowsum\n",
    "    return transmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POIs in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_train = sorted(poi_info_all.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Transition Matrix between POI Cateogries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_cats = poi_all.loc[poi_train, 'poiCat'].unique().tolist()\n",
    "poi_cats.sort()\n",
    "POI_CAT_LIST = poi_cats\n",
    "POI_CAT_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transmat_cat(trajid_list, traj_dict, poi_info, poi_cats=POI_CAT_LIST):\n",
    "    transmat_cat_cnt = pd.DataFrame(data=np.zeros((len(poi_cats), len(poi_cats)), dtype=np.float), \\\n",
    "                                    columns=poi_cats, index=poi_cats)\n",
    "    for tid in trajid_list:\n",
    "        t = traj_dict[tid]\n",
    "        if len(t) > 1:\n",
    "            for pi in range(len(t)-1):\n",
    "                p1 = t[pi]\n",
    "                p2 = t[pi+1]\n",
    "                assert(p1 in poi_info.index and p2 in poi_info.index)\n",
    "                cat1 = poi_info.loc[p1, 'poiCat']\n",
    "                cat2 = poi_info.loc[p2, 'poiCat']\n",
    "                transmat_cat_cnt.loc[cat1, cat2] += 1\n",
    "    return normalise_transmat(transmat_cat_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gen_transmat_cat(trajid_set_all, traj_dict, poi_info_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Transition Matrix between POI Popularity Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_pops = poi_info_all.loc[poi_train, 'popularity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretize POI popularity with uniform log-scale bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expo_pop1 = np.log10(max(1, min(poi_pops)))\n",
    "expo_pop2 = np.log10(max(poi_pops))\n",
    "print(expo_pop1, expo_pop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbins_pop = BIN_CLUSTER\n",
    "logbins_pop = np.logspace(np.floor(expo_pop1), np.ceil(expo_pop2), nbins_pop+1)\n",
    "logbins_pop[0] = 0  # deal with underflow\n",
    "if logbins_pop[-1] < poi_info_all['popularity'].max():\n",
    "    logbins_pop[-1] = poi_info_all['popularity'].max() + 1\n",
    "logbins_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ax = pd.Series(poi_pops).hist(figsize=(5, 3), bins=logbins_pop)\n",
    "#ax.set_xlim(xmin=0.1)\n",
    "#ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transmat_pop(trajid_list, traj_dict, poi_info, logbins_pop=logbins_pop):\n",
    "    nbins = len(logbins_pop) - 1\n",
    "    transmat_pop_cnt = pd.DataFrame(data=np.zeros((nbins, nbins), dtype=np.float), \\\n",
    "                                    columns=np.arange(1, nbins+1), index=np.arange(1, nbins+1))\n",
    "    for tid in trajid_list:\n",
    "        t = traj_dict[tid]\n",
    "        if len(t) > 1:\n",
    "            for pi in range(len(t)-1):\n",
    "                p1 = t[pi]\n",
    "                p2 = t[pi+1]\n",
    "                assert(p1 in poi_info.index and p2 in poi_info.index)\n",
    "                pop1 = poi_info.loc[p1, 'popularity']\n",
    "                pop2 = poi_info.loc[p2, 'popularity']\n",
    "                pc1, pc2 = np.digitize([pop1, pop2], logbins_pop)\n",
    "                transmat_pop_cnt.loc[pc1, pc2] += 1\n",
    "    return normalise_transmat(transmat_pop_cnt), logbins_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gen_transmat_pop(trajid_set_all, traj_dict, poi_info_all)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Transition Matrix between the Number of POI Visit Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_visits = poi_info_all.loc[poi_train, 'nVisit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretize the number of POI visit with uniform log-scale bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expo_visit1 = np.log10(max(1, min(poi_visits)))\n",
    "expo_visit2 = np.log10(max(poi_visits))\n",
    "print(expo_visit1, expo_visit2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbins_visit = BIN_CLUSTER\n",
    "logbins_visit = np.logspace(np.floor(expo_visit1), np.ceil(expo_visit2), nbins_visit+1)\n",
    "logbins_visit[0] = 0  # deal with underflow\n",
    "if logbins_visit[-1] < poi_info_all['nVisit'].max():\n",
    "    logbins_visit[-1] = poi_info_all['nVisit'].max() + 1\n",
    "logbins_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ax = pd.Series(poi_visits).hist(figsize=(5, 3), bins=logbins_visit)\n",
    "#ax.set_xlim(xmin=0.1)\n",
    "#ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transmat_visit(trajid_list, traj_dict, poi_info, logbins_visit=logbins_visit):\n",
    "    nbins = len(logbins_visit) - 1\n",
    "    transmat_visit_cnt = pd.DataFrame(data=np.zeros((nbins, nbins), dtype=np.float), \\\n",
    "                                      columns=np.arange(1, nbins+1), index=np.arange(1, nbins+1))\n",
    "    for tid in trajid_list:\n",
    "        t = traj_dict[tid]\n",
    "        if len(t) > 1:\n",
    "            for pi in range(len(t)-1):\n",
    "                p1 = t[pi]\n",
    "                p2 = t[pi+1]\n",
    "                assert(p1 in poi_info.index and p2 in poi_info.index)\n",
    "                visit1 = poi_info.loc[p1, 'nVisit']\n",
    "                visit2 = poi_info.loc[p2, 'nVisit']\n",
    "                vc1, vc2 = np.digitize([visit1, visit2], logbins_visit)\n",
    "                transmat_visit_cnt.loc[vc1, vc2] += 1\n",
    "    return normalise_transmat(transmat_visit_cnt), logbins_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gen_transmat_visit(trajid_set_all, traj_dict, poi_info_all)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Transition Matrix between POI Average Visit Duration Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_durations = poi_info_all.loc[poi_train, 'avgDuration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expo_duration1 = np.log10(max(1, min(poi_durations)))\n",
    "expo_duration2 = np.log10(max(poi_durations))\n",
    "print(expo_duration1, expo_duration2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbins_duration = BIN_CLUSTER\n",
    "logbins_duration = np.logspace(np.floor(expo_duration1), np.ceil(expo_duration2), nbins_duration+1)\n",
    "logbins_duration[0] = 0  # deal with underflow\n",
    "logbins_duration[-1] = np.power(10, expo_duration2+2)\n",
    "logbins_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ax = pd.Series(poi_durations).hist(figsize=(5, 3), bins=logbins_duration)\n",
    "#ax.set_xlim(xmin=0.1)\n",
    "#ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transmat_duration(trajid_list, traj_dict, poi_info, logbins_duration=logbins_duration):\n",
    "    nbins = len(logbins_duration) - 1\n",
    "    transmat_duration_cnt = pd.DataFrame(data=np.zeros((nbins, nbins), dtype=np.float), \\\n",
    "                                         columns=np.arange(1, nbins+1), index=np.arange(1, nbins+1))\n",
    "    for tid in trajid_list:\n",
    "        t = traj_dict[tid]\n",
    "        if len(t) > 1:\n",
    "            for pi in range(len(t)-1):\n",
    "                p1 = t[pi]\n",
    "                p2 = t[pi+1]\n",
    "                assert(p1 in poi_info.index and p2 in poi_info.index)\n",
    "                d1 = poi_info.loc[p1, 'avgDuration']\n",
    "                d2 = poi_info.loc[p2, 'avgDuration']\n",
    "                dc1, dc2 = np.digitize([d1, d2], logbins_duration)\n",
    "                transmat_duration_cnt.loc[dc1, dc2] += 1\n",
    "    return normalise_transmat(transmat_duration_cnt), logbins_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gen_transmat_duration(trajid_set_all, traj_dict, poi_info_all)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Transition Matrix between POI Neighborhood Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans in scikit-learn seems unable to use custom distance metric and no implementation of [Haversine formula](http://en.wikipedia.org/wiki/Great-circle_distance), use Euclidean distance to approximate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = poi_all.loc[poi_train, ['poiLon', 'poiLat']]\n",
    "nclusters = BIN_CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=nclusters, random_state=987654321)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = kmeans.predict(X)\n",
    "POI_CLUSTER_LIST = sorted(np.unique(clusters))\n",
    "POI_CLUSTERS = pd.DataFrame(data=clusters, index=poi_train)\n",
    "POI_CLUSTERS.index.name = 'poiID'\n",
    "POI_CLUSTERS.rename(columns={0:'clusterID'}, inplace=True)\n",
    "POI_CLUSTERS['clusterID'] = POI_CLUSTERS['clusterID'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plot of POI coordinates with clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#diff = poi_all.loc[poi_train, ['poiLon', 'poiLat']].max() - poi_all.loc[poi_train, ['poiLon', 'poiLat']].min()\n",
    "#ratio = diff['poiLon'] / diff['poiLat']\n",
    "#height = 6; width = int(round(ratio)*height)\n",
    "#plt.figure(figsize=[width, height])\n",
    "#plt.scatter(poi_all.loc[poi_train, 'poiLon'], poi_all.loc[poi_train, 'poiLat'], c=clusters, s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transmat_neighbor(trajid_list, traj_dict, poi_info, poi_clusters=POI_CLUSTERS):\n",
    "    nclusters = len(poi_clusters['clusterID'].unique())\n",
    "    transmat_neighbor_cnt = pd.DataFrame(data=np.zeros((nclusters, nclusters), dtype=np.float), \\\n",
    "                                         columns=np.arange(nclusters), index=np.arange(nclusters))\n",
    "    for tid in trajid_list:\n",
    "        t = traj_dict[tid]\n",
    "        if len(t) > 1:\n",
    "            for pi in range(len(t)-1):\n",
    "                p1 = t[pi]\n",
    "                p2 = t[pi+1]\n",
    "                assert(p1 in poi_info.index and p2 in poi_info.index)\n",
    "                c1 = poi_clusters.loc[p1, 'clusterID']\n",
    "                c2 = poi_clusters.loc[p2, 'clusterID']\n",
    "                transmat_neighbor_cnt.loc[c1, c2] += 1\n",
    "    return normalise_transmat(transmat_neighbor_cnt), poi_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gen_transmat_neighbor(trajid_set_all, traj_dict, poi_info_all)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Transition Matrix between POIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate transition probabilities (matrix) between different POI features (vector) using the [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product) of individual transition matrix corresponding to each feature, i.e., POI category, POI popularity (discritized), POI average visit duration (discritized) and POI neighborhoods (clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with features without corresponding POIs and feature with more than one corresponding POIs. (*Before Normalisation*)\n",
    "- For features without corresponding POIs, just remove the rows and columns from the matrix obtained by Kronecker product.\n",
    "- For different POIs with the exact same feature, \n",
    "  - Let POIs with the same feature as a POI group,\n",
    "  - The *incoming* **transition value (i.e., unnormalised transition probability)** of this POI group \n",
    "    should be divided uniformly among the group members, \n",
    "    *which corresponds to choose a group member uniformly at random in the incoming case*.\n",
    "  - The *outgoing* transition value should be duplicated (i.e., the same) among all group members, \n",
    "    **as we were already in that group in the outgoing case**.\n",
    "  - For each POI in the group, the allocation transition value of the *self-loop of the POI group* is similar to \n",
    "    that in the *outgoing* case, **as we were already in that group**, so just duplicate and then divide uniformly among \n",
    "    the transitions from this POI to other POIs in the same group, \n",
    "    *which corresponds to choose a outgoing transition uniformly at random from all outgoing transitions\n",
    "    excluding the self-loop of this POI*.\n",
    "- **Concretely**, for a POI group with $n$ POIs, \n",
    "    1. If the *incoming* transition value of POI group is $m_1$,\n",
    "       then the corresponding *incoming* transition value for each group member is $\\frac{m_1}{n}$.\n",
    "    1. If the *outgoing* transition value of POI group is $m_2$,\n",
    "       then the corresponding *outgoing* transition value for each group member is also $m_2$.\n",
    "    1. If the transition value of *self-loop of the POI group* is $m_3$,\n",
    "       then transition value of *self-loop of individual POIs* should be $0$,  \n",
    "       and *other in-group transitions* with value $\\frac{m_3}{n-1}$\n",
    "       as the total number of outgoing transitions to other POIs in the same group is $n-1$ (excluding the self-loop),\n",
    "       i.e. $n-1$ choose $1$.\n",
    "       \n",
    "**NOTE**: execute the above division before or after row normalisation will lead to the same result, *as the division itself does NOT change the normalising constant of each row (i.e., the sum of each row before normalising)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_poi_logtransmat(trajid_list, poi_set, traj_dict, poi_info, debug=False):\n",
    "    transmat_cat                        = gen_transmat_cat(trajid_list, traj_dict, poi_info)\n",
    "    transmat_pop,      logbins_pop      = gen_transmat_pop(trajid_list, traj_dict, poi_info)\n",
    "    transmat_visit,    logbins_visit    = gen_transmat_visit(trajid_list, traj_dict, poi_info)\n",
    "    transmat_duration, logbins_duration = gen_transmat_duration(trajid_list, traj_dict, poi_info)\n",
    "    transmat_neighbor, poi_clusters     = gen_transmat_neighbor(trajid_list, traj_dict, poi_info)\n",
    "\n",
    "    # Kronecker product\n",
    "    transmat_ix = list(itertools.product(transmat_cat.index, transmat_pop.index, transmat_visit.index, \\\n",
    "                                         transmat_duration.index, transmat_neighbor.index))\n",
    "    transmat_value = transmat_cat.values\n",
    "    for transmat in [transmat_pop, transmat_visit, transmat_duration, transmat_neighbor]:\n",
    "        transmat_value = kron(transmat_value, transmat.values)\n",
    "    transmat_feature = pd.DataFrame(data=transmat_value, index=transmat_ix, columns=transmat_ix)\n",
    "    \n",
    "    poi_train = sorted(poi_set)\n",
    "    feature_names = ['poiCat', 'popularity', 'nVisit', 'avgDuration', 'clusterID']\n",
    "    poi_features = pd.DataFrame(data=np.zeros((len(poi_train), len(feature_names))), \\\n",
    "                                columns=feature_names, index=poi_train)\n",
    "    poi_features.index.name = 'poiID'\n",
    "    poi_features['poiCat'] = poi_info.loc[poi_train, 'poiCat']\n",
    "    poi_features['popularity'] = np.digitize(poi_info.loc[poi_train, 'popularity'], logbins_pop)\n",
    "    poi_features['nVisit'] = np.digitize(poi_info.loc[poi_train, 'nVisit'], logbins_visit)\n",
    "    poi_features['avgDuration'] = np.digitize(poi_info.loc[poi_train, 'avgDuration'], logbins_duration)\n",
    "    poi_features['clusterID'] = poi_clusters.loc[poi_train, 'clusterID']\n",
    "    \n",
    "    # shrink the result of Kronecker product and deal with POIs with the same features\n",
    "    poi_logtransmat = pd.DataFrame(data=np.zeros((len(poi_train), len(poi_train)), dtype=np.float), \\\n",
    "                                   columns=poi_train, index=poi_train)\n",
    "    for p1 in poi_logtransmat.index:\n",
    "        rix = tuple(poi_features.loc[p1])\n",
    "        for p2 in poi_logtransmat.columns:\n",
    "            cix = tuple(poi_features.loc[p2])\n",
    "            value_ = transmat_feature.loc[(rix,), (cix,)]\n",
    "            poi_logtransmat.loc[p1, p2] = value_.values[0, 0]\n",
    "    \n",
    "    # group POIs with the same features\n",
    "    features_dup = dict()\n",
    "    for poi in poi_features.index:\n",
    "        key = tuple(poi_features.loc[poi])\n",
    "        if key in features_dup:\n",
    "            features_dup[key].append(poi)\n",
    "        else:\n",
    "            features_dup[key] = [poi]\n",
    "    if debug == True:\n",
    "        for key in sorted(features_dup.keys()):\n",
    "            print(key, '->', features_dup[key])\n",
    "            \n",
    "    # deal with POIs with the same features\n",
    "    for feature in sorted(features_dup.keys()):\n",
    "        n = len(features_dup[feature])\n",
    "        if n > 1:\n",
    "            group = features_dup[feature]\n",
    "            v1 = poi_logtransmat.loc[group[0], group[0]]  # transition value of self-loop of POI group\n",
    "            \n",
    "            # divide incoming transition value (i.e. unnormalised transition probability) uniformly among group members\n",
    "            for poi in group:\n",
    "                poi_logtransmat[poi] /= n\n",
    "                \n",
    "            # outgoing transition value has already been duplicated (value copied above)\n",
    "            \n",
    "            # duplicate & divide transition value of self-loop of POI group uniformly among all outgoing transitions,\n",
    "            # from a POI to all other POIs in the same group (excluding POI self-loop)\n",
    "            v2 = v1 / (n - 1)\n",
    "            for pair in itertools.permutations(group, 2):\n",
    "                poi_logtransmat.loc[pair[0], pair[1]] = v2\n",
    "                            \n",
    "    # normalise each row\n",
    "    for p1 in poi_logtransmat.index:\n",
    "        poi_logtransmat.loc[p1, p1] = 0\n",
    "        rowsum = poi_logtransmat.loc[p1].sum()\n",
    "        assert(rowsum > 0)\n",
    "        logrowsum = np.log10(rowsum)\n",
    "        for p2 in poi_logtransmat.columns:\n",
    "            if p1 == p2:\n",
    "                poi_logtransmat.loc[p1, p2] = LOG_ZERO  # deal with log(0) explicitly\n",
    "            else:\n",
    "                poi_logtransmat.loc[p1, p2] = np.log10(poi_logtransmat.loc[p1, p2]) - logrowsum\n",
    "    \n",
    "    return poi_logtransmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#transmat_ = gen_poi_logtransmat(trajid_set_all, set(poi_info_all.index), traj_dict, poi_info_all, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Viterbi Decoding vs ILP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dynamic programming to find a possibly non-simple path, i.e., walk.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can include/exclude `startPOI` and `endPOI` when evaluating intermediate POIs in dynamic programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_viterbi(V, E, ps, pe, L, withNodeWeight=False, alpha=0.5, withStartEndIntermediate=False):\n",
    "    assert(isinstance(V, pd.DataFrame))\n",
    "    assert(isinstance(E, pd.DataFrame))\n",
    "    assert(ps in V.index)\n",
    "    assert(pe in V.index)\n",
    "    assert(2 < L <= V.index.shape[0])  \n",
    "    if withNodeWeight == True:\n",
    "        assert(0 < alpha < 1)\n",
    "        beta = 1 - alpha\n",
    "    else:\n",
    "        alpha = 0\n",
    "        beta = 1\n",
    "        weightkey = 'weight'\n",
    "        if weightkey not in V.columns:\n",
    "            V['weight'] = 1  # dummy weights, will not be used as alpha=0\n",
    "    if withStartEndIntermediate == True:\n",
    "        excludes = [ps]\n",
    "    else:\n",
    "        excludes = [ps, pe]\n",
    "    \n",
    "    A = pd.DataFrame(data=np.zeros((L-1, V.shape[0]), dtype=np.float), columns=V.index, index=np.arange(2, L+1))\n",
    "    B = pd.DataFrame(data=np.zeros((L-1, V.shape[0]), dtype=np.int),   columns=V.index, index=np.arange(2, L+1))\n",
    "    A += np.inf\n",
    "    for v in V.index:            \n",
    "        if v not in excludes:\n",
    "            A.loc[2, v] = alpha * (V.loc[ps, 'weight'] + V.loc[v, 'weight']) + beta * E.loc[ps, v]  # ps--v\n",
    "            B.loc[2, v] = ps\n",
    "    \n",
    "    for l in range(3, L+1):\n",
    "        for v in V.index:\n",
    "            if withStartEndIntermediate == True: # ps-~-v1---v \n",
    "                values = [A.loc[l-1, v1] + alpha * V.loc[v, 'weight'] + beta * E.loc[v1, v] for v1 in V.index]\n",
    "            else: # ps-~-v1---v \n",
    "                values = [A.loc[l-1, v1] + alpha * V.loc[v, 'weight'] + beta * E.loc[v1, v] \\\n",
    "                          if v1 not in [ps, pe] else -np.inf for v1 in V.index] # exclude ps and pe\n",
    "            \n",
    "            maxix = np.argmax(values)\n",
    "            A.loc[l, v] = values[maxix]\n",
    "            B.loc[l, v] = V.index[maxix]\n",
    "            \n",
    "    path = [pe]\n",
    "    v = path[-1]\n",
    "    l = L\n",
    "    while l >= 2:\n",
    "        path.append(B.loc[l, v])\n",
    "        v = path[-1]\n",
    "        l -= 1\n",
    "    path.reverse()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use integer linear programming (ILP) to find a simple path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_ILP(V, E, ps, pe, L, withNodeWeight=False, alpha=0.5):\n",
    "    assert(isinstance(V, pd.DataFrame))\n",
    "    assert(isinstance(E, pd.DataFrame))\n",
    "    assert(ps in V.index)\n",
    "    assert(pe in V.index)\n",
    "    assert(2 < L <= V.index.shape[0])\n",
    "    if withNodeWeight == True:\n",
    "        assert(0 < alpha < 1)\n",
    "    beta = 1 - alpha\n",
    "    \n",
    "    p0 = str(ps); pN = str(pe); N = V.index.shape[0]\n",
    "    \n",
    "    # REF: pythonhosted.org/PuLP/index.html\n",
    "    pois = [str(p) for p in V.index] # create a string list for each POI\n",
    "    pb = pulp.LpProblem('MostLikelyTraj', pulp.LpMaximize) # create problem\n",
    "    # visit_i_j = 1 means POI i and j are visited in sequence\n",
    "    visit_vars = pulp.LpVariable.dicts('visit', (pois, pois), 0, 1, pulp.LpInteger) \n",
    "    # a dictionary contains all dummy variables\n",
    "    dummy_vars = pulp.LpVariable.dicts('u', [x for x in pois if x != p0], 2, N, pulp.LpInteger)\n",
    "    \n",
    "    # add objective\n",
    "    objlist = []\n",
    "    if withNodeWeight == True:\n",
    "        objlist.append(alpha * V.loc[int(p0), 'weight'])\n",
    "    for pi in [x for x in pois if x != pN]:     # from\n",
    "        for pj in [y for y in pois if y != p0]: # to\n",
    "            if withNodeWeight == True:\n",
    "                objlist.append(visit_vars[pi][pj] * (alpha * V.loc[int(pj), 'weight'] + beta * E.loc[int(pi), int(pj)]))\n",
    "            else:\n",
    "                objlist.append(visit_vars[pi][pj] * E.loc[int(pi), int(pj)])\n",
    "    pb += pulp.lpSum(objlist), 'Objective'\n",
    "    \n",
    "    # add constraints, each constraint should be in ONE line\n",
    "    pb += pulp.lpSum([visit_vars[p0][pj] for pj in pois if pj != p0]) == 1, 'StartAt_p0'\n",
    "    pb += pulp.lpSum([visit_vars[pi][pN] for pi in pois if pi != pN]) == 1, 'EndAt_pN'\n",
    "    if p0 != pN:\n",
    "        pb += pulp.lpSum([visit_vars[pi][p0] for pi in pois]) == 0, 'NoIncoming_p0'\n",
    "        pb += pulp.lpSum([visit_vars[pN][pj] for pj in pois]) == 0, 'NoOutgoing_pN'\n",
    "    pb += pulp.lpSum([visit_vars[pi][pj] for pi in pois if pi != pN for pj in pois if pj != p0]) == L-1, 'Length'\n",
    "    for pk in [x for x in pois if x not in {p0, pN}]:\n",
    "        pb += pulp.lpSum([visit_vars[pi][pk] for pi in pois if pi != pN]) == \\\n",
    "              pulp.lpSum([visit_vars[pk][pj] for pj in pois if pj != p0]), 'ConnectedAt_' + pk\n",
    "        pb += pulp.lpSum([visit_vars[pi][pk] for pi in pois if pi != pN]) <= 1, 'Enter_' + pk + '_AtMostOnce'\n",
    "        pb += pulp.lpSum([visit_vars[pk][pj] for pj in pois if pj != p0]) <= 1, 'Leave_' + pk + '_AtMostOnce'\n",
    "    for pi in [x for x in pois if x != p0]:\n",
    "        for pj in [y for y in pois if y != p0]:\n",
    "            pb += dummy_vars[pi] - dummy_vars[pj] + 1 <= (N - 1) * (1 - visit_vars[pi][pj]), \\\n",
    "                    'SubTourElimination_' + pi + '_' + pj\n",
    "    #pb.writeLP(\"traj_tmp.lp\")\n",
    "    # solve problem: solver should be available in PATH\n",
    "    if USE_GUROBI == True:\n",
    "        gurobi_options = [('TimeLimit', '7200'), ('Threads', str(N_JOBS)), ('NodefileStart', '0.2'), ('Cuts', '2')]\n",
    "        pb.solve(pulp.GUROBI_CMD(path='gurobi_cl', options=gurobi_options)) # GUROBI\n",
    "    else:\n",
    "        pb.solve(pulp.COIN_CMD(path='cbc', options=['-threads', str(N_JOBS), '-strategy', '1', '-maxIt', '2000000']))#CBC\n",
    "    visit_mat = pd.DataFrame(data=np.zeros((len(pois), len(pois)), dtype=np.float), index=pois, columns=pois)\n",
    "    for pi in pois:\n",
    "        for pj in pois: visit_mat.loc[pi, pj] = visit_vars[pi][pj].varValue\n",
    "\n",
    "    # build the recommended trajectory\n",
    "    recseq = [p0]\n",
    "    while True:\n",
    "        pi = recseq[-1]\n",
    "        pj = visit_mat.loc[pi].idxmax()\n",
    "        assert(round(visit_mat.loc[pi, pj]) == 1)\n",
    "        recseq.append(pj)\n",
    "        if pj == pN: return [int(x) for x in recseq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectory Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring POIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_features(startPOI, nPOI, poi_info, poi_clusters, cats, clusters):\n",
    "    \"\"\"\n",
    "    Generate feature vectors for all POIs given query (startPOI, nPOI)\n",
    "    \"\"\"\n",
    "    assert(isinstance(cats, list))\n",
    "    assert(isinstance(clusters, list))\n",
    "    \n",
    "    columns = DF_COLUMNS[3:]\n",
    "    poi_distmat = POI_DISTMAT\n",
    "    query_id_dict = QUERY_ID_DICT\n",
    "    key = (p0, trajLen) = (startPOI, nPOI)\n",
    "    assert(key in query_id_dict)\n",
    "    assert(p0 in poi_info.index)\n",
    "    \n",
    "    df_ = pd.DataFrame(index=np.arange(poi_info.shape[0]), columns=columns)\n",
    "    poi_list = sorted(poi_info.index)\n",
    "        \n",
    "    for i in range(df_.index.shape[0]):\n",
    "        poi = poi_list[i]\n",
    "        lon, lat = poi_info.loc[poi, 'poiLon'], poi_info.loc[poi, 'poiLat']\n",
    "        pop, nvisit = poi_info.loc[poi, 'popularity'], poi_info.loc[poi, 'nVisit']\n",
    "        cat, cluster = poi_info.loc[poi, 'poiCat'], poi_clusters.loc[poi, 'clusterID']\n",
    "        duration = poi_info.loc[poi, 'avgDuration']\n",
    "        idx = df_.index[i]\n",
    "        df_.loc[idx, 'poiID'] = poi\n",
    "        df_.set_value(idx, 'category', tuple((cat == np.array(cats)).astype(np.int) * 2 - 1))\n",
    "        df_.set_value(idx, 'neighbourhood', tuple((cluster == np.array(clusters)).astype(np.int) * 2 - 1))\n",
    "        df_.loc[idx, 'popularity'] = LOG_SMALL if pop < 1 else np.log10(pop)\n",
    "        df_.loc[idx, 'nVisit'] = LOG_SMALL if nvisit < 1 else np.log10(nvisit)\n",
    "        df_.loc[idx, 'avgDuration'] = LOG_SMALL if duration < 1 else np.log10(duration)\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_all.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi, p0]\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'sameNeighbourhoodStart'] = 1 if cluster == poi_clusters.loc[p0, 'clusterID'] else -1\n",
    "        \n",
    "    # convert to matrix\n",
    "    nrows = df_.shape[0]\n",
    "    ncols = df_.shape[1] + len(cats) + len(clusters) - 2\n",
    "    \n",
    "    # features other than category and neighbourhood\n",
    "    X = df_[list(set(df_.columns) - {'category', 'neighbourhood'})].values  \n",
    "    \n",
    "    # boolean features: category (+1, -1)\n",
    "    cat_features = np.vstack([list(df_.loc[x, 'category']) for x in df_.index])\n",
    "    \n",
    "    # boolean features: neighbourhood (+1, -1)\n",
    "    neigh_features = np.vstack([list(df_.loc[x, 'neighbourhood']) for x in df_.index])\n",
    "    \n",
    "    return np.hstack([X, cat_features.astype(np.float), neigh_features.astype(np.float)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_labels(traj_truth, poi_info, binarise=False):\n",
    "    \"\"\"\n",
    "    Generate labels for all POIs given a ground truth trajectory\n",
    "    \"\"\"\n",
    "    poi_list = sorted(poi_info.index)\n",
    "    ranks = np.zeros(len(poi_list), dtype=np.float)\n",
    "    for j in range(len(poi_list)):\n",
    "        try:\n",
    "            poi = poi_list[j]\n",
    "            ranks[j] = (len(poi_list) - traj_truth.index(poi)) / len(poi_list) # normalise\n",
    "        except ValueError:\n",
    "            pass # default rank is 0\n",
    "        \n",
    "    if binarise == True:\n",
    "        return (ranks > 0).astype(np.float)*2 - 1 # binary labels (+1, -1)\n",
    "    \n",
    "    return ranks.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function for pairwise POI ranking using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "cpdef cost_logpwr(w, X, Y, long C, long M):\n",
    "    \"\"\"\n",
    "    w - parameter vector\n",
    "    X - feature matrix for all training examples (features of all POIs for the 1st example, then 2nd, ...)\n",
    "    Y - labels/ranks for all training examples (labels of all POIs for the 1st example, then 2nd, ...)\n",
    "    C - regularisation constant\n",
    "    M - total number of POIs\n",
    "    \"\"\"\n",
    "    assert(C > 0)\n",
    "    assert(M > 0)\n",
    "    cdef N, D, i, pj, pk, ix_j, ix_k\n",
    "    N = int(np.shape(X)[0]/M)\n",
    "    D = np.shape(X)[1]\n",
    "    assert(D == np.shape(w)[0])\n",
    "    assert(np.shape(X)[0] == np.shape(Y)[0])\n",
    "    \n",
    "    cdef double result, l_jk\n",
    "    result = 0.0\n",
    "    for i in range(N):\n",
    "        for pj in range(M):\n",
    "            ix_j = i*M + pj  # index of feature vector/label for POI pj\n",
    "            for pk in range(M):\n",
    "                if pj == pk:\n",
    "                    #result += np.log(2)  # constant\n",
    "                    continue\n",
    "                ix_k = i*M + pk\n",
    "                l_jk = 1.0\n",
    "                if Y[ix_j] < Y[ix_k]: l_jk = -1.0\n",
    "                result += np.log1p(np.exp(-1.0 * l_jk * np.dot(w, (X[ix_j] - X[ix_k])))) \n",
    "    \n",
    "    return 0.5 * np.dot(w, w) + result * C / np.shape(X)[0]  # note the normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of cost function for pairwise POI ranking using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "cpdef grad_logpwr(w, X, Y, long C, long M):\n",
    "    \"\"\"\n",
    "    w - parameter vector\n",
    "    X - feature matrix for all training examples (features of all POIs for the 1st example, then 2nd, ...)\n",
    "    Y - labels/ranks for all training examples (labels of all POIs for the 1st example, then 2nd, ...)\n",
    "    C - regularisation constant\n",
    "    M - total number of POIs\n",
    "    \"\"\"\n",
    "    assert(C > 0)\n",
    "    assert(M > 0)\n",
    "    cdef int N, D, i, pj, pk, ix_j, ix_k\n",
    "    N = int(np.shape(X)[0]/M)\n",
    "    D = np.shape(X)[1]\n",
    "    assert(D == np.shape(w)[0])\n",
    "    assert(np.shape(X)[0] == np.shape(Y)[0])\n",
    "    \n",
    "    cdef double l_jk\n",
    "    grad = np.zeros(D, dtype=np.float)\n",
    "    for i in range(N):\n",
    "        for pj in range(M):\n",
    "            ix_j = i*M + pj  # index of feature vector/label for POI pj\n",
    "            for pk in range(M):\n",
    "                if pj == pk: continue\n",
    "                ix_k = i*M + pk\n",
    "                l_jk = 1.0\n",
    "                if Y[ix_j] < Y[ix_k]: l_jk = -1.0\n",
    "                term = l_jk * (X[ix_j] - X[ix_k])  # vector\n",
    "                grad = grad + term * (-1.0 / (1.0 + np.exp(np.dot(w, term))))\n",
    "    \n",
    "    return w + grad * C / np.shape(X)[0]  # note the normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POI occurrence prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI occurrence prediction: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_logreg == True:\n",
    "    recdict_logreg = dict()\n",
    "    logreg_dict = dict()\n",
    "    cnt = 1\n",
    "    for i in range(len(trajid_set_all)):\n",
    "        tid = trajid_set_all[i]\n",
    "        te = traj_dict[tid]\n",
    "        if len(te) < 2: continue\n",
    "        trajid_list_train = trajid_set_all[:i] + trajid_set_all[i+1:]\n",
    "        poi_info = calc_poi_info(trajid_list_train, traj_all, poi_all)\n",
    "        poi_list = sorted(poi_info.index)\n",
    "        if not (te[0] in poi_info.index): continue\n",
    "        \n",
    "        print('%d: %s ->' % (cnt, te)); cnt += 1; sys.stdout.flush()\n",
    "        \n",
    "        # if this query has been processed, use the cached recommendation directly\n",
    "        query = (te[0], len(te)) \n",
    "        if query in logreg_dict:\n",
    "            recdict_logreg[tid] = {'REAL':te, 'PRED':logreg_dict[query]}\n",
    "            print(' '*10, 'LogReg:', logreg_dict[query]); sys.stdout.flush()\n",
    "            continue\n",
    "            \n",
    "        train_traj_list = [traj_dict[x] for x in trajid_list_train if len(traj_dict[x]) >= 2]\n",
    "        feature_list = Parallel(n_jobs=N_JOBS)\\\n",
    "                               (delayed(gen_features)(tr[0], len(tr), poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                        cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "                                for tr in train_traj_list)\n",
    "        labels_logreg = Parallel(n_jobs=N_JOBS)\\\n",
    "                                (delayed(gen_labels)(tr, poi_info, binarise=True) for tr in train_traj_list)\n",
    "        X = np.vstack(feature_list)\n",
    "        Y_logreg = np.hstack(labels_logreg)\n",
    "        X_test = gen_features(te[0], len(te), poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                              cats=POI_CAT_LIST,clusters=POI_CLUSTER_LIST)\n",
    "        p0_ix = poi_list.index(te[0])\n",
    "        \n",
    "        # feature normalisation\n",
    "        if DO_NORM == True:\n",
    "            normalizer = Normalizer(copy=False)\n",
    "            X = normalizer.fit_transform(X)\n",
    "            X_test = normalizer.transform(X_test)\n",
    "\n",
    "        \n",
    "        # train and test\n",
    "        logreg = LogisticRegression(C=LOGIT_C, n_jobs=N_JOBS)\n",
    "        logreg.fit(X, Y_logreg)\n",
    "        scores_logreg = logreg.decision_function(X_test)\n",
    "        \n",
    "        # form recommendation\n",
    "        assert(len(scores_logreg) == len(poi_list))\n",
    "        scores_logreg[p0_ix] = -1e6  # mask the start POI\n",
    "        topk_logreg = list(np.argsort(-np.asarray(scores_logreg))[:len(te)-1])\n",
    "        pred_logreg = [te[0]] + list(np.array(poi_list)[topk_logreg])\n",
    "        \n",
    "        recdict_logreg[tid] = {'REAL':te, 'PRED':pred_logreg}\n",
    "        logreg_dict[query] = pred_logreg\n",
    "        print(' '*10, 'LogReg:', pred_logreg); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct rank/location prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct rank/location prediction: linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_linreg == True:\n",
    "    recdict_linreg = dict()\n",
    "    linreg_dict = dict()\n",
    "    cnt = 1\n",
    "    for i in range(len(trajid_set_all)):\n",
    "        tid = trajid_set_all[i]\n",
    "        te = traj_dict[tid]\n",
    "        if len(te) < 2: continue\n",
    "        trajid_list_train = trajid_set_all[:i] + trajid_set_all[i+1:]\n",
    "        poi_info = calc_poi_info(trajid_list_train, traj_all, poi_all)\n",
    "        poi_list = sorted(poi_info.index)\n",
    "        if not (te[0] in poi_info.index): continue\n",
    "        \n",
    "        print('%d: %s ->' % (cnt, te)); cnt += 1; sys.stdout.flush()\n",
    "        \n",
    "        # if this query has been processed, use the cached recommendation directly\n",
    "        query = (te[0], len(te)) \n",
    "        if query in linreg_dict:\n",
    "            recdict_linreg[tid] = {'REAL':te, 'PRED':linreg_dict[query]}\n",
    "            print(' '*10, 'LinReg:', linreg_dict[query]); sys.stdout.flush()\n",
    "            continue\n",
    "            \n",
    "        train_traj_list = [traj_dict[x] for x in trajid_list_train if len(traj_dict[x]) >= 2]\n",
    "        feature_list = Parallel(n_jobs=N_JOBS)\\\n",
    "                               (delayed(gen_features)(tr[0], len(tr), poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                        cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "                                for tr in train_traj_list)\n",
    "        labels_linreg = Parallel(n_jobs=N_JOBS)\\\n",
    "                                (delayed(gen_labels)(tr, poi_info, binarise=False) for tr in train_traj_list)\n",
    "        X = np.vstack(feature_list)\n",
    "        Y_linreg = np.hstack(labels_linreg)\n",
    "        X_test = gen_features(te[0], len(te), poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                              cats=POI_CAT_LIST,clusters=POI_CLUSTER_LIST)\n",
    "        p0_ix = poi_list.index(te[0])\n",
    "        \n",
    "        # remove training examples with label '0', i.e., features of POIs that do not exist in trajectory\n",
    "        valid_ix = np.nonzero(Y_linreg)[0]\n",
    "        X = X[valid_ix, :]\n",
    "        Y_linreg = Y_linreg[valid_ix]\n",
    "        \n",
    "        # feature normalisation\n",
    "        if DO_NORM == True:\n",
    "            normalizer = Normalizer(copy=False)\n",
    "            X = normalizer.fit_transform(X)\n",
    "            X_test = normalizer.transform(X_test)\n",
    "\n",
    "         \n",
    "        # train and test\n",
    "        linreg = LinearRegression(normalize=True, n_jobs=N_JOBS)\n",
    "        linreg.fit(X, Y_linreg)\n",
    "        scores_linreg = linreg.predict(X_test)\n",
    "        \n",
    "        # form recommendation\n",
    "        assert(len(scores_linreg) == len(poi_list))\n",
    "        scores_linreg[p0_ix] = -1e6  # mask the start POI\n",
    "        topk_linreg = list(np.argsort(-np.asarray(scores_linreg))[:len(te)-1])\n",
    "        pred_linreg = [te[0]] + list(np.array(poi_list)[topk_linreg])\n",
    "        recdict_linreg[tid] = {'REAL':te, 'PRED':pred_linreg}\n",
    "        linreg_dict[query] = pred_linreg\n",
    "        print(' '*10, 'LinReg:', pred_linreg); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise POI ranking using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_logpwr == True:\n",
    "    recdict_logpwr = dict()\n",
    "    logpwr_dict = dict()\n",
    "    cnt = 1\n",
    "    for i in range(len(trajid_set_all)):\n",
    "        tid = trajid_set_all[i]\n",
    "        te = traj_dict[tid]\n",
    "        if len(te) < 2: continue\n",
    "        trajid_list_train = trajid_set_all[:i] + trajid_set_all[i+1:]\n",
    "        poi_info = calc_poi_info(trajid_list_train, traj_all, poi_all)\n",
    "        poi_list = sorted(poi_info.index)\n",
    "        if not (te[0] in poi_info.index): continue\n",
    "        \n",
    "        print('%d: %s ->' % (cnt, te)); cnt += 1; sys.stdout.flush()\n",
    "            \n",
    "        # if this query has been processed, use the cached recommendation directly\n",
    "        query = (te[0], len(te)) \n",
    "        if query in logpwr_dict:\n",
    "            recdict_logpwr[tid] = {'REAL':te, 'PRED':logpwr_dict[query]}\n",
    "            print(' '*10, 'LinPwr:', logpwr_dict[query]); sys.stdout.flush()\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        train_traj_list = [traj_dict[x] for x in trajid_list_train if len(traj_dict[x]) >= 2]\n",
    "        feature_list = Parallel(n_jobs=N_JOBS)\\\n",
    "                               (delayed(gen_features)(tr[0], len(tr), poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                        cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "                                for tr in train_traj_list)\n",
    "        labels_linreg = Parallel(n_jobs=N_JOBS)\\\n",
    "                                (delayed(gen_labels)(tr, poi_info, binarise=False) for tr in train_traj_list)\n",
    "        X = np.vstack(feature_list)\n",
    "        Y_linreg = np.hstack(labels_linreg)        \n",
    "        X_test = gen_features(te[0], len(te), poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                              cats=POI_CAT_LIST,clusters=POI_CLUSTER_LIST)\n",
    "        p0_ix = poi_list.index(te[0])\n",
    "        \n",
    "        # remove training examples with label '0', i.e., features of POIs that do not exist in trajectory\n",
    "        valid_ix = np.nonzero(Y_linreg)[0]\n",
    "        X = X[valid_ix, :]\n",
    "        Y_linreg = Y_linreg[valid_ix]\n",
    "        \n",
    "        # feature normalisation\n",
    "        if DO_NORM == True:\n",
    "            normalizer = Normalizer(copy=False)\n",
    "            X = normalizer.fit_transform(X)\n",
    "            X_test = normalizer.transform(X_test)\n",
    "        \n",
    "        # train and test\n",
    "        w = np.random.rand(np.shape(X)[1])*2  # initial guess\n",
    "        opt_method = 'Newton-CG' # 'BFGS'\n",
    "        opt = minimize(cost_logpwr, w, args=(X, Y_linreg, LOGIT_C, len(poi_list)), method=opt_method, jac=grad_logpwr, \\\n",
    "                       options={'gtol': 1e-6, 'disp': True})\n",
    "        if opt.success == True:\n",
    "            w = opt.x\n",
    "            scores_logpwr = 1.0 / (1.0 + np.exp(-1.0 * np.dot(X_test, w)))\n",
    "        \n",
    "            # form recommendation\n",
    "            assert(len(scores_logpwr) == len(poi_list))\n",
    "            scores_logpwr[p0_ix] = -1e6  # mask the start POI\n",
    "            topk_logpwr = list(np.argsort(-np.asarray(scores_logpwr))[:len(te)-1])\n",
    "            pred_logpwr = [te[0]] + list(np.array(poi_list)[topk_logpwr])\n",
    "            recdict_logpwr[tid] = {'REAL':te, 'PRED':pred_logpwr}\n",
    "            logpwr_dict[query] = pred_logpwr\n",
    "            print(' '*10, 'LinPwr:', pred_logpwr); sys.stdout.flush()\n",
    "            \n",
    "        else:\n",
    "            sys.stderr.write('LogPwr: Optimisation failed\\n')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RankSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_rank == True:\n",
    "    recdict_rank = dict()\n",
    "    cnt = 1\n",
    "    for i in range(len(trajid_set_all)):\n",
    "        tid = trajid_set_all[i]\n",
    "        te = traj_dict[tid]\n",
    "        \n",
    "        # trajectory is too short\n",
    "        if len(te) < 2: continue\n",
    "            \n",
    "        trajid_list_train = trajid_set_all[:i] + trajid_set_all[i+1:]\n",
    "        \n",
    "        poi_info = calc_poi_info(trajid_list_train, traj_all, poi_all)\n",
    "           \n",
    "        # start/end is not in training set\n",
    "        if not (te[0] in poi_info.index): continue\n",
    "        \n",
    "        print('%d: %s ->' % (cnt, te)); cnt += 1; sys.stdout.flush()\n",
    "        \n",
    "        # recommendation leveraging ranking\n",
    "        train_df = gen_train_df(trajid_list_train, traj_dict, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST, n_jobs=N_JOBS)\n",
    "        ranksvm = RankSVM(ranksvm_dir, useLinear=True)\n",
    "        ranksvm.train(train_df, cost=RANKSVM_C)\n",
    "        test_df = gen_test_df(te[0], len(te), poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                              cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "        rank_df = ranksvm.predict(test_df)\n",
    "\n",
    "        # POI popularity based ranking\n",
    "        poi_info.sort_values(by='popularity', ascending=False, inplace=True)\n",
    "        ranks1 = poi_info.index.tolist()\n",
    "        rank_pop = [te[0]] + [x for x in ranks1 if x not in {te[0]}][:len(te)-1]\n",
    "\n",
    "        # POI feature based ranking\n",
    "        rank_df.sort_values(by='rank', ascending=False, inplace=True)\n",
    "        ranks2 = rank_df.index.tolist()\n",
    "        rank_feature = [te[0]] + [x for x in ranks2 if x not in {te[0]}][:len(te)-1]\n",
    "\n",
    "        recdict_rank[tid] = {'REAL':te, 'REC_POP':rank_pop, 'REC_FEATURE':rank_feature}\n",
    "        print(' '*10, 'Rank POP:', rank_pop);print(' '*10, 'Rank POI:', rank_feature);sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend trajectories by leveraging POI-POI transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if run_tran == True:\n",
    "    recdict_tran = dict()\n",
    "    cnt = 1\n",
    "    for i in range(len(trajid_set_all)):\n",
    "        tid = trajid_set_all[i]\n",
    "        te = traj_dict[tid]\n",
    "        \n",
    "        # trajectory is too short\n",
    "        if len(te) < 3: continue\n",
    "            \n",
    "        trajid_list_train = trajid_set_all[:i] + trajid_set_all[i+1:]\n",
    "        \n",
    "        poi_info = calc_poi_info(trajid_list_train, traj_all, poi_all)\n",
    "           \n",
    "        # start/end is not in training set\n",
    "        if not (te[0] in poi_info.index and te[-1] in poi_info.index): continue\n",
    "        \n",
    "        print(te, '#%d ->' % cnt); cnt += 1; sys.stdout.flush()\n",
    "        \n",
    "        # recommendation leveraging transition probabilities\n",
    "        poi_logtransmat = gen_poi_logtransmat(trajid_list_train, set(poi_info.index), traj_dict, poi_info)\n",
    "        edges = poi_logtransmat.copy()\n",
    "\n",
    "        tran_dp = find_viterbi(poi_info.copy(), edges.copy(), te[0], te[-1], len(te))\n",
    "        tran_ilp = find_ILP(poi_info.copy(), edges.copy(), te[0], te[-1], len(te))\n",
    "\n",
    "        recdict_tran[tid] = {'REAL':te, 'REC_DP':tran_dp, 'REC_ILP':tran_ilp}\n",
    "        print(' '*10, 'Tran  DP:', tran_dp); print(' '*10, 'Tran ILP:', tran_ilp); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend trajectories by leveraging both POI ranking and POI-POI transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_comb == True:\n",
    "    short_traj_set = list(set(trajid_set_all) - set(PART1) - set(PART2))  # assume NO duplicated trajID\n",
    "    recdict_comb = dict()\n",
    "    cnt = 1\n",
    "    settings = [(PART1.copy(), PART2.copy()), (PART2.copy(), PART1.copy())]\n",
    "    for (validation_set, test_set) in settings:\n",
    "        # use validation set to tune alpha\n",
    "        alpha_cv = cv_choose_alpha(ALPHA_SET, validation_set, short_traj_set)\n",
    "        print('alpha:', alpha_cv)\n",
    "        \n",
    "        # leave-one-out cross validation on test set\n",
    "        # NOTE: validation set as well as short trajectories are included when training\n",
    "        for i in range(len(test_set)):\n",
    "            tid = test_set[i]\n",
    "            te = traj_dict[tid]\n",
    "            assert(len(te) >= 3)\n",
    "            \n",
    "            trajid_list_train = list(short_traj_set) + list(validation_set) + list(test_set[:i]) + list(test_set[i+1:])\n",
    "            poi_info = calc_poi_info(trajid_list_train, traj_all, poi_all)\n",
    "           \n",
    "            # start/end is not in training set\n",
    "            if not (te[0] in poi_info.index and te[-1] in poi_info.index): continue\n",
    "\n",
    "            print(te, '#%d ->' % cnt); cnt += 1; sys.stdout.flush()\n",
    "        \n",
    "            train_df = gen_train_df(trajid_list_train, traj_dict, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                    cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST, n_jobs=N_JOBS)\n",
    "            ranksvm = RankSVM(ranksvm_dir, useLinear=True)\n",
    "            ranksvm.train(train_df, cost=RANKSVM_C)\n",
    "            test_df = gen_test_df(te[0], te[-1], len(te), poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                  cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "            rank_df = ranksvm.predict(test_df)\n",
    "            \n",
    "            poi_logtransmat = gen_poi_logtransmat(trajid_list_train, set(poi_info.index), traj_dict, poi_info)\n",
    "            edges = poi_logtransmat.copy()\n",
    "\n",
    "            # recommendation leveraging both ranking and transitions\n",
    "            nodes = rank_df.copy()\n",
    "            nodes['weight'] = np.log10(nodes['probability'])\n",
    "            nodes.drop('probability', axis=1, inplace=True)\n",
    "            comb_dp = find_viterbi(nodes.copy(), edges.copy(), te[0], te[-1],len(te),withNodeWeight=True,alpha=alpha_cv)\n",
    "            comb_ilp = find_ILP(nodes, edges, te[0], te[-1], len(te), withNodeWeight=True, alpha=alpha_cv)\n",
    "\n",
    "            recdict_comb[tid] = {'REAL':te, 'REC_DP':comb_dp, 'REC_ILP':comb_ilp}\n",
    "            print(' '*10, 'Comb  DP:', comb_dp); print(' '*10, 'Comb ILP:', comb_ilp); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_logreg == True:\n",
    "    F1_logreg = []; pF1_logreg = []\n",
    "    for key in sorted(recdict_logreg.keys()):\n",
    "        F1_logreg.append(calc_F1(recdict_logreg[key]['REAL'], recdict_logreg[key]['PRED']))\n",
    "        pF1_logreg.append(calc_pairsF1(recdict_logreg[key]['REAL'], recdict_logreg[key]['PRED']))\n",
    "    print('LogReg: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_logreg), np.std(F1_logreg)/np.sqrt(len(F1_logreg)), \\\n",
    "           np.mean(pF1_logreg), np.std(pF1_logreg)/np.sqrt(len(pF1_logreg))))\n",
    "    \n",
    "if run_linreg == True:\n",
    "    F1_linreg = []; pF1_linreg = []\n",
    "    for key in sorted(recdict_linreg.keys()):\n",
    "        F1_linreg.append(calc_F1(recdict_linreg[key]['REAL'], recdict_linreg[key]['PRED']))\n",
    "        pF1_linreg.append(calc_pairsF1(recdict_linreg[key]['REAL'], recdict_linreg[key]['PRED']))\n",
    "    print('LinReg: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_linreg), np.std(F1_linreg)/np.sqrt(len(F1_linreg)), \\\n",
    "           np.mean(pF1_linreg), np.std(pF1_linreg)/np.sqrt(len(pF1_linreg))))\n",
    "    \n",
    "if run_logpwr == True:\n",
    "    F1_logpwr = []; pF1_logpwr = []\n",
    "    for key in sorted(recdict_logpwr.keys()):\n",
    "        F1_logpwr.append(calc_F1(recdict_logpwr[key]['REAL'], recdict_logpwr[key]['PRED']))\n",
    "        pF1_logpwr.append(calc_pairsF1(recdict_logpwr[key]['REAL'], recdict_logpwr[key]['PRED']))\n",
    "    print('LogPwr: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_logpwr), np.std(F1_logpwr)/np.sqrt(len(F1_logpwr)), \\\n",
    "           np.mean(pF1_logpwr), np.std(pF1_logpwr)/np.sqrt(len(pF1_logpwr))))\n",
    "    \n",
    "if run_rank == True:\n",
    "    F11_rank = []; F12_rank = []; pF11_rank = []; pF12_rank = []\n",
    "    for key in sorted(recdict_rank.keys()):\n",
    "        F11_rank.append(calc_F1(recdict_rank[key]['REAL'], recdict_rank[key]['REC_POP']))\n",
    "        F12_rank.append(calc_F1(recdict_rank[key]['REAL'], recdict_rank[key]['REC_FEATURE']))\n",
    "        pF11_rank.append(calc_pairsF1(recdict_rank[key]['REAL'], recdict_rank[key]['REC_POP']))\n",
    "        pF12_rank.append(calc_pairsF1(recdict_rank[key]['REAL'], recdict_rank[key]['REC_FEATURE']))\n",
    "    print('Rank POP: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f)' % \\\n",
    "          (np.mean(F11_rank), np.std(F11_rank)/np.sqrt(len(F11_rank)), \\\n",
    "           np.mean(pF11_rank), np.std(pF11_rank)/np.sqrt(len(pF11_rank))))\n",
    "    print('Rank FOI: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f)' % \\\n",
    "          (np.mean(F12_rank), np.std(F12_rank)/np.sqrt(len(F12_rank)), \\\n",
    "           np.mean(pF12_rank), np.std(pF12_rank)/np.sqrt(len(pF12_rank))))\n",
    "    \n",
    "if run_tran == True:\n",
    "    F11_tran = []; F12_tran = []; pF11_tran = []; pF12_tran = []\n",
    "    for tid in sorted(recdict_tran.keys()):\n",
    "        F11_tran.append(calc_F1(recdict_tran[tid]['REAL'], recdict_tran[tid]['REC_DP']))\n",
    "        F12_tran.append(calc_F1(recdict_tran[tid]['REAL'], recdict_tran[tid]['REC_ILP']))\n",
    "        pF11_tran.append(calc_pairsF1(recdict_tran[tid]['REAL'], recdict_tran[tid]['REC_DP']))\n",
    "        pF12_tran.append(calc_pairsF1(recdict_tran[tid]['REAL'], recdict_tran[tid]['REC_ILP']))\n",
    "    print('Tran  DP: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f)' % \\\n",
    "          (np.mean(F11_tran), np.std(F11_tran)/np.sqrt(len(F11_tran)), \\\n",
    "           np.mean(pF11_tran), np.std(pF11_tran)/np.sqrt(len(pF11_tran))))\n",
    "    print('Tran ILP: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f)' % \\\n",
    "          (np.mean(F12_tran), np.std(F12_tran)/np.sqrt(len(F12_tran)), \\\n",
    "           np.mean(pF12_tran), np.std(pF12_tran)/np.sqrt(len(pF12_tran))))\n",
    "\n",
    "if run_comb == True:\n",
    "    F11_comb = []; F12_comb = []; pF11_comb = []; pF12_comb = []\n",
    "    for tid in sorted(recdict_comb.keys()):\n",
    "        F11_comb.append(calc_F1(recdict_comb[tid]['REAL'], recdict_comb[tid]['REC_DP']))\n",
    "        F12_comb.append(calc_F1(recdict_comb[tid]['REAL'], recdict_comb[tid]['REC_ILP']))\n",
    "        pF11_comb.append(calc_pairsF1(recdict_comb[tid]['REAL'], recdict_comb[tid]['REC_DP']))\n",
    "        pF12_comb.append(calc_pairsF1(recdict_comb[tid]['REAL'], recdict_comb[tid]['REC_ILP']))\n",
    "    print('Comb  DP: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f)' % \\\n",
    "          (np.mean(F11_comb), np.std(F11_comb)/np.sqrt(len(F11_comb)), \\\n",
    "           np.mean(pF11_comb), np.std(pF11_comb))/np.sqrt(len(pF11_comb)))\n",
    "    print('Comb ILP: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f)' % \\\n",
    "          (np.mean(F12_comb), np.std(F12_comb)/np.sqrt(len(F12_comb)), \\\n",
    "           np.mean(pF12_comb), np.std(pF12_comb)/np.sqrt(len(pF12_comb))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_logreg == True: pickle.dump(recdict_logreg, open(frecdict_logreg, 'bw'))\n",
    "if run_linreg == True: pickle.dump(recdict_linreg, open(frecdict_linreg, 'bw'))\n",
    "if run_logpwr == True: pickle.dump(recdict_logpwr, open(frecdict_logpwr, 'bw'))\n",
    "if run_rank == True: pickle.dump(recdict_rank, open(frecdict_rank, 'bw'))\n",
    "if run_tran == True: pickle.dump(recdict_tran, open(frecdict_tran, 'bw'))\n",
    "if run_comb == True: pickle.dump(recdict_comb, open(frecdict_comb, 'bw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DO_NORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Random Guessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the two approaches of random guessing: combinatorial and experimental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import comb\n",
    "from math import factorial\n",
    "def rand_guess(npoi, length):\n",
    "    assert(length <= npoi)\n",
    "    if length == npoi: return 1\n",
    "    N = npoi - 2\n",
    "    m = length - 2 # number of correct POIs\n",
    "    k = m\n",
    "    expected_F1 = 0\n",
    "    while k >= 0:\n",
    "        F1 = (k + 2) / length\n",
    "        prob = comb(m, k) * comb(N-m, m-k) / comb(N, m)\n",
    "        expected_F1 += prob * F1\n",
    "        k -= 1\n",
    "    return expected_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rand_guess(20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1_rand1 = []\n",
    "F1_rand2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_rand == True:\n",
    "    recdict_rand = dict()\n",
    "    cnt = 1\n",
    "    total0 = traj_all[traj_all['trajLen'] > 2]['trajID'].unique().shape[0]\n",
    "    poi_dict = dict()\n",
    "    for tid in trajid_set_all:\n",
    "        tr = extract_traj(tid, traj_all)\n",
    "        for poi in tr:\n",
    "            if poi in poi_dict: poi_dict[poi] += 1\n",
    "            else: poi_dict[poi] = 1\n",
    "    \n",
    "    for i in range(len(trajid_set_all)):\n",
    "        tid = trajid_set_all[i]\n",
    "        t = extract_traj(tid, traj_all)\n",
    "        \n",
    "        # trajectory is too short\n",
    "        if len(t) < 3: continue\n",
    "            \n",
    "        pois = [x for x in sorted(poi_dict.keys()) if poi_dict[x] > 1]\n",
    "            \n",
    "        # start/end is not in training set\n",
    "        if not (t[0] in pois and t[-1] in pois): continue\n",
    "        \n",
    "        print_progress(cnt, total0); cnt += 1\n",
    "        \n",
    "        F1_rand1.append(rand_guess(len(pois), len(t)))\n",
    "        pois1 = [x for x in pois if x not in {t[0], t[-1]}]\n",
    "        rec_ix = np.random.choice(len(pois1), len(t)-2, replace=False)\n",
    "        rec_rand = [t[0]] + list(np.array(pois1)[rec_ix]) + [t[-1]]\n",
    "        F1_rand2.append(calc_F1(t, rec_rand))\n",
    "        recdict_rand[tid] = {'REAL': t, 'REC_RAND': rec_rand}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if run_rand == True:\n",
    "    pickle.dump(recdict_rand, open(frecdict_rand, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_rand == True:\n",
    "    print('Combinatorial F1: mean=%.3f, std=%.3f' % (np.mean(F1_rand1), np.std(F1_rand1)))\n",
    "    print('Experimental  F1: mean=%.3f, std=%.3f' % (np.mean(F1_rand2), np.std(F1_rand2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kendall's $\\tau$ as evaluation metric: The ranks of all POIs in trajectory $\\mathbf{y}$ should be greater than all other POIs that do not appear in trajectory $\\mathbf{y}$, and we require that they have the same rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_rank(y, M, default_rank):\n",
    "    \"\"\"\n",
    "    compute the rank of all POIs given a trajectory\n",
    "    y - trajectory: a sequence of POIs without duplication\n",
    "    M - total number of POIs\n",
    "    default_rank - the rank for all POIs do not appear in y\n",
    "    \"\"\"\n",
    "    assert(len(y) > 0)\n",
    "    assert(len(y) <= M)\n",
    "    assert(default_rank >= 0)\n",
    "    assert(default_rank <= M)\n",
    "    rank = np.ones(M) * default_rank\n",
    "    for j in range(len(y)):\n",
    "        poi = y[j]\n",
    "        prank = M - j\n",
    "        rank[poi - 1] = prank\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y1 = [1, 3, 2, 5]\n",
    "y2 = [1, 2, 5, 3]\n",
    "M = 10\n",
    "default = 0\n",
    "print(gen_rank(y1, M, default))\n",
    "print(gen_rank(y2, M, default))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the effect of the rank for all POIs not in trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for default in np.arange(0, 10.5, .5):\n",
    "    assert(len(y1) == len(y2))\n",
    "    if default >= M - len(y1) + 1: continue\n",
    "    r1 = gen_rank(y1, M, default)\n",
    "    r2 = gen_rank(y2, M, default)\n",
    "    #print(r1, r2)\n",
    "    print(default, kendalltau(r1, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#default = 7\n",
    "#r1 = gen_rank(y1, M, default)\n",
    "#r2 = gen_rank(y2, M, default)\n",
    "#print(r1, r2)\n",
    "#print(default, kendalltau(r1, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Structured Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Structured Predition using PyStruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyse the process of using structured SVM to training a CRF and make preditions on new instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the 1-slack formulation (with margin rescaling) of structured SVM is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\min_{\\mathbf{w}, \\xi \\ge 0} & \\frac{1}{2} \\mathbf{w}^T \\mathbf{w} + C \\xi \\\\\n",
    "s.t. \\forall(\\bar{y}_1, \\dots, \\bar{y}_n) \\in \\mathcal{Y}^n: & \n",
    "\\frac{1}{n} \\mathbf{w}^T \\sum_{i=1}^n \\left( \\Psi(x_i, y_i) - \\Psi(x_i, \\bar{y}_i) \\right) \\ge \n",
    "\\frac{1}{n} \\sum_{i=1}^n \\Delta(y_i, \\bar{y}_i) - \\xi\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where \n",
    "- $\\mathbf{w}$ is the parameter vector\n",
    "- $\\Psi(x_i, y_i)$ is the joint feature (vector) related to example $x_i$ and its label $y_i$\n",
    "- The size of $\\mathbf{w}$ is the same as $\\Psi(x_i, y_i)$\n",
    "- $\\Delta(\\centerdot)$ is the loss function, here we use Hamming loss, i.e., per-variable 0-1 loss, as indicated by function [loss()](https://github.com/pystruct/pystruct/blob/master/pystruct/models/base.py) and [fit()](https://github.com/pystruct/pystruct/blob/master/pystruct/learners/one_slack_ssvm.py).\n",
    "- $n$ is the total number of training examples, $C$ is the regularisation parameter, $\\xi$ is the slack variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before introducing the training and prediction procedure, we define some concepts that will be used later.\n",
    "- `n_states`: #states for all variables, (this is the total number of unique POIs in training set here).\n",
    "- `n_features`: #features per node, (this is the number of POI features, i.e., the ranking probabilities of all POIs).\n",
    "- `n_edges`: #edges in each training/test example, (this is the number of POIs in a trajectory).\n",
    "- `n_edge_features`: #features per edge, (this is the number of features for each transition, \n",
    "   i.e., the out-going transition probabilities to all POIs).\n",
    "- $x$ is made up of three parts: (`node_features`, `edges`, `edge_features`).\n",
    "- `node_features`: `n_nodes` $\\times$ `n_features`\n",
    "- `edge_features`: `n_edges` $\\times$ `n_edge_features`\n",
    "- `edges`: `n_edges` $\\times$ $2$, e.g. for trajectory `[3, 1, 2]` and `[5, 9, 6]`, their `edges` are the same matrix\n",
    "   `[[0, 1], [1, 2]]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For [EdgeFeatureGraphCRF](https://pystruct.github.io/generated/pystruct.models.EdgeFeatureGraphCRF.html), the pairwise potentials are asymmetric and shared over all edges, and the size of \n",
    "- **Parameter vector $\\mathbf{w}$: `n_states` $\\times$ `n_features` $+$ `n_edge_features` $\\times$ `(n_states)`$^2$**\n",
    "- The first part of $\\mathbf{w}$, let's call it **`unary_params`**: `n_states` $\\times$ `n_features`, is the parameters \n",
    "  used to compute unary potentials.\n",
    "- The second part of $\\mathbf{w}$, let's call it **`pairwise_params`**: `n_edge_features` $\\times$ `(n_states)`$^2$, \n",
    "  is the parameters used to compute pairwise potentials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the joint feature vector $\\Psi(x, y)$\n",
    "\n",
    "When training a CRF using [OneSlackSSVM](https://pystruct.github.io/generated/pystruct.learners.OneSlackSSVM.html), we need to compute the joint feature vector $\\Psi(x, y)$ for each training example, it is computed \n",
    "(for EdgeFeatureGraphCRF in PyStruct) as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unary part of $\\Psi(x, y)$**:\n",
    "- make one-hot encoding of $y$, its size: `n_nodes` $\\times$ `n_states`\n",
    "- *value*: $y^T \\times$ `node_features`\n",
    "- *dimension*: `(n_nodes` $\\times$ `n_states)`$^T$ $\\times$ `(n_nodes` $\\times$ `n_features)` \n",
    "  $\\to$ `(n_states` $\\times$ `n_features)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pairwise Part of $\\Psi(x, y)$**:\n",
    "- make one-hot encoding of `edges`, its size: `n_edges` $\\times$ `(n_states)`$^2$\n",
    "- *value*: `edge_features`$^T$ $\\times$ `edges`\n",
    "- *dimension*: `(n_edges` $\\times$ `n_edge_features)`$^T$ $\\times$ `(n_edges` $\\times$ `(n_states)`$^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for each training example, $\\Psi(x_i, y_i)$ = `[unary part, pairwise part]`, solve the above QP problem (1-slack formulation) to get a parameter vector $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a trajectory is chain structured, so we use `max-product` belief propagation (Viterbi algorithm in this case) to do inference in the trained CRF.  \n",
    "To predict the label of a new instance $x$, we need to compute the unary potential and pairwise potential of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unary potential:** \n",
    "- *value*: `node_features` $\\times$ `(unary_params)`$^T$ (first part of $\\mathbf{w}$)\n",
    "- *dimension*: `(n_nodes` $\\times$ `n_features)` $\\times$ `(n_states` $\\times$ `n_features)`$^T$ $\\to$ \n",
    "  `(n_nodes` $\\times$ `n_states)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pairwise potential:**\n",
    "- *value*: `edge_features` $\\times$ `pairwise_params` (second part of $\\mathbf{w}$)\n",
    "- *dimension*: `(n_edges` $\\times$ `n_edge_features)` $\\times$ `(n_edge_features` $\\times$ `n_states`$^2$ `)`, \n",
    "  reshape to `(n_edges` $\\times$ `n_states` $\\times$ `n_states)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With unary potential and pairwise potential computed, as we could know from `edges` that our example $x$ is chain structured, so we do inference using Viterbi algorithm to compute the most likely label of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Node Features - POI/Query Specific Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a trajectory `[start, ..., end]`, the features used to train/test are those that used to rank POIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyStruct](https://pystruct.github.io/) assumes that [label `y` is a discrete vector](https://pystruct.github.io/intro.html) and [pystruct.learners assume labels `y` are integers starting with `0`](https://github.com/pystruct/pystruct/issues/114), concretely,\n",
    "- values in label vector $y$ should satisfy $y_i \\in Y$, \n",
    "  where $Y$ is the **index** of a discrete value space, and the index starts at 0.\n",
    "- label vector $y$ will be [transformed to one hot encoding (see function `joint_feature()`)](https://github.com/pystruct/pystruct/blob/master/pystruct/models/graph_crf.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if labels in training set is `[[1, 2], [0, 4, 9]]`, \n",
    "then it will cause an index out of bounds error as pystruct did something like this,\n",
    "1. construct an discrete value space: \n",
    "   - `set([1, 2] + [0, 4, 9]) -> {0, 1, 2, 4, 9}`\n",
    "   - `size({0, 1, 2, 4, 9}) = 5`\n",
    "1. convert labels using one hot encoding: \n",
    "   - label vector `[1, 2]` will be converted to a matrix of shape $2 \\times 5$,\n",
    "     with cells at `(0, 1), (1, 2)` set to `1` and others set to `0`.\n",
    "   - label vector `[0, 4, 9]` will be converted to a matrix of shape $3 \\times 5$,\n",
    "     with cells at `(0, 0)`, `(1, 4)`, **`(2, 9)` INDEX_OUT_OF_BOUNDS** set to `1` and others set to `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus need to build a mapping for POIs: *POI_ID $\\to$ POI_INDEX* with POIs in trajectories in training set, also a map of the reverse direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the joint features (when training) linearly to `[-1, 1]`, i.e., for feature $x$, we fit a linear function\n",
    "\\begin{equation}\n",
    "    f(x) = ax + b \n",
    "\\end{equation}\n",
    "such that \n",
    "\\begin{equation}\n",
    "    a x_\\texttt{max} + b = +1\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    a x_\\texttt{min} + b = -1\n",
    "\\end{equation}\n",
    "\n",
    "Solve the above linear equations result in a function\n",
    "\\begin{equation}\n",
    "    f(x) = -1 + \\frac{2(x-x_\\texttt{min})}\n",
    "                     {x_\\texttt{max} - x_\\texttt{min}}\n",
    "\\end{equation}\n",
    "This approach is used by [libsvm and ranksvm](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ranksvm/liblinear-ranksvm-2.1.zip), one can find the code at lines `349-383` in `svm-scale.c` (function `output` and `output_target`).\n",
    "\n",
    "In addition, for features with uniform values, we set them to `0`, i.e., \n",
    "\\begin{equation}\n",
    "    \\textbf{if}~ x_\\texttt{max} == x_\\texttt{min} ~\\textbf{then}~ f(x) = 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "#%%cython -a\n",
    "cimport numpy as np # for np.ndarray\n",
    "import numpy as np # for np.shape\n",
    "\n",
    "cpdef tuple scale_features_linear(\n",
    "    np.ndarray[dtype=np.float64_t, ndim=2] node_features, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=3] edge_features, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=2] node_max, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=2] node_min, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=3] edge_max, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=3] edge_min):\n",
    "    \n",
    "    # DEBUG\n",
    "    #return (node_features, edge_features)\n",
    "    \n",
    "    assert(np.shape(node_features) == np.shape(node_max) == np.shape(node_min))\n",
    "    assert(np.shape(edge_features) == np.shape(edge_max) == np.shape(edge_min))\n",
    "    \n",
    "    # x_max == x_min means feature x has uniform (such as constant) values, i.e. x == x_max == x_min\n",
    "    #node_delta = node_max - node_min\n",
    "    #edge_delta = edge_max - edge_min\n",
    "    #node_delta[np.abs(node_delta) < 1e-9] = 1.\n",
    "    #edge_delta[np.abs(edge_delta) < 1e-9] = 1.\n",
    "    #return (2 * np.divide(x0-node_min, node_delta) - 1, 2 * np.divide(x1-edge_min, edge_delta) - 1)\n",
    "\n",
    "    #TODO: loop-over each element using cython\n",
    "    # max <=1 and min >= -1 and x in [-1, 1], no need to scale\n",
    "    # max == min, set x = 0\n",
    "    # boolean features, no scaling\n",
    "    cdef int I, J, K, M, N\n",
    "    M, N = np.shape(node_features)\n",
    "    for m in range(M):\n",
    "        for n in range(N):\n",
    "            # skip features distributed in [-1, 1] and single-valued features \n",
    "            if (node_max[m, n] > 1. or node_min[m, n] < -1.) and node_max[m, n] - node_min[m, n] > 1e-6:\n",
    "                    node_features[m, n] = 2. * (node_features[m,n]-node_min[m,n]) / (node_max[m,n]-node_min[m,n]) - 1\n",
    "                         \n",
    "            #if node_max[m, n] < 1.1 and node_min[m, n] > -1.1 and -1.1 < node_features[m, n] < 1.1: continue\n",
    "            #elif np.fabs(node_max[m, n] - node_min[m, n]) < 1e-9: node_features[m, n] = 0. #continue\n",
    "            #else: node_features[m, n] = 2. * (node_features[m,n]-node_min[m,n]) / (node_max[m,n]-node_min[m,n]) - 1\n",
    "            \n",
    "    I, J, K = np.shape(edge_features)\n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "            for k in range(K):\n",
    "                #if edge_max[i,j,k] < 1.1 and edge_min[i,j,k] > -1.1 and -1.1 < edge_features[i,j,k] < 1.1: continue\n",
    "                #elif np.fabs(edge_max[i,j,k] - edge_min[i,j,k]) < 1e-9: edge_features[i,j,k] = 0. #continue\n",
    "                #else:edge_features[i,j,k]=2.*(edge_features[i,j,k]-edge_min[i,j,k])/(edge_max[i,j,k]-edge_min[i,j,k])-1\n",
    "                if (edge_max[i,j,k] > 1. or edge_min[i,j,k] < -1.) and edge_max[i,j,k] - edge_min[i,j,k] > 1e-6:\n",
    "                        edge_features[i,j,k] = \\\n",
    "                        2. * (edge_features[i,j,k] - edge_min[i,j,k]) / (edge_max[i,j,k] - edge_min[i,j,k]) - 1\n",
    "    return (node_features, edge_features)\n",
    "\n",
    "\n",
    "cpdef scale_vector(np.ndarray[dtype=np.float64_t, ndim=1] features_unscaled, \n",
    "                              np.ndarray[dtype=np.float64_t, ndim=1] features_max, \n",
    "                              np.ndarray[dtype=np.float64_t, ndim=1] features_min):\n",
    "    # DEBUG\n",
    "    return features_unscaled\n",
    "\n",
    "    assert(np.shape(features_unscaled) == np.shape(features_max) == np.shape(features_min))\n",
    "    cdef int N, n\n",
    "    N = np.shape(features_unscaled)[0]\n",
    "    feature_scaled = np.zeros(N, dtype=np.float64)\n",
    "    \n",
    "    for n in range(N):\n",
    "        if (features_max[n] > 1. or features_min[n] < -1.) and features_max[n] - features_min[n] > 1e-6:\n",
    "            feature_scaled[n] = -1 + 2. * (features_unscaled[n]-features_min[n]) / (features_max[n]-features_min[n])\n",
    "    return feature_scaled\n",
    "\n",
    "\n",
    "cpdef tuple scale_features_norm(\n",
    "    np.ndarray[dtype=np.float64_t, ndim=2] node_features, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=3] edge_features, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=2] node_mean, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=2] node_std, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=3] edge_mean, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=3] edge_std):\n",
    "    \n",
    "    assert(np.shape(node_features) == np.shape(node_mean) == np.shape(node_std))\n",
    "    assert(np.shape(edge_features) == np.shape(edge_mean) == np.shape(edge_std))\n",
    "    \n",
    "    #return (np.divide(x0-node_means, node_stds), np.divide(x1-edge_means, edge_stds))\n",
    "    \n",
    "    cdef int I, J, K, M, N\n",
    "    cdef int i, j, k, m, n\n",
    "    M, N = np.shape(node_features)\n",
    "    for m in range(M):\n",
    "        for n in range(N):\n",
    "            # skip single-valued features\n",
    "            if np.fabs(node_std[m, n]) > 1e-6:\n",
    "                node_features[m, n] = (node_features[m, n] - node_mean[m, n]) / node_std[m, n]\n",
    "            \n",
    "    I, J, K = np.shape(edge_features)\n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "            for k in range(K):\n",
    "                if np.fabs(edge_std[i, j, k]) > 1e-6:\n",
    "                    edge_features[i, j, k] = (edge_features[i, j, k] - edge_mean[i, j, k]) / edge_std[i, j, k]\n",
    "    \n",
    "    return (node_features, edge_features)\n",
    "\n",
    "\n",
    "cpdef tuple build_joint_feature(np.ndarray[dtype=np.float64_t, ndim=2] node_features,\n",
    "                                np.ndarray[dtype=np.float64_t, ndim=3] edge_features,\n",
    "                                np.ndarray[dtype=np.float64_t, ndim=2] node_max, \n",
    "                                np.ndarray[dtype=np.float64_t, ndim=2] node_min, \n",
    "                                np.ndarray[dtype=np.float64_t, ndim=3] edge_max, \n",
    "                                np.ndarray[dtype=np.float64_t, ndim=3] edge_min,\n",
    "                                np.ndarray[dtype=np.long_t, ndim=1] y):\n",
    "    cdef int j\n",
    "    L = np.shape(y)[0]\n",
    "\n",
    "    unary_features = np.zeros(np.shape(node_features), dtype=np.float)\n",
    "    pw_features = np.zeros(np.shape(edge_features), dtype=np.float)\n",
    "    \n",
    "    unary_features[y[0], :] = node_features[y[0], :]\n",
    "    \n",
    "    for j in range(L-1):\n",
    "        ss, tt = y[j], y[j+1]\n",
    "        unary_features[tt, :] = node_features[tt, :]\n",
    "        pw_features[ss, tt, :] = edge_features[ss, tt, :]\n",
    "\n",
    "    # feature scaling here\n",
    "    ret = scale_features_linear(unary_features, pw_features,\n",
    "                                node_max=node_max, node_min=node_min,\n",
    "                                edge_max=edge_max, edge_min=edge_min)\n",
    "    return (ret[0], ret[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nf = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float)\n",
    "ef = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]).astype(np.float)\n",
    "nmax = np.array([[2, 2, 3], [5, 5, 6]]).astype(np.float)\n",
    "nmin = np.array([[1, 1, 2], [3, 3, 3]]).astype(np.float)\n",
    "emax = np.array([[[2, 2], [4, 4]], [[6, 6], [9, 8]]]).astype(np.float)\n",
    "emin = np.array([[[1, 1], [2, 2]], [[3, 3], [3, 3]]]).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scale_features_linear(nf, ef, nmax, nmin, emax, emin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "#%%cython -a\n",
    "\n",
    "cpdef float loss_F1(y, y_hat):\n",
    "    assert(len(y) > 0)\n",
    "    assert(len(y_hat) > 0)\n",
    "    cdef float intersize, recall, precision, F1\n",
    "    intersize = len(set(y) & set(y_hat))\n",
    "    recall = intersize / len(y)\n",
    "    precision = intersize / len(y_hat)\n",
    "    F1 = 2. * precision * recall / (precision + recall)\n",
    "    return 1 - F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "#%%cython -a\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "cpdef float loss_pairsF1(y, y_hat):\n",
    "    #np.ndarray[np.long_t, ndim=1] y, np.ndarray[np.long_t, ndim=1] y_hat\n",
    "    assert(len(y) > 0)\n",
    "    assert(len(y) == len(set(y))) # no loops in y\n",
    "    cdef int n, nr, n0, n0r, nc, poi1, poi2, i, j\n",
    "    n = len(y)\n",
    "    nr = len(y_hat)\n",
    "    n0 = int(n*(n-1) / 2)\n",
    "    n0r = int(nr*(nr-1) / 2)\n",
    "    \n",
    "    # y determines the correct visiting order\n",
    "    order_dict = dict()\n",
    "    for i in range(n):\n",
    "        order_dict[y[i]] = i\n",
    "        \n",
    "    nc = 0\n",
    "    for i in range(nr):\n",
    "        poi1 = y_hat[i]\n",
    "        for j in range(i+1, nr):\n",
    "            poi2 = y_hat[j]\n",
    "            if poi1 in order_dict and poi2 in order_dict and poi1 != poi2:\n",
    "                if order_dict[poi1] < order_dict[poi2]: nc += 1\n",
    "\n",
    "    cdef float precision, recall, F1\n",
    "    precision = (1.0 * nc) / (1.0 * n0r)\n",
    "    recall = (1.0 * nc) / (1.0 * n0)\n",
    "    if nc == 0:\n",
    "        F1 = 0\n",
    "    else:\n",
    "        F1 = 2. * precision * recall / (precision + recall)\n",
    "    return 1 - F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def max_Hammingloss(y_true, n_states=5):\n",
    "    L = len(y_true)\n",
    "    N = n_states\n",
    "    A = np.zeros((L, N), dtype=np.int)\n",
    "    B = np.zeros((L, N), dtype=np.int)\n",
    "    \n",
    "    for n in range(N): # 0--n\n",
    "        A[0, n] = int(n != y_true[0])\n",
    "        B[0, n] = 0\n",
    "    for l in range(L-1):\n",
    "        for n in range(N): # 0~~m--n\n",
    "            loss = int(n != y_true[l+1])\n",
    "            maxix = np.argmax(A[l])\n",
    "            A[l+1, n] = A[l, maxix] + loss\n",
    "            B[l+1, n] = np.arange(N)[maxix]\n",
    "    y = [np.arange(N)[np.argmax(A[L-1, N-1])]]\n",
    "    n, l = y[-1], L-1\n",
    "    while l > 0:\n",
    "        y.append(B[l, n])\n",
    "        n, l = y[-1], l-1\n",
    "    y.reverse()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_Hammingloss([1, 0, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference for SSVM: loss-augmented inference for cutting-plane training and inference for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N0, L0 = 5, 3\n",
    "w_u = np.array([1, 2, 3, 2, 3]).reshape((N0, 1))\n",
    "f_u = np.array([2, 1, 1, 3, 1]).reshape((N0, 1))\n",
    "w_p = np.array([1,1,1,1,3, 1,1,1,2,1, 1,3,1,1,1, 2,1,1,1,1, 1,1,3,1,1]).reshape((N0, N0, 1))\n",
    "f_p = np.array([1,2,1,1,1, 1,1,1,1,3, 2,1,1,1,1, 1,1,3,1,1, 1,1,1,2,1]).reshape((N0, N0, 1))\n",
    "ps0, y_true0 = 1, [1, 0, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference using the Viterbi algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_inference_viterbi0(ps, L, N, unary_params, pw_params, unary_features, pw_features, y_true=None):\n",
    "    assert(L > 1)\n",
    "    assert(N >= L)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < N)\n",
    "    \n",
    "    A = np.zeros((L-1, N), dtype=np.float) # scores matrix\n",
    "    B = np.zeros((L-1, N), dtype=np.int)   # backtracking pointers\n",
    "    \n",
    "    for p in range(N): # ps--p\n",
    "        A[0, p] = np.dot(unary_params[p, :], unary_features[p]) + \\\n",
    "                  np.dot(pw_params[ps, p, :], pw_features[ps, p, :]) if p != ps else -np.inf\n",
    "        #if y_true is not None and p != ps: A[0, p] += float(p != y_true[1])/L  # loss term: normalised\n",
    "        if y_true is not None and p != ps: A[0, p] += float(p != y_true[1])\n",
    "        B[0, p] = ps\n",
    "\n",
    "    for l in range(0, L-2):\n",
    "        for p in range(N):                \n",
    "            # should NOT be the start/end POI for intermediate POIs, NO self-loops\n",
    "            #loss = float(p != y_true[l+2])/L if y_true is not None else 0  # loss term: normlised\n",
    "            loss = float(p != y_true[l+2]) if y_true is not None else 0\n",
    "            scores = [A[l, p1] + np.dot(unary_params[p, :], unary_features[p, :]) + \\\n",
    "                      np.dot(pw_params[p1, p, :], pw_features[p1, p, :])\n",
    "                      #for p1 in range(N)] # ps~~p1--p\n",
    "                      if p1 not in [ps, p] else -np.inf for p1 in range(N)] # ps~~p1--p\n",
    "            maxix = np.argmax(scores)\n",
    "            A[l+1, p] = scores[maxix] + loss\n",
    "            B[l+1, p] = np.array(range(N))[maxix]\n",
    "\n",
    "    y_hat = [np.argmax(A[L-2, :])]\n",
    "    p, l = y_hat[-1], L-2\n",
    "    while l >= 0:\n",
    "        y_hat.append(B[l, p])\n",
    "        p, l = y_hat[-1], l-1\n",
    "    y_hat.reverse()\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_inference_viterbi(ps, L, N, unary_params, pw_params, unary_features, pw_features, y_true=None):\n",
    "    assert(L > 1)\n",
    "    assert(N >= L)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < N)\n",
    "    \n",
    "    Cu = np.zeros(N, dtype=np.float)      # unary_param[p] x unary_features[p]\n",
    "    Cp = np.zeros((N, N), dtype=np.float) # pw_param[pi, pj] x pw_features[pi, pj]\n",
    "    # a intermediate POI should NOT be the start POI, NO self-loops\n",
    "    for pi in range(N):\n",
    "        Cu[pi] = np.dot(unary_params[pi, :], unary_features[pi, :]) if pi != ps else -np.inf\n",
    "        for pj in range(N):\n",
    "            Cp[pi, pj] = -np.inf if (pj == ps or pi == pj) else np.dot(pw_params[pi, pj, :], pw_features[pi, pj, :])\n",
    "    \n",
    "    A = np.zeros((L-1, N), dtype=np.float)     # scores matrix\n",
    "    B = np.ones((L-1, N), dtype=np.int) * (-1) # backtracking pointers\n",
    "    \n",
    "    for p in range(N): # ps--p\n",
    "        A[0, p] = Cu[p] + Cp[ps, p]\n",
    "        #if y_true is not None and p != ps: A[0, p] += float(p != y_true[1])/L  # loss term: normalised\n",
    "        if y_true is not None and p != ps: A[0, p] += float(p != y_true[1])\n",
    "        B[0, p] = ps\n",
    "\n",
    "    for t in range(0, L-2):\n",
    "        for p in range(N):\n",
    "            #loss = float(p != y_true[l+2])/L if y_true is not None else 0  # loss term: normlised\n",
    "            loss = float(p != y_true[t+2]) if y_true is not None else 0\n",
    "            scores = [A[t, p1] + Cu[p] + Cp[p1, p] for p1 in range(N)] # ps~~p1--p\n",
    "            maxix = np.argmax(scores)\n",
    "            A[t+1, p] = scores[maxix] + loss\n",
    "            #B[l+1, p] = np.array(range(N))[maxix]\n",
    "            B[t+1, p] = maxix\n",
    "\n",
    "    y_hat = [np.argmax(A[L-2, :])]\n",
    "    p, t = y_hat[-1], L-2\n",
    "    while t >= 0:\n",
    "        y_hat.append(B[t, p])\n",
    "        p, t = y_hat[-1], t-1\n",
    "    y_hat.reverse()\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(do_inference_viterbi(ps0, L0, N0, w_u, w_p, f_u, f_p))\n",
    "print(do_inference_viterbi(ps0, L0, N0, w_u, w_p, f_u, f_p, y_true=y_true0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference using the List Viterbi algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = np.array([1, 3, 2, 6, 5])\n",
    "np.argsort(-aa)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bb = np.zeros((1, 2, 3), dtype=np.float)\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bb = np.vstack([bb, np.ones((1, 2, 3), dtype=np.float)])\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bb[1, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd = np.ones((2,2), dtype=np.int)\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd = dd * (-1)\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[i for i in range(2) for j in range(0+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = np.array([1, 3, 2, 6, 3])\n",
    "aa[ np.argsort(aa)[-0-1] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_dummy, L_dummy = 90, 10\n",
    "np.power(N_dummy, L_dummy-1) - np.prod([N_dummy - kx + 1 for kx in range(2, L_dummy+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bb = np.array([[[1, 2, 3], [1,1,2]]])\n",
    "np.argmax(bb[0, 1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_inference_listViterbi(ps, L, N, unary_params, pw_params, unary_features, pw_features, y_true=None, debug=False):\n",
    "    assert(L > 1)\n",
    "    assert(N >= L)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < N)\n",
    "    \n",
    "    Cu = np.zeros(N, dtype=np.float)      # unary_param[p] x unary_features[p]\n",
    "    Cp = np.zeros((N, N), dtype=np.float) # pw_param[pi, pj] x pw_features[pi, pj]\n",
    "    # a intermediate POI should NOT be the start POI, NO self-loops\n",
    "    for pi in range(N):\n",
    "        Cu[pi] = np.dot(unary_params[pi, :], unary_features[pi, :])# if pi != ps else -np.inf\n",
    "        for pj in range(N):\n",
    "            Cp[pi, pj] = -np.inf if (pj == ps or pi == pj) else np.dot(pw_params[pi, pj, :], pw_features[pi, pj, :])\n",
    "    \n",
    "    A = np.zeros((1, L, N), dtype=np.float)      # scores matrix\n",
    "    B = np.ones ((1, L, N), dtype=np.int) * (-1) # backtracking pointers\n",
    "    R = np.ones ((1, L, N), dtype=np.int) * (-1) # backtracking pointers (ranking index in ListViterbi)\n",
    "    \n",
    "    k = 0  # start from the best path (or walk)\n",
    "    k_limit = np.power(N, L-1) - np.prod([N - kx + 1 for kx in range(2, L+1)])\n",
    "    k_limit = np.min([k_limit, 10])\n",
    "    while k < k_limit:\n",
    "        print('k:', k)\n",
    "        \n",
    "        for p in range(N): # ps--p\n",
    "            loss = float(p != y_true[1]) if y_true is not None else 0\n",
    "            A[k, 1, p] = Cp[ps, p] + Cu[p] + loss\n",
    "            B[k, 1, p] = ps\n",
    "            \n",
    "        for p in range(N): # ps--p1--p\n",
    "            loss = float(p != y_true[2]) if y_true is not None else 0\n",
    "            scores = [A[k, 1, p1] + Cp[p1, p] + Cu[p] for p1 in range(N)]\n",
    "            maxix = np.argsort(scores)[-k-1] if k < len(scores) else 0\n",
    "            A[k, 2, p] = scores[maxix] + loss     # best score\n",
    "            B[k, 2, p] = maxix                    # best p1\n",
    "\n",
    "        for t in range(3, L):\n",
    "            for p in range(N): # ps~~p2--p\n",
    "                loss = float(p != y_true[t]) if y_true is not None else 0\n",
    "                scores = [A[k1, t-1, p2] + Cp[p2, p] + Cu[p] for k1 in range(k+1) for p2 in range(N)]\n",
    "                kmaxix = np.argsort(scores)[-k-1]\n",
    "                A[k, t, p] = scores[kmaxix] + loss     # best score\n",
    "                B[k, t, p] = kmaxix % N                # best p2\n",
    "                R[k, t, p] = int(np.floor(kmaxix / N)) # best k1\n",
    "                print('scores:', scores)\n",
    "                print('kmaxix:', kmaxix)\n",
    "                print('A[k, t, p]:', A[k, t, p])\n",
    "                print('B[k, t, p]:', B[k, t, p])\n",
    "                print('R[k, t, p]:', R[k, t, p])\n",
    "                #print('A:', A)\n",
    "                #print('B:', B); #print(B.shape)\n",
    "                #print('R:', R); #print(R.shape)\n",
    "        \n",
    "        # backtracking\n",
    "        last_scores = [A[k1, L-1, p1] for k1 in range(k+1) for p1 in range(N)]\n",
    "        last_kmaxix = np.argsort(last_scores)[-k-1]\n",
    "        jt = last_kmaxix % N\n",
    "        lt = int(np.floor(last_kmaxix / N))\n",
    "        #lt = k\n",
    "        y_hat = [jt]\n",
    "        t = L-1\n",
    "        while t > 0:\n",
    "            jt1, lt1 = jt, lt\n",
    "            jt = B[lt1, t, jt1]\n",
    "            lt = R[lt1, t, jt1]\n",
    "            t = t - 1\n",
    "            y_hat.append(jt)\n",
    "            \n",
    "        y_hat.reverse()\n",
    "        if debug == True: print(y_hat, '\\n--------------------\\n')\n",
    "        else:\n",
    "            if len(y_hat) == len(set(y_hat)): return y_hat\n",
    "\n",
    "        # prepare for the next iteration\n",
    "        A = np.vstack([A, np.zeros((1, L, N), dtype=np.float)])\n",
    "        B = np.vstack([B, np.ones ((1, L, N), dtype=np.int) * (-1)])\n",
    "        R = np.vstack([R, np.ones ((1, L, N), dtype=np.int) * (-1)])\n",
    "        k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "do_inference_listViterbi(ps0, L0, N0, w_u, w_p, f_u, f_p, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(do_inference_listViterbi(ps0, L0, N0, w_u, w_p, f_u, f_p))\n",
    "print(do_inference_listViterbi(ps0, L0, N0, w_u, w_p, f_u, f_p, y_true=y_true0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference using integer linear programming (ILP) to avoid sub-tours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_inference_ILP(ps, L, N, unary_params, pw_params, unary_features, pw_features, y_true=None):\n",
    "    assert(L > 1)\n",
    "    assert(N >= L)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < N)\n",
    "    p0 = str(ps)\n",
    "    \n",
    "    #print('===:', p0)\n",
    "    \n",
    "    pois = [str(p) for p in range(N)] # create a string list for each POI\n",
    "    pb = pulp.LpProblem('Inference_ILP', pulp.LpMaximize) # create problem\n",
    "    # visit_i_j = 1 means POI i and j are visited in sequence\n",
    "    visit_vars = pulp.LpVariable.dicts('visit', (pois, pois), 0, 1, pulp.LpInteger) \n",
    "    # isend_l = 1 means POI l is the END POI of trajectory\n",
    "    isend_vars = pulp.LpVariable.dicts('isend', pois, 0, 1, pulp.LpInteger) \n",
    "    # a dictionary contains all dummy variables\n",
    "    dummy_vars = pulp.LpVariable.dicts('u', [x for x in pois if x != p0], 2, N, pulp.LpInteger)\n",
    "    \n",
    "    # add objective\n",
    "    objlist = []\n",
    "    for pi in pois:     # from\n",
    "        for pj in pois: # to\n",
    "            objlist.append(visit_vars[pi][pj] * (np.dot(unary_params[int(pj)], unary_features[int(pj)]) + \\\n",
    "                                                 np.dot(pw_params[int(pi), int(pj)], pw_features[int(pi), int(pj)])))\n",
    "    if y_true is not None: # Loss: normalised number of mispredicted POIs, Hamming loss is non-linear of 'visit'\n",
    "        objlist.append(1)\n",
    "        for j in range(N):\n",
    "            pj = pois[j]\n",
    "            for k in range(1, L): \n",
    "                pk = str(y_true[k])\n",
    "                #objlist.append(-1.0 * visit_vars[pj][pk] / L) # loss term: normalised\n",
    "                objlist.append(-1.0 * visit_vars[pj][pk])\n",
    "    pb += pulp.lpSum(objlist), 'Objective'\n",
    "    \n",
    "    # add constraints, each constraint should be in ONE line\n",
    "    pb += pulp.lpSum([visit_vars[pi][pi] for pi in pois]) == 0, 'NoSelfLoops'\n",
    "    pb += pulp.lpSum([visit_vars[p0][pj] for pj in pois]) == 1, 'StartAt_p0'\n",
    "    pb += pulp.lpSum([visit_vars[pi][p0] for pi in pois]) == 0, 'NoIncoming_p0'\n",
    "    pb += pulp.lpSum([visit_vars[pi][pj] for pi in pois for pj in pois]) == L-1, 'Length'\n",
    "    pb += pulp.lpSum([isend_vars[pi] for pi in pois]) == 1, 'OneEnd'\n",
    "    pb += isend_vars[p0] == 0, 'StartNotEnd'\n",
    "    \n",
    "    for pk in [x for x in pois if x != p0]:\n",
    "        pb += pulp.lpSum([visit_vars[pi][pk] for pi in pois]) == isend_vars[pk] + \\\n",
    "              pulp.lpSum([visit_vars[pk][pj] for pj in pois if pj != p0]), 'ConnectedAt_' + pk\n",
    "        pb += pulp.lpSum([visit_vars[pi][pk] for pi in pois]) <= 1, 'Enter_' + pk + '_AtMostOnce'\n",
    "        pb += pulp.lpSum([visit_vars[pk][pj] for pj in pois if pj != p0]) + isend_vars[pk] <= 1, \\\n",
    "              'Leave_' + pk + '_AtMostOnce'\n",
    "    for pi in [x for x in pois if x != p0]:\n",
    "        for pj in [y for y in pois if y != p0]:\n",
    "            pb += dummy_vars[pi] - dummy_vars[pj] + 1 <= (N - 1) * (1 - visit_vars[pi][pj]), \\\n",
    "                    'SubTourElimination_' + pi + '_' + pj\n",
    "    #pb.writeLP(\"traj_tmp.lp\")\n",
    "    \n",
    "    # solve problem: solver should be available in PATH\n",
    "    if USE_GUROBI == True:\n",
    "        gurobi_options = [('TimeLimit', '7200'), ('Threads', str(N_JOBS)), ('NodefileStart', '0.2'), ('Cuts', '2')]\n",
    "        pb.solve(pulp.GUROBI_CMD(path='gurobi_cl', options=gurobi_options)) # GUROBI\n",
    "    else:\n",
    "        pb.solve(pulp.COIN_CMD(path='cbc', options=['-threads', str(N_JOBS), '-strategy', '1', '-maxIt', '2000000']))#CBC\n",
    "    visit_mat = pd.DataFrame(data=np.zeros((len(pois), len(pois)), dtype=np.float), index=pois, columns=pois)\n",
    "    isend_vec = pd.Series(data=np.zeros(len(pois), dtype=np.float), index=pois)\n",
    "    for pi in pois:\n",
    "        isend_vec.loc[pi] = isend_vars[pi].varValue\n",
    "        for pj in pois: visit_mat.loc[pi, pj] = visit_vars[pi][pj].varValue\n",
    "    #visit_mat.to_csv('visit.csv')\n",
    "\n",
    "    # build the recommended trajectory\n",
    "    recseq = [p0]\n",
    "    while True:\n",
    "        pi = recseq[-1]\n",
    "        pj = visit_mat.loc[pi].idxmax()\n",
    "        value = visit_mat.loc[pi, pj]\n",
    "        #print(value, int(round(value)))\n",
    "        #print(recseq)\n",
    "        assert(int(round(value)) == 1)\n",
    "        recseq.append(pj)\n",
    "        if len(recseq) == L: \n",
    "            assert(int(round(isend_vec[pj])) == 1)\n",
    "            #print('===:', recseq, ':====')\n",
    "            return [int(x) for x in recseq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(do_inference_ILP(ps0, L0, N0, w_u, w_p, f_u, f_p))\n",
    "print(do_inference_ILP(ps0, L0, N0, w_u, w_p, f_u, f_p, y_true=y_true0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyModel(StructuredModel):\n",
    "    \n",
    "    def __init__(self, n_states=None, n_features=None, n_edge_features=None):\n",
    "        self.inference_method = 'customized'\n",
    "        self.class_weight = None\n",
    "        self.inference_calls = 0\n",
    "        self.n_states = n_states\n",
    "        self.n_features = n_features\n",
    "        self.n_edge_features = n_edge_features\n",
    "        self._set_size_joint_feature()\n",
    "        self._set_class_weight()\n",
    "\n",
    "        \n",
    "    def _set_size_joint_feature(self):\n",
    "        if None not in [self.n_states, self.n_features, self.n_edge_features]:\n",
    "            self.size_joint_feature = self.n_states * self.n_features + \\\n",
    "                                      self.n_states * self.n_states * self.n_edge_features\n",
    "   \n",
    "\n",
    "    def loss(self, y, y_hat):\n",
    "        #return np.mean(np.asarray(y) != np.asarray(y_hat))     # hamming loss (normalised)\n",
    "        return np.sum(np.asarray(y) != np.asarray(y_hat))     # hamming loss\n",
    "        #return loss_F1(y, y_hat)      # F1 loss\n",
    "        #return loss_pairsF1(y, y_hat) # pairsF1 loss\n",
    "        #return loss_pairsF1(np.array(y), np.array(y_hat)) # pairsF1 loss\n",
    "\n",
    "    \n",
    "    def initialize(self, X, Y):\n",
    "        assert(len(X) == len(Y))\n",
    "        n_features = X[0][0].shape[1]\n",
    "        if self.n_features is None: \n",
    "            self.n_features = n_features\n",
    "        else:\n",
    "            assert(self.n_features == n_featurees)\n",
    "\n",
    "        n_states = len(np.unique(np.hstack([y.ravel() for y in Y])))\n",
    "        if self.n_states is None: \n",
    "            self.n_states = n_states\n",
    "        else:\n",
    "            assert(self.n_states == n_states)\n",
    "            \n",
    "        n_edge_features = X[0][1].shape[2]\n",
    "        if self.n_edge_features is None:\n",
    "            self.n_edge_features = n_edge_features\n",
    "        else:\n",
    "            assert(self.n_edge_features == n_edge_features)\n",
    "            \n",
    "        self._set_size_joint_feature()\n",
    "        self._set_class_weight()\n",
    "        \n",
    "        # joint feature scaling\n",
    "        n_samples = len(Y)\n",
    "        node_features_all = np.zeros((n_samples, self.n_states, self.n_features), dtype=np.float)\n",
    "        edge_features_all = np.zeros((n_samples, self.n_states, self.n_states, self.n_edge_features), dtype=np.float)\n",
    "        for ii in range(n_samples):\n",
    "            x0, x1, y = X[ii][0], X[ii][1], Y[ii]\n",
    "            node_features_all[ii, y[0], :] = x0[y[0], :]\n",
    "            for jj in range(len(y)-1):\n",
    "                ss, tt = y[jj], y[jj+1]\n",
    "                node_features_all[ii, tt, :] = x0[tt, :]\n",
    "                edge_features_all[ii, ss, tt, :] = x1[ss, tt, :]\n",
    "        \n",
    "        node_max = np.max(node_features_all, axis=0)\n",
    "        node_min = np.min(node_features_all, axis=0)\n",
    "        edge_max = np.max(edge_features_all, axis=0)\n",
    "        edge_min = np.min(edge_features_all, axis=0)\n",
    "        assert(node_max.shape == (self.n_states, self.n_features))\n",
    "        assert(node_min.shape == (self.n_states, self.n_features))\n",
    "        assert(edge_max.shape == (self.n_states, self.n_states, self.n_edge_features))\n",
    "        assert(edge_min.shape == (self.n_states, self.n_states, self.n_edge_features))\n",
    "        \n",
    "        #node_mean = np.mean(node_features_all, axis=0)\n",
    "        #edge_mean = np.mean(edge_features_all, axis=0)\n",
    "        #node_std = np.std(node_features_all, axis=0)\n",
    "        #edge_std = np.std(edge_features_all, axis=0)\n",
    "        #assert(node_mean.shape == (self.n_states, self.n_features))\n",
    "        #assert(node_std.shape  == (self.n_states, self.n_features))\n",
    "        #assert(edge_mean.shape == (self.n_states, self.n_states, self.n_edge_features))\n",
    "        #assert(edge_std.shape  == (self.n_states, self.n_states, self.n_edge_features))\n",
    "        \n",
    "        # save for scaling test data\n",
    "        self.node_max = node_max; self.node_min = node_min\n",
    "        self.edge_max = edge_max; self.edge_min = edge_min\n",
    "        #self.node_mean = node_mean; self.node_std = node_std\n",
    "        #self.edge_mean = edge_mean; self.edge_std = edge_std\n",
    "        \n",
    "        # scaling features\n",
    "        for ii in range(n_samples):\n",
    "            unaries, pw = scale_features_linear(X[ii][0], X[ii][1],\n",
    "                                                node_max=self.node_max, node_min=self.node_min,\n",
    "                                                edge_max=self.edge_max, edge_min=self.edge_min)\n",
    "            X[ii] = (unaries, pw, X[ii][2])\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\"%s(n_states: %d, inference_method: %s, n_features: %d, n_edge_features: %d)\"\n",
    "                % (type(self).__name__, self.n_states, self.inference_method, self.n_features, self.n_edge_features))\n",
    "    \n",
    "    \n",
    "    def joint_feature(self, x, y):\n",
    "        assert(not isinstance(y, tuple))\n",
    "        unary_features = x[0] # unary features of all POIs: n_POIs x n_features\n",
    "        pw_features = x[1]    # pairwise features of all transitions: n_POIs x n_POIs x n_edge_features\n",
    "        query = x[2]          # query = (startPOI, length)\n",
    "        n_nodes = query[1]\n",
    "        \n",
    "        #print('y:', y)\n",
    "        \n",
    "        #assert(unary_features.ndim == 2)\n",
    "        #assert(pw_features.ndim == 3)\n",
    "        #assert(len(query) == 3)\n",
    "        assert(n_nodes == len(y))\n",
    "        assert(unary_features.shape == (self.n_states, self.n_features))\n",
    "        assert(pw_features.shape == (self.n_states, self.n_states, self.n_edge_features))\n",
    "        \n",
    "        node_features = np.zeros((self.n_states, self.n_features), dtype=np.float)\n",
    "        edge_features = np.zeros((self.n_states, self.n_states, self.n_edge_features), dtype=np.float)\n",
    "        \n",
    "        node_features[y[0], :] = unary_features[y[0], :]\n",
    "        for j in range(len(y)-1):\n",
    "            ss, tt = y[j], y[j+1]\n",
    "            node_features[tt, :] = unary_features[tt, :]\n",
    "            edge_features[ss, tt, :] = pw_features[ss, tt, :]\n",
    "        \n",
    "        # sum node/edge features after scaling: \n",
    "        # equivalent to share parameters between features of different POIs/transitions\n",
    "        joint_feature_vector = np.hstack([node_features.ravel(), edge_features.ravel()])\n",
    "        \n",
    "        return joint_feature_vector\n",
    "            \n",
    "    \n",
    "    def loss_augmented_inference(self, x, y, w, relaxed=None):\n",
    "        #print('loss_augmented_inference:', y)\n",
    "        # inference procedure for training: (x, y) from training set (with features already scaled)\n",
    "        #\n",
    "        # argmax_y_hat np.dot(w, joint_feature(x, y_hat)) + loss(y, y_hat)\n",
    "        # \n",
    "        # the loss function should be decomposible in order to use Viterbi decoding, here we use Hamming loss\n",
    "        #\n",
    "        # x[0]: (unscaled) unary features of all POIs: n_POIs x n_features\n",
    "        # x[1]: (unscaled) pairwise features of all transitions: n_POIs x n_POIs x n_edge_features\n",
    "        # x[2]: query = (startPOI, length)\n",
    "        unary_features = x[0]\n",
    "        pw_features = x[1]\n",
    "        query = x[2]\n",
    "        \n",
    "        assert(unary_features.ndim == 2)\n",
    "        assert(pw_features.ndim == 3)\n",
    "        assert(len(query) == 2)\n",
    "        \n",
    "        ps = query[0]\n",
    "        L = query[1]\n",
    "        N = unary_features.shape[0]  # total number of POIs\n",
    "        \n",
    "        self._check_size_w(w)\n",
    "        unary_params = w[:self.n_states * self.n_features].reshape((self.n_states, self.n_features))\n",
    "        pw_params = w[self.n_states * self.n_features:].reshape((self.n_states, self.n_states, self.n_edge_features))\n",
    "        \n",
    "        y_hat = do_inference_viterbi(ps, L, N, unary_params, pw_params, unary_features, pw_features, y_true=y)\n",
    "        \n",
    "        #y_hat = do_inference_ILP(ps, L, N, unary_params, pw_params, unary_features, pw_features, y_true=y)\n",
    "        #assert(len(y_hat) == len(set(y_hat)))\n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "    def inference(self, x, w, relaxed=False, return_energy=False):\n",
    "        #print('inference')\n",
    "        # inference procedure for testing: x from test set (features needs to be scaled)\n",
    "        #\n",
    "        # argmax_y np.dot(w, joint_feature(x, y))\n",
    "        #\n",
    "        # x[0]: (unscaled) unary features of all POIs: n_POIs x n_features\n",
    "        # x[1]: (unscaled) pairwise features of all transitions: n_POIs x n_POIs x n_edge_features\n",
    "        # x[2]: query = (startPOI, length)\n",
    "        unary_features = x[0]\n",
    "        pw_features = x[1]\n",
    "        query = x[2]\n",
    "        \n",
    "        assert(unary_features.ndim == 2)\n",
    "        assert(pw_features.ndim == 3)\n",
    "        assert(len(query) == 2)\n",
    "        \n",
    "        ps = query[0]\n",
    "        L = query[1]\n",
    "        N = unary_features.shape[0]  # total number of POIs\n",
    "        \n",
    "        self._check_size_w(w)\n",
    "        unary_params = w[:self.n_states * self.n_features].reshape((self.n_states, self.n_features))\n",
    "        pw_params = w[self.n_states * self.n_features:].reshape((self.n_states, self.n_states, self.n_edge_features))\n",
    "        \n",
    "        #y_pred = do_inference_viterbi(ps, L, N, unary_params, pw_params, unary_features, pw_features)\n",
    "        y_pred = do_inference_ILP(ps, L, N, unary_params, pw_params, unary_features, pw_features)\n",
    "        assert(len(y_pred) == len(set(y_pred)))\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_node_features(startPOI, nPOI, poi_ix, poi_info, poi_clusters, cats, clusters):\n",
    "    \"\"\"\n",
    "    Generate feature vectors for all POIs given query (startPOI, nPOI)\n",
    "    \"\"\"\n",
    "    assert(isinstance(cats, list))\n",
    "    assert(isinstance(clusters, list))\n",
    "    \n",
    "    columns = DF_COLUMNS[3:]\n",
    "    poi_distmat = POI_DISTMAT\n",
    "    query_id_dict = QUERY_ID_DICT\n",
    "    key = (p0, trajLen) = (startPOI, nPOI)\n",
    "    assert(key in query_id_dict)\n",
    "    assert(p0 in poi_info.index)\n",
    "    \n",
    "    # DEBUG: use uniform node features\n",
    "    nrows = len(poi_ix)\n",
    "    ncols = len(columns) + len(cats) + len(clusters) - 2\n",
    "    #return np.ones((nrows, ncols), dtype=np.float)\n",
    "    #return np.zeros((nrows, ncols), dtype=np.float)\n",
    "    \n",
    "    poi_list = poi_ix\n",
    "    df_ = pd.DataFrame(index=np.arange(len(poi_list)), columns=columns)\n",
    "        \n",
    "    for i in range(df_.index.shape[0]):\n",
    "        poi = poi_list[i]\n",
    "        lon, lat = poi_info.loc[poi, 'poiLon'], poi_info.loc[poi, 'poiLat']\n",
    "        pop, nvisit = poi_info.loc[poi, 'popularity'], poi_info.loc[poi, 'nVisit']\n",
    "        cat, cluster = poi_info.loc[poi, 'poiCat'], poi_clusters.loc[poi, 'clusterID']\n",
    "        duration = poi_info.loc[poi, 'avgDuration']\n",
    "        idx = df_.index[i]\n",
    "        df_.loc[idx, 'poiID'] = poi\n",
    "        df_.set_value(idx, 'category', tuple((cat == np.array(cats)).astype(np.int) * 2 - 1))\n",
    "        df_.set_value(idx, 'neighbourhood', tuple((cluster == np.array(clusters)).astype(np.int) * 2 - 1))\n",
    "        df_.loc[idx, 'popularity'] = LOG_SMALL if pop < 1 else np.log10(pop)\n",
    "        df_.loc[idx, 'nVisit'] = LOG_SMALL if nvisit < 1 else np.log10(nvisit)\n",
    "        df_.loc[idx, 'avgDuration'] = LOG_SMALL if duration < 1 else np.log10(duration)\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_all.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi, p0]\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'sameNeighbourhoodStart'] = 1 if cluster == poi_clusters.loc[p0, 'clusterID'] else -1\n",
    "    \n",
    "    # features other than category and neighbourhood\n",
    "    X = df_[list(set(df_.columns) - {'category', 'neighbourhood'})].values  \n",
    "    \n",
    "    # boolean features: category (+1, -1)\n",
    "    cat_features = np.vstack([list(df_.loc[x, 'category']) for x in df_.index])\n",
    "    \n",
    "    # boolean features: neighbourhood (+1, -1)\n",
    "    neigh_features = np.vstack([list(df_.loc[x, 'neighbourhood']) for x in df_.index])\n",
    "    \n",
    "    return np.hstack([X, cat_features, neigh_features]).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_edge_features(trajid_list, poi_ix, traj_dict, poi_info):    \n",
    "    feature_names = ['poiCat', 'popularity', 'nVisit', 'avgDuration', 'clusterID']\n",
    "    n_features = len(feature_names)\n",
    "    \n",
    "    # DEBUG: use uniform edge features\n",
    "    #return np.ones((len(poi_ix), len(poi_ix), n_features), dtype=np.float)\n",
    "    #return np.zeros((len(poi_ix), len(poi_ix), n_features), dtype=np.float)\n",
    "    \n",
    "    transmat_cat                        = gen_transmat_cat(trajid_list, traj_dict, poi_info)\n",
    "    transmat_pop,      logbins_pop      = gen_transmat_pop(trajid_list, traj_dict, poi_info)\n",
    "    transmat_visit,    logbins_visit    = gen_transmat_visit(trajid_list, traj_dict, poi_info)\n",
    "    transmat_duration, logbins_duration = gen_transmat_duration(trajid_list, traj_dict, poi_info)\n",
    "    transmat_neighbor, poi_clusters     = gen_transmat_neighbor(trajid_list, traj_dict, poi_info)\n",
    "    \n",
    "    poi_features = pd.DataFrame(data=np.zeros((len(poi_ix), len(feature_names))), \\\n",
    "                                columns=feature_names, index=poi_ix)\n",
    "    poi_features.index.name = 'poiID'\n",
    "    poi_features['poiCat'] = poi_info.loc[poi_ix, 'poiCat']\n",
    "    poi_features['popularity'] = np.digitize(poi_info.loc[poi_ix, 'popularity'], logbins_pop)\n",
    "    poi_features['nVisit'] = np.digitize(poi_info.loc[poi_ix, 'nVisit'], logbins_visit)\n",
    "    poi_features['avgDuration'] = np.digitize(poi_info.loc[poi_ix, 'avgDuration'], logbins_duration)\n",
    "    poi_features['clusterID'] = poi_clusters.loc[poi_ix, 'clusterID']\n",
    "    \n",
    "    edge_features = np.zeros((len(poi_ix), len(poi_ix), n_features), dtype=np.float64)\n",
    "    \n",
    "    for j in range(len(poi_ix)): # NOTE: POI order\n",
    "        pj = poi_ix[j]\n",
    "        cat, pop = poi_features.loc[pj, 'poiCat'], poi_features.loc[pj, 'popularity']\n",
    "        visit, cluster = poi_features.loc[pj, 'nVisit'], poi_features.loc[pj, 'clusterID']\n",
    "        duration = poi_features.loc[pj, 'avgDuration']\n",
    "        \n",
    "        for k in range(len(poi_ix)): # NOTE: POI order\n",
    "            pk = poi_ix[k]\n",
    "            edge_features[j, k, :] = np.log10( np.array(\n",
    "                    [transmat_cat.loc[cat, poi_features.loc[pk, 'poiCat']], \\\n",
    "                     transmat_pop.loc[pop, poi_features.loc[pk, 'popularity']], \\\n",
    "                     transmat_visit.loc[visit, poi_features.loc[pk, 'nVisit']], \\\n",
    "                     transmat_duration.loc[duration, poi_features.loc[pk, 'avgDuration']], \\\n",
    "                     transmat_neighbor.loc[cluster, poi_features.loc[pk, 'clusterID']]] ) )\n",
    "    return edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_ssvm == True:\n",
    "    recdict_ssvm = dict()\n",
    "    cnt = 1\n",
    "    for i in range(len(trajid_set_all)):\n",
    "        t0 = time.time()\n",
    "        tid_ = trajid_set_all[i]\n",
    "        te = traj_dict[tid_]\n",
    "        \n",
    "        # trajectory too short\n",
    "        if len(te) < 2: continue\n",
    "            \n",
    "        trajid_list_train = trajid_set_all[:i] + trajid_set_all[i+1:]\n",
    "        poi_info = calc_poi_info(trajid_list_train, traj_all, poi_all)\n",
    "        \n",
    "        assert(len(te) <= poi_info.shape[0])\n",
    "            \n",
    "        # build POI_ID <--> POI__INDEX mapping for POIs used to train CRF\n",
    "        # which means only POIs in traj such that len(traj) >= 3 are included\n",
    "        poi_set = set()\n",
    "        for x in trajid_list_train:\n",
    "            if len(traj_dict[x]) >= 2:\n",
    "                poi_set = poi_set | set(traj_dict[x])                \n",
    "        poi_ix = sorted(poi_set)\n",
    "        poi_id_dict, poi_id_rdict = dict(), dict()\n",
    "        for idx, poi in enumerate(poi_ix):\n",
    "            poi_id_dict[poi] = idx\n",
    "            poi_id_rdict[idx] = poi\n",
    "            \n",
    "        # start/end is not in training set\n",
    "        if not (te[0] in poi_set): continue\n",
    "            \n",
    "        print(te, '#%d ->' % cnt); cnt += 1; sys.stdout.flush()\n",
    "        \n",
    "        t1 = time.time()\n",
    "        \n",
    "        # generate training data\n",
    "        train_traj_list = [traj_dict[x] for x in trajid_list_train if len(traj_dict[x]) >= 2]\n",
    "        node_features_list = Parallel(n_jobs=N_JOBS)\\\n",
    "                             (delayed(calc_node_features)\\\n",
    "                              (tr[0], len(tr), poi_ix, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                               cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST) for tr in train_traj_list)\n",
    "        edge_features = calc_edge_features(trajid_list_train, poi_ix, traj_dict, poi_info)\n",
    "        \n",
    "        #print('node_feature:', node_features_list[0].shape)\n",
    "        #print('edge_feature:', edge_features.shape)\n",
    "        \n",
    "        assert(len(train_traj_list) == len(node_features_list))\n",
    "        X_train = [(node_features_list[x], edge_features.copy(), \\\n",
    "                    (poi_id_dict[train_traj_list[x][0]], len(train_traj_list[x]))) for x in range(len(train_traj_list))]\n",
    "        y_train = [np.array([poi_id_dict[x] for x in tr]) for tr in train_traj_list]\n",
    "        assert(len(X_train) == len(y_train))\n",
    "        \n",
    "        t2 = time.time()\n",
    "                \n",
    "        # train\n",
    "        sm = MyModel()\n",
    "        verbose = 0 #5\n",
    "        ssvm = OneSlackSSVM(model=sm, C=SSVM_C, n_jobs=N_JOBS, verbose=verbose)\n",
    "        ssvm.fit(X_train, y_train, initialize=True)\n",
    "        \n",
    "        t3 = time.time()\n",
    "        \n",
    "        # generate test data\n",
    "        node_features = calc_node_features(te[0], len(te), poi_ix, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                           cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)        \n",
    "        # normalise test features\n",
    "        unaries, pw = scale_features_linear(node_features, edge_features, node_max=sm.node_max, node_min=sm.node_min, \\\n",
    "                                            edge_max=sm.edge_max, edge_min=sm.edge_min)\n",
    "        X_test = [(unaries, pw, (poi_id_dict[te[0]], len(te)))]\n",
    "        \n",
    "        # test\n",
    "        y_pred = ssvm.predict(X_test)\n",
    "        rec = [poi_id_rdict[x] for x in y_pred[0]] # map POIs back\n",
    "        rec1 = [te[0]] + rec[1:]\n",
    "        \n",
    "        recdict_ssvm[tid_] = {'REAL':te, 'PRED':rec1} \n",
    "        \n",
    "        t4 = time.time()\n",
    "        \n",
    "        print(' '*10, rec)\n",
    "        print(' '*10, 'train_data: %.1f sec, train: %.1f sec, total: %.1f sec' % \\\n",
    "              (t2 - t1, t3 - t2, t4 - t0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ssvm.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(poi_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssvm.w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_ssvm == True:\n",
    "    F1_ssvm = []; pF1_ssvm = []\n",
    "    for key in sorted(recdict_ssvm.keys()):\n",
    "        F1_ssvm.append(calc_F1(recdict_ssvm[key]['REAL'], recdict_ssvm[key]['PRED']))\n",
    "        pF1_ssvm.append(calc_pairsF1(recdict_ssvm[key]['REAL'], recdict_ssvm[key]['PRED']))\n",
    "    print('SSVM: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_ssvm), np.std(F1_ssvm)/np.sqrt(len(F1_ssvm)), \\\n",
    "           np.mean(pF1_ssvm), np.std(pF1_ssvm)/np.sqrt(len(pF1_ssvm))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if run_ssvm == True: pickle.dump(recdict_ssvm, open(frecdict_ssvm, 'bw'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
