{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trajectory Recommendation - Structured SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of contents:\n",
    "1. [Description of structured prediction](#1.-Description-of-structured-prediction)\n",
    "1. [Inference](#2.-Inference)\n",
    " 1. [Brute force search](#2.1-Brute-force-search)\n",
    " 1. [Greedy search](#2.2-Greedy-search)\n",
    " 1. [The Viterbi algorithm](#2.3-The-Viterbi-algorithm)\n",
    " 1. [The list Viterbi algorithm](#2.4-The-list-Viterbi-algorithm)\n",
    " 1. [Integer linear programming](#2.5-Integer-linear-programming)\n",
    "1. [Structured SVM](#3.-Structured-SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#% matplotlib inline\n",
    "\n",
    "import os, sys, time, pickle, tempfile\n",
    "import math, random, itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq as hq\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "\n",
    "from pystruct.models import StructuredModel\n",
    "from pystruct.learners import OneSlackSSVM\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import cython\n",
    "import pulp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dat_ix``` is required in notebook ```shared.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_ix = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run notebook ```shared.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run 'shared.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_JOBS = 6         # number of parallel jobs\n",
    "USE_GUROBI = False # whether to use GUROBI as ILP solver\n",
    "ABS_SCALER = True  # feature scaling, True: MaxAbsScaler, False: MinMaxScaler #False: StandardScaler\n",
    "C_SET = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000]  # regularisation parameter\n",
    "MC_PORTION = 0.1   # the portion of data that sampled by Monte-Carlo cross-validation\n",
    "MC_NITER = 5       # number of iterations for Monte-Carlo cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle files for saving results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Description of structured prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Structured Predition using PyStruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyse the process of using structured SVM to training a CRF and make preditions on new instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the 1-slack formulation (with margin rescaling) of structured SVM is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\min_{\\mathbf{w}, \\xi \\ge 0} & \\frac{1}{2} \\mathbf{w}^T \\mathbf{w} + C \\xi \\\\\n",
    "s.t. \\forall(\\bar{y}_1, \\dots, \\bar{y}_n) \\in \\mathcal{Y}^n: & \n",
    "\\frac{1}{n} \\mathbf{w}^T \\sum_{i=1}^n \\left( \\Psi(x_i, y_i) - \\Psi(x_i, \\bar{y}_i) \\right) \\ge \n",
    "\\frac{1}{n} \\sum_{i=1}^n \\Delta(y_i, \\bar{y}_i) - \\xi\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where \n",
    "- $\\mathbf{w}$ is the parameter vector\n",
    "- $\\Psi(x_i, y_i)$ is the joint feature (vector) related to example $x_i$ and its label $y_i$\n",
    "- The size of $\\mathbf{w}$ is the same as $\\Psi(x_i, y_i)$\n",
    "- $\\Delta(\\centerdot)$ is the loss function, here we use Hamming loss, i.e., per-variable 0-1 loss, as indicated by function [loss()](https://github.com/pystruct/pystruct/blob/master/pystruct/models/base.py) and [fit()](https://github.com/pystruct/pystruct/blob/master/pystruct/learners/one_slack_ssvm.py).\n",
    "- $n$ is the total number of training examples, $C$ is the regularisation parameter, $\\xi$ is the slack variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before introducing the training and prediction procedure, we define some concepts that will be used later.\n",
    "- `n_states`: #states for all variables, (this is the total number of unique POIs in training set here).\n",
    "- `n_features`: #features per node, (this is the number of POI features, i.e., the ranking probabilities of all POIs).\n",
    "- `n_edges`: #edges in each training/test example, (this is the number of POIs in a trajectory).\n",
    "- `n_edge_features`: #features per edge, (this is the number of features for each transition, \n",
    "   i.e., the out-going transition probabilities to all POIs).\n",
    "- $x$ is made up of three parts: (`node_features`, `edges`, `edge_features`).\n",
    "- `node_features`: `n_nodes` $\\times$ `n_features`\n",
    "- `edge_features`: `n_edges` $\\times$ `n_edge_features`\n",
    "- `edges`: `n_edges` $\\times$ $2$, e.g. for trajectory `[3, 1, 2]` and `[5, 9, 6]`, their `edges` are the same matrix\n",
    "   `[[0, 1], [1, 2]]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For [EdgeFeatureGraphCRF](https://pystruct.github.io/generated/pystruct.models.EdgeFeatureGraphCRF.html), the pairwise potentials are asymmetric and shared over all edges, and the size of \n",
    "- **Parameter vector $\\mathbf{w}$: `n_states` $\\times$ `n_features` $+$ `n_edge_features` $\\times$ `(n_states)`$^2$**\n",
    "- The first part of $\\mathbf{w}$, let's call it **`unary_params`**: `n_states` $\\times$ `n_features`, is the parameters \n",
    "  used to compute unary potentials.\n",
    "- The second part of $\\mathbf{w}$, let's call it **`pairwise_params`**: `n_edge_features` $\\times$ `(n_states)`$^2$, \n",
    "  is the parameters used to compute pairwise potentials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the joint feature vector $\\Psi(x, y)$\n",
    "\n",
    "When training a CRF using [OneSlackSSVM](https://pystruct.github.io/generated/pystruct.learners.OneSlackSSVM.html), we need to compute the joint feature vector $\\Psi(x, y)$ for each training example, it is computed \n",
    "(for EdgeFeatureGraphCRF in PyStruct) as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unary part of $\\Psi(x, y)$**:\n",
    "- make one-hot encoding of $y$, its size: `n_nodes` $\\times$ `n_states`\n",
    "- *value*: $y^T \\times$ `node_features`\n",
    "- *dimension*: `(n_nodes` $\\times$ `n_states)`$^T$ $\\times$ `(n_nodes` $\\times$ `n_features)` \n",
    "  $\\to$ `(n_states` $\\times$ `n_features)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pairwise Part of $\\Psi(x, y)$**:\n",
    "- make one-hot encoding of `edges`, its size: `n_edges` $\\times$ `(n_states)`$^2$\n",
    "- *value*: `edge_features`$^T$ $\\times$ `edges`\n",
    "- *dimension*: `(n_edges` $\\times$ `n_edge_features)`$^T$ $\\times$ `(n_edges` $\\times$ `(n_states)`$^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for each training example, $\\Psi(x_i, y_i)$ = `[unary part, pairwise part]`, solve the above QP problem (1-slack formulation) to get a parameter vector $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a trajectory is chain structured, so we use `max-product` belief propagation (Viterbi algorithm in this case) to do inference in the trained CRF.  \n",
    "To predict the label of a new instance $x$, we need to compute the unary potential and pairwise potential of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unary potential:** \n",
    "- *value*: `node_features` $\\times$ `(unary_params)`$^T$ (first part of $\\mathbf{w}$)\n",
    "- *dimension*: `(n_nodes` $\\times$ `n_features)` $\\times$ `(n_states` $\\times$ `n_features)`$^T$ $\\to$ \n",
    "  `(n_nodes` $\\times$ `n_states)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pairwise potential:**\n",
    "- *value*: `edge_features` $\\times$ `pairwise_params` (second part of $\\mathbf{w}$)\n",
    "- *dimension*: `(n_edges` $\\times$ `n_edge_features)` $\\times$ `(n_edge_features` $\\times$ `n_states`$^2$ `)`, \n",
    "  reshape to `(n_edges` $\\times$ `n_states` $\\times$ `n_states)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With unary potential and pairwise potential computed, as we could know from `edges` that our example $x$ is chain structured, so we do inference using Viterbi algorithm to compute the most likely label of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Node Features - POI/Query Specific Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a trajectory `[start, ..., end]`, the features used to train/test are those that used to rank POIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyStruct](https://pystruct.github.io/) assumes that [label `y` is a discrete vector](https://pystruct.github.io/intro.html) and [pystruct.learners assume labels `y` are integers starting with `0`](https://github.com/pystruct/pystruct/issues/114), concretely,\n",
    "- values in label vector $y$ should satisfy $y_i \\in Y$, \n",
    "  where $Y$ is the **index** of a discrete value space, and the index starts at 0.\n",
    "- label vector $y$ will be [transformed to one hot encoding (see function `joint_feature()`)](https://github.com/pystruct/pystruct/blob/master/pystruct/models/graph_crf.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if labels in training set is `[[1, 2], [0, 4, 9]]`, \n",
    "then it will cause an index out of bounds error as pystruct did something like this,\n",
    "1. construct an discrete value space: \n",
    "   - `set([1, 2] + [0, 4, 9]) -> {0, 1, 2, 4, 9}`\n",
    "   - `size({0, 1, 2, 4, 9}) = 5`\n",
    "1. convert labels using one hot encoding: \n",
    "   - label vector `[1, 2]` will be converted to a matrix of shape $2 \\times 5$,\n",
    "     with cells at `(0, 1), (1, 2)` set to `1` and others set to `0`.\n",
    "   - label vector `[0, 4, 9]` will be converted to a matrix of shape $3 \\times 5$,\n",
    "     with cells at `(0, 0)`, `(1, 4)`, **`(2, 9)` INDEX_OUT_OF_BOUNDS** set to `1` and others set to `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus need to build a mapping for POIs: *POI_ID $\\to$ POI_INDEX* with POIs in trajectories in training set, also a map of the reverse direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the joint features (when training) linearly to `[-1, 1]`, i.e., for feature $x$, we fit a linear function\n",
    "\\begin{equation}\n",
    "    f(x) = ax + b \n",
    "\\end{equation}\n",
    "such that \n",
    "\\begin{equation}\n",
    "    a x_\\texttt{max} + b = +1\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    a x_\\texttt{min} + b = -1\n",
    "\\end{equation}\n",
    "\n",
    "Solve the above linear equations result in a function\n",
    "\\begin{equation}\n",
    "    f(x) = -1 + \\frac{2(x-x_\\texttt{min})}\n",
    "                     {x_\\texttt{max} - x_\\texttt{min}}\n",
    "\\end{equation}\n",
    "This approach is used by [libsvm and ranksvm](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ranksvm/liblinear-ranksvm-2.1.zip), one can find the code at lines `349-383` in `svm-scale.c` (function `output` and `output_target`).\n",
    "\n",
    "In addition, for features with uniform values, we set them to `0`, i.e., \n",
    "\\begin{equation}\n",
    "    \\textbf{if}~ x_\\texttt{max} == x_\\texttt{min} ~\\textbf{then}~ f(x) = 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "#%%cython -a\n",
    "cimport numpy as np # for np.ndarray\n",
    "import numpy as np # for np.shape\n",
    "\n",
    "cpdef tuple scale_features_linear(\n",
    "    np.ndarray[dtype=np.float64_t, ndim=2] node_features, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=3] edge_features, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=2] node_max, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=2] node_min, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=3] edge_max, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=3] edge_min):\n",
    "    \n",
    "    # DEBUG\n",
    "    #return (node_features, edge_features)\n",
    "    \n",
    "    assert(np.shape(node_features) == np.shape(node_max) == np.shape(node_min))\n",
    "    assert(np.shape(edge_features) == np.shape(edge_max) == np.shape(edge_min))\n",
    "    \n",
    "    # x_max == x_min means feature x has uniform (such as constant) values, i.e. x == x_max == x_min\n",
    "    #node_delta = node_max - node_min\n",
    "    #edge_delta = edge_max - edge_min\n",
    "    #node_delta[np.abs(node_delta) < 1e-9] = 1.\n",
    "    #edge_delta[np.abs(edge_delta) < 1e-9] = 1.\n",
    "    #return (2 * np.divide(x0-node_min, node_delta) - 1, 2 * np.divide(x1-edge_min, edge_delta) - 1)\n",
    "\n",
    "    #TODO: loop-over each element using cython\n",
    "    # max <=1 and min >= -1 and x in [-1, 1], no need to scale\n",
    "    # max == min, set x = 0\n",
    "    # boolean features, no scaling\n",
    "    cdef int I, J, K, M, N\n",
    "    M, N = np.shape(node_features)\n",
    "    for m in range(M):\n",
    "        for n in range(N):\n",
    "            # skip features distributed in [-1, 1] and single-valued features \n",
    "            if (node_max[m, n] > 1. or node_min[m, n] < -1.) and node_max[m, n] - node_min[m, n] > 1e-6:\n",
    "                    node_features[m, n] = 2. * (node_features[m,n]-node_min[m,n]) / (node_max[m,n]-node_min[m,n]) - 1\n",
    "                         \n",
    "            #if node_max[m, n] < 1.1 and node_min[m, n] > -1.1 and -1.1 < node_features[m, n] < 1.1: continue\n",
    "            #elif np.fabs(node_max[m, n] - node_min[m, n]) < 1e-9: node_features[m, n] = 0. #continue\n",
    "            #else: node_features[m, n] = 2. * (node_features[m,n]-node_min[m,n]) / (node_max[m,n]-node_min[m,n]) - 1\n",
    "            \n",
    "    I, J, K = np.shape(edge_features)\n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "            for k in range(K):\n",
    "                #if edge_max[i,j,k] < 1.1 and edge_min[i,j,k] > -1.1 and -1.1 < edge_features[i,j,k] < 1.1: continue\n",
    "                #elif np.fabs(edge_max[i,j,k] - edge_min[i,j,k]) < 1e-9: edge_features[i,j,k] = 0. #continue\n",
    "                #else:edge_features[i,j,k]=2.*(edge_features[i,j,k]-edge_min[i,j,k])/(edge_max[i,j,k]-edge_min[i,j,k])-1\n",
    "                if (edge_max[i,j,k] > 1. or edge_min[i,j,k] < -1.) and edge_max[i,j,k] - edge_min[i,j,k] > 1e-6:\n",
    "                        edge_features[i,j,k] = \\\n",
    "                        2. * (edge_features[i,j,k] - edge_min[i,j,k]) / (edge_max[i,j,k] - edge_min[i,j,k]) - 1\n",
    "    return (node_features, edge_features)\n",
    "\n",
    "\n",
    "cpdef scale_vector(np.ndarray[dtype=np.float64_t, ndim=1] features_unscaled, \n",
    "                              np.ndarray[dtype=np.float64_t, ndim=1] features_max, \n",
    "                              np.ndarray[dtype=np.float64_t, ndim=1] features_min):\n",
    "    # DEBUG\n",
    "    return features_unscaled\n",
    "\n",
    "    assert(np.shape(features_unscaled) == np.shape(features_max) == np.shape(features_min))\n",
    "    cdef int N, n\n",
    "    N = np.shape(features_unscaled)[0]\n",
    "    feature_scaled = np.zeros(N, dtype=np.float64)\n",
    "    \n",
    "    for n in range(N):\n",
    "        if (features_max[n] > 1. or features_min[n] < -1.) and features_max[n] - features_min[n] > 1e-6:\n",
    "            feature_scaled[n] = -1 + 2. * (features_unscaled[n]-features_min[n]) / (features_max[n]-features_min[n])\n",
    "    return feature_scaled\n",
    "\n",
    "\n",
    "cpdef tuple scale_features_norm(\n",
    "    np.ndarray[dtype=np.float64_t, ndim=2] node_features, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=3] edge_features, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=2] node_mean, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=2] node_std, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=3] edge_mean, \n",
    "    np.ndarray[dtype=np.float64_t, ndim=3] edge_std):\n",
    "    \n",
    "    assert(np.shape(node_features) == np.shape(node_mean) == np.shape(node_std))\n",
    "    assert(np.shape(edge_features) == np.shape(edge_mean) == np.shape(edge_std))\n",
    "    \n",
    "    #return (np.divide(x0-node_means, node_stds), np.divide(x1-edge_means, edge_stds))\n",
    "    \n",
    "    cdef int I, J, K, M, N\n",
    "    cdef int i, j, k, m, n\n",
    "    M, N = np.shape(node_features)\n",
    "    for m in range(M):\n",
    "        for n in range(N):\n",
    "            # skip single-valued features\n",
    "            if np.fabs(node_std[m, n]) > 1e-6:\n",
    "                node_features[m, n] = (node_features[m, n] - node_mean[m, n]) / node_std[m, n]\n",
    "            \n",
    "    I, J, K = np.shape(edge_features)\n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "            for k in range(K):\n",
    "                if np.fabs(edge_std[i, j, k]) > 1e-6:\n",
    "                    edge_features[i, j, k] = (edge_features[i, j, k] - edge_mean[i, j, k]) / edge_std[i, j, k]\n",
    "    \n",
    "    return (node_features, edge_features)\n",
    "\n",
    "\n",
    "cpdef tuple build_joint_feature(np.ndarray[dtype=np.float64_t, ndim=2] node_features,\n",
    "                                np.ndarray[dtype=np.float64_t, ndim=3] edge_features,\n",
    "                                np.ndarray[dtype=np.float64_t, ndim=2] node_max, \n",
    "                                np.ndarray[dtype=np.float64_t, ndim=2] node_min, \n",
    "                                np.ndarray[dtype=np.float64_t, ndim=3] edge_max, \n",
    "                                np.ndarray[dtype=np.float64_t, ndim=3] edge_min,\n",
    "                                np.ndarray[dtype=np.long_t, ndim=1] y):\n",
    "    cdef int j\n",
    "    L = np.shape(y)[0]\n",
    "\n",
    "    unary_features = np.zeros(np.shape(node_features), dtype=np.float)\n",
    "    pw_features = np.zeros(np.shape(edge_features), dtype=np.float)\n",
    "    \n",
    "    unary_features[y[0], :] = node_features[y[0], :]\n",
    "    \n",
    "    for j in range(L-1):\n",
    "        ss, tt = y[j], y[j+1]\n",
    "        unary_features[tt, :] = node_features[tt, :]\n",
    "        pw_features[ss, tt, :] = edge_features[ss, tt, :]\n",
    "\n",
    "    # feature scaling here\n",
    "    ret = scale_features_linear(unary_features, pw_features,\n",
    "                                node_max=node_max, node_min=node_min,\n",
    "                                edge_max=edge_max, edge_min=edge_min)\n",
    "    return (ret[0], ret[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nf = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float)\n",
    "ef = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]).astype(np.float)\n",
    "nmax = np.array([[2, 2, 3], [5, 5, 6]]).astype(np.float)\n",
    "nmin = np.array([[1, 1, 2], [3, 3, 3]]).astype(np.float)\n",
    "emax = np.array([[[2, 2], [4, 4]], [[6, 6], [9, 8]]]).astype(np.float)\n",
    "emin = np.array([[[1, 1], [2, 2]], [[3, 3], [3, 3]]]).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scale_features_linear(nf, ef, nmax, nmin, emax, emin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference for SSVM: loss-augmented inference for cutting-plane training and inference for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples for sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M0, L0 = 5, 3\n",
    "w_u = np.array([1, 2, 3, 2, 3]).reshape((M0, 1))\n",
    "f_u = np.array([2, 1, 1, 3, 1]).reshape((M0, 1))\n",
    "w_p = np.array([1,1,1,1,3, 1,1,1,2,1, 1,3,1,1,1, 2,1,1,1,1, 1,1,3,1,1]).reshape((M0, M0, 1))\n",
    "f_p = np.array([1,2,1,1,1, 1,1,1,1,3, 2,1,1,1,1, 1,1,3,1,1, 1,1,1,2,1]).reshape((M0, M0, 1))\n",
    "ps0, y_true0 = 1, [1, 0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M0, L0 = 6, 4\n",
    "w_u = np.array([1, 1, 1, 2, 1, 2]).reshape((M0, 1))\n",
    "f_u = np.array([2, 1, 1, 2, 1, 1]).reshape((M0, 1))\n",
    "w_p = np.array([1,1,1,1,3,2, 1,1,1,2,1,1, 1,3,1,1,1,2, 2,1,1,1,1,1, 1,1,3,1,1,1, 1,2,1,1,2,1]).reshape((M0, M0, 1))\n",
    "f_p = np.array([1,2,1,1,1,1, 1,1,1,1,3,2, 2,1,1,1,1,2, 1,1,3,1,1,1, 1,1,1,2,1,1, 2,1,1,2,1,1]).reshape((M0, M0, 1))\n",
    "ps0, y_true0 = 1, [1, 2, 0, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Brute force search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference using **brute force search** (for sanity check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_inference_bruteForce(ps, L, M, unary_params, pw_params, unary_features, pw_features, y_true=None):\n",
    "    assert(L > 1)\n",
    "    assert(L <= M)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < M)\n",
    "    \n",
    "    Cu = np.zeros(M, dtype=np.float)      # unary_param[p] x unary_features[p]\n",
    "    Cp = np.zeros((M, M), dtype=np.float) # pw_param[pi, pj] x pw_features[pi, pj]\n",
    "    # a intermediate POI should NOT be the start POI, NO self-loops\n",
    "    for pi in range(M):\n",
    "        Cu[pi] = np.dot(unary_params[pi, :], unary_features[pi, :]) # if pi != ps else -np.inf\n",
    "        for pj in range(M):\n",
    "            Cp[pi, pj] = -np.inf if (pj == ps or pi == pj) else np.dot(pw_params[pi, pj, :], pw_features[pi, pj, :])\n",
    "            \n",
    "    Q = []\n",
    "    for x in itertools.permutations([p for p in range(M) if p != ps], L-1):\n",
    "        #print([ps] + list(x))\n",
    "        y = [ps] + list(x)\n",
    "        score = 0\n",
    "        for j in range(1, L):\n",
    "            score += Cp[y[j-1], y[j]] + Cu[y[j]]\n",
    "        if y_true is not None:\n",
    "            score += np.sum(np.asarray(y) != np.asarray(y_true))\n",
    "        priority = -score\n",
    "        hq.heappush(Q, (priority, y))\n",
    "    \n",
    "    k = 20\n",
    "    while k > 0 and len(Q) > 0:\n",
    "        priority, pathwalk = hq.heappop(Q)\n",
    "        print(pathwalk, -priority)\n",
    "        k -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#do_inference_bruteForce(ps0, L0, M0, w_u, w_p, f_u, f_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Greedy search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference using **greedy search** (baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_inference_greedy(ps, L, M, unary_params, pw_params, unary_features, pw_features, y_true=None):\n",
    "    assert(L > 1)\n",
    "    assert(L <= M)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < M)\n",
    "    \n",
    "    Cu = np.zeros(M, dtype=np.float)      # unary_param[p] x unary_features[p]\n",
    "    Cp = np.zeros((M, M), dtype=np.float) # pw_param[pi, pj] x pw_features[pi, pj]\n",
    "    # a intermediate POI should NOT be the start POI, NO self-loops\n",
    "    for pi in range(M):\n",
    "        Cu[pi] = np.dot(unary_params[pi, :], unary_features[pi, :]) # if pi != ps else -np.inf\n",
    "        for pj in range(M):\n",
    "            Cp[pi, pj] = -np.inf if (pj == ps or pi == pj) else np.dot(pw_params[pi, pj, :], pw_features[pi, pj, :])\n",
    "    \n",
    "    y_hat = [ps]\n",
    "    \n",
    "    for t in range(1, L):\n",
    "        candidate_points = [p for p in range(M) if p not in y_hat]\n",
    "        p = y_hat[-1]\n",
    "        maxix = np.argmax([Cp[p, p1] + Cu[p1] + float(p1 != y_true[t]) if y_true is not None else \\\n",
    "                           Cp[p, p1] + Cu[p1] for p1 in candidate_points])\n",
    "        y_hat.append(candidate_points[maxix])\n",
    "        \n",
    "    return np.asarray(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(do_inference_greedy(ps0, L0, M0, w_u, w_p, f_u, f_p))\n",
    "#print(do_inference_greedy(ps0, L0, M0, w_u, w_p, f_u, f_p, y_true=y_true0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 The Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference using **the Viterbi algorithm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_inference_viterbi(ps, L, M, unary_params, pw_params, unary_features, pw_features, y_true=None):\n",
    "    assert(L > 1)\n",
    "    assert(L <= M)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < M)\n",
    "    \n",
    "    Cu = np.zeros(M, dtype=np.float)      # unary_param[p] x unary_features[p]\n",
    "    Cp = np.zeros((M, M), dtype=np.float) # pw_param[pi, pj] x pw_features[pi, pj]\n",
    "    # a intermediate POI should NOT be the start POI, NO self-loops\n",
    "    for pi in range(M):\n",
    "        Cu[pi] = np.dot(unary_params[pi, :], unary_features[pi, :]) # if pi != ps else -np.inf\n",
    "        for pj in range(M):\n",
    "            Cp[pi, pj] = -np.inf if (pj == ps or pi == pj) else np.dot(pw_params[pi, pj, :], pw_features[pi, pj, :])\n",
    "    \n",
    "    A = np.zeros((L-1, M), dtype=np.float)     # scores matrix\n",
    "    B = np.ones((L-1, M), dtype=np.int) * (-1) # backtracking pointers\n",
    "    \n",
    "    for p in range(M): # ps--p\n",
    "        A[0, p] = Cp[ps, p] + Cu[p]\n",
    "        #if y_true is not None and p != ps: A[0, p] += float(p != y_true[1])/L  # loss term: normalised\n",
    "        if y_true is not None and p != ps: A[0, p] += float(p != y_true[1])\n",
    "        B[0, p] = ps\n",
    "\n",
    "    for t in range(0, L-2):\n",
    "        for p in range(M):\n",
    "            #loss = float(p != y_true[l+2])/L if y_true is not None else 0  # loss term: normlised\n",
    "            loss = float(p != y_true[t+2]) if y_true is not None else 0\n",
    "            scores = [A[t, p1] + Cp[p1, p] + Cu[p] for p1 in range(M)] # ps~~p1--p\n",
    "            maxix = np.argmax(scores)\n",
    "            A[t+1, p] = scores[maxix] + loss\n",
    "            #B[l+1, p] = np.array(range(N))[maxix]\n",
    "            B[t+1, p] = maxix\n",
    "\n",
    "    y_hat = [np.argmax(A[L-2, :])]\n",
    "    p, t = y_hat[-1], L-2\n",
    "    while t >= 0:\n",
    "        y_hat.append(B[t, p])\n",
    "        p, t = y_hat[-1], t-1\n",
    "    y_hat.reverse()\n",
    "\n",
    "    return np.asarray(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(do_inference_viterbi(ps0, L0, M0, w_u, w_p, f_u, f_p))\n",
    "#print(do_inference_viterbi(ps0, L0, M0, w_u, w_p, f_u, f_p, y_true=y_true0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 The list Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference using **the List Viterbi algorithm**, which *sequentially* find the (k+1)-th best path/walk given the 1st, 2nd, ..., k-th best paths/walks.\n",
    "\n",
    "Reference papers:\n",
    "- [*Sequentially finding the N-Best List in Hidden Markov Models*](http://www.eng.biu.ac.il/~goldbej/papers/ijcai01.pdf), Dennis Nilsson and Jacob Goldberger, IJCAI 2001.\n",
    "- [*A tutorial on hidden Markov models and selected applications in speech recognition*](http://www.cs.ubc.ca/~murphyk/Bayes/rabiner.pdf), L.R. Rabiner, Proceedings of the IEEE, 1989.\n",
    "\n",
    "Implementation is adapted from the above references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HeapItem:  # an item in heapq (min-heap)\n",
    "    def __init__(self, priority, task):\n",
    "        self.priority = priority\n",
    "        self.task = task\n",
    "        self.string = str(priority) + ': ' + str(task)\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        return self.priority < other.priority\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.string\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_inference_listViterbi(ps, L, M, unary_params, pw_params, unary_features, pw_features, y_true=None, debug=False):\n",
    "    assert(L > 1)\n",
    "    assert(M >= L)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < M)\n",
    "    \n",
    "    Cu = np.zeros(M, dtype=np.float)      # unary_param[p] x unary_features[p]\n",
    "    Cp = np.zeros((M, M), dtype=np.float) # pw_param[pi, pj] x pw_features[pi, pj]\n",
    "    \n",
    "    # a intermediate POI should NOT be the start POI, NO self-loops\n",
    "    for pi in range(M):\n",
    "        Cu[pi] = np.dot(unary_params[pi, :], unary_features[pi, :]) # if pi != ps else -np.inf\n",
    "        for pj in range(M):\n",
    "            Cp[pi, pj] = -np.inf if (pj == ps or pi == pj) else np.dot(pw_params[pi, pj, :], pw_features[pi, pj, :])\n",
    "            \n",
    "    # forward-backward procedure: adapted from the Rabiner paper\n",
    "    Alpha = np.zeros((L, M), dtype=np.float)  # alpha_t(p_i)\n",
    "    Beta  = np.zeros((L, M), dtype=np.float)  # beta_t(p_i)\n",
    "    \n",
    "    for pj in range(M): Alpha[1, pj] = Cp[ps, pj] + Cu[pj]\n",
    "    for t in range(2, L):\n",
    "        for pj in range(M):\n",
    "            Alpha[t, pj] = np.max([Alpha[t-1, pi] + Cp[pi, pj] + Cu[pj] for pi in range(M)])\n",
    "            \n",
    "    for t in range(L-1, 1, -1):\n",
    "        for pi in range(M):\n",
    "            Beta[t-1, pi] = np.max([Cp[pi, pj] + Cu[pj] + Beta[t, pj] for pj in range(M)])        \n",
    "    Beta[0, ps] = np.max([Cp[ps, pj] + Cu[pj] + Beta[1, pj] for pj in range(M)])\n",
    "    \n",
    "    Fp = np.zeros((L-1, M, M), dtype=np.float)  # f_{t, t+1}(p, p')\n",
    "    \n",
    "    for t in range(L-1):\n",
    "        for pi in range(M):\n",
    "            for pj in range(M):\n",
    "                Fp[t, pi, pj] = Alpha[t, pi] + Cp[pi, pj] + Cu[pj] + Beta[t+1, pj]\n",
    "                \n",
    "    # identify the best path/walk: adapted from the IJCAI01 paper\n",
    "    y_best = np.ones(L, dtype=np.int) * (-1)\n",
    "    y_best[0] = ps\n",
    "    maxix = np.argmax(Fp[0, ps, :])  # the start POI is specified\n",
    "    y_best[1] = maxix\n",
    "    for t in range(2, L): \n",
    "        y_best[t] = np.argmax(Fp[t-1, y_best[t-1], :])\n",
    "        \n",
    "    Q = []  # priority queue (min-heap)\n",
    "    maxIter = np.power(M,L-1) - np.prod([M-kx for kx in range(1,L)]) + 1 #gauranteed to find a path in maxIter iterations\n",
    "    if debug == True: maxIter = np.min([maxIter, 200]); print('#iterations:', maxIter) \n",
    "        \n",
    "    # heap item for the best path/walk\n",
    "    priority, partition_index, exclude_set = -np.max(Alpha[L-1, :]), None, set()  # -1 * score as priority\n",
    "    hq.heappush(Q, HeapItem(priority, (y_best, partition_index, exclude_set)))\n",
    "    \n",
    "    histories = set()\n",
    "        \n",
    "    k = 0\n",
    "    while len(Q) > 0 and k < maxIter:\n",
    "        #print('------------------\\n', Q, '\\n------------------')\n",
    "        hitem = hq.heappop(Q)\n",
    "        k_priority, (k_best, k_partition_index, k_exclude_set) = hitem.priority, hitem.task\n",
    "        k += 1\n",
    "        \n",
    "        histories.add(''.join([str(x) + ',' for x in k_best]))\n",
    "        #print(k, len(histories))\n",
    "        \n",
    "        #print('pop:', k_priority, k_best, k_partition_index, k_exclude_set)\n",
    "        if debug == True: \n",
    "            print(k_best, -k_priority)\n",
    "        else:\n",
    "            if len(set(k_best)) == L: return k_best\n",
    "            \n",
    "        \n",
    "        # identify the (k+1)-th best path/walk given the 1st, 2nd, ..., k-th best: adapted from the IJCAI01 paper\n",
    "        partition_index_start = 1\n",
    "        if k_partition_index is not None:\n",
    "            assert(k_partition_index > 0)\n",
    "            assert(k_partition_index < L)\n",
    "            partition_index_start = k_partition_index\n",
    "            \n",
    "        for parix in range(partition_index_start, L):    \n",
    "            new_exclude_set = set({k_best[parix]})\n",
    "            if parix == partition_index_start:\n",
    "                new_exclude_set = new_exclude_set | k_exclude_set\n",
    "            \n",
    "            new_best = np.ones(L, dtype=np.int) * (-1)\n",
    "            for pk in range(parix):\n",
    "                new_best[pk] = k_best[pk]\n",
    "            \n",
    "            candidate_points = [p for p in range(M) if p not in new_exclude_set]\n",
    "            if len(candidate_points) == 0: continue\n",
    "            candidate_maxix = np.argmax([Fp[parix-1, k_best[parix-1], p] for p in candidate_points])\n",
    "            new_best[parix] = candidate_points[candidate_maxix]\n",
    "            \n",
    "            for pk in range(parix+1, L):\n",
    "                new_best[pk] = np.argmax([Fp[pk-1, new_best[pk-1], p] for p in range(M)])\n",
    "            \n",
    "            new_priority = Fp[parix-1, k_best[parix-1], new_best[parix]]\n",
    "            if k_partition_index is not None:\n",
    "                new_priority += (-k_priority) - Fp[parix-1, k_best[parix-1], k_best[parix]]\n",
    "            new_priority *= -1.0  # NOTE: -np.inf - np.inf + np.inf = nan\n",
    "            \n",
    "            #if debug == True and np.isnan(new_priority):\n",
    "            #    print(Fp[parix-1,k_best[parix-1],new_best[parix]], (-k_priority), \\\n",
    "            #          Fp[parix-1,k_best[parix-1],k_best[parix]])\n",
    "            #    print(Fp[parix-1,k_best[parix-1],new_best[parix]] - k_priority - \\\n",
    "            #          Fp[parix-1,k_best[parix-1],k_best[parix]])\n",
    "                \n",
    "            #print(' '*3, 'push:', new_priority, new_best, parix, new_exclude_set)\n",
    "            \n",
    "            hq.heappush(Q, HeapItem(new_priority, (new_best, parix, new_exclude_set)))\n",
    "            #print('------------------\\n', Q, '\\n------------------')\n",
    "    if debug == True: print('#iterations: %d, #distinct_trajectories: %d' % (k, len(histories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#do_inference_listViterbi(ps0, L0, M0, w_u, w_p, f_u, f_p, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(do_inference_listViterbi(ps0, L0, M0, w_u, w_p, f_u, f_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Integer linear programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference using **integer linear programming (ILP)** to avoid sub-tours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_inference_ILP(ps, L, M, unary_params, pw_params, unary_features, pw_features, y_true=None):\n",
    "    assert(L > 1)\n",
    "    assert(L <= M)\n",
    "    assert(ps >= 0)\n",
    "    assert(ps < M)\n",
    "    p0 = str(ps)\n",
    "    \n",
    "    #print('===:', p0)\n",
    "    \n",
    "    pois = [str(p) for p in range(M)] # create a string list for each POI\n",
    "    pb = pulp.LpProblem('Inference_ILP', pulp.LpMaximize) # create problem\n",
    "    # visit_i_j = 1 means POI i and j are visited in sequence\n",
    "    visit_vars = pulp.LpVariable.dicts('visit', (pois, pois), 0, 1, pulp.LpInteger) \n",
    "    # isend_l = 1 means POI l is the END POI of trajectory\n",
    "    isend_vars = pulp.LpVariable.dicts('isend', pois, 0, 1, pulp.LpInteger) \n",
    "    # a dictionary contains all dummy variables\n",
    "    dummy_vars = pulp.LpVariable.dicts('u', [x for x in pois if x != p0], 2, M, pulp.LpInteger)\n",
    "    \n",
    "    # add objective\n",
    "    objlist = []\n",
    "    for pi in pois:     # from\n",
    "        for pj in pois: # to\n",
    "            objlist.append(visit_vars[pi][pj] * (np.dot(unary_params[int(pj)], unary_features[int(pj)]) + \\\n",
    "                                                 np.dot(pw_params[int(pi), int(pj)], pw_features[int(pi), int(pj)])))\n",
    "    if y_true is not None: # Loss: normalised number of mispredicted POIs, Hamming loss is non-linear of 'visit'\n",
    "        objlist.append(1)\n",
    "        for j in range(M):\n",
    "            pj = pois[j]\n",
    "            for k in range(1, L): \n",
    "                pk = str(y_true[k])\n",
    "                #objlist.append(-1.0 * visit_vars[pj][pk] / L) # loss term: normalised\n",
    "                objlist.append(-1.0 * visit_vars[pj][pk])\n",
    "    pb += pulp.lpSum(objlist), 'Objective'\n",
    "    \n",
    "    # add constraints, each constraint should be in ONE line\n",
    "    pb += pulp.lpSum([visit_vars[pi][pi] for pi in pois]) == 0, 'NoSelfLoops'\n",
    "    pb += pulp.lpSum([visit_vars[p0][pj] for pj in pois]) == 1, 'StartAt_p0'\n",
    "    pb += pulp.lpSum([visit_vars[pi][p0] for pi in pois]) == 0, 'NoIncoming_p0'\n",
    "    pb += pulp.lpSum([visit_vars[pi][pj] for pi in pois for pj in pois]) == L-1, 'Length'\n",
    "    pb += pulp.lpSum([isend_vars[pi] for pi in pois]) == 1, 'OneEnd'\n",
    "    pb += isend_vars[p0] == 0, 'StartNotEnd'\n",
    "    \n",
    "    for pk in [x for x in pois if x != p0]:\n",
    "        pb += pulp.lpSum([visit_vars[pi][pk] for pi in pois]) == isend_vars[pk] + \\\n",
    "              pulp.lpSum([visit_vars[pk][pj] for pj in pois if pj != p0]), 'ConnectedAt_' + pk\n",
    "        pb += pulp.lpSum([visit_vars[pi][pk] for pi in pois]) <= 1, 'Enter_' + pk + '_AtMostOnce'\n",
    "        pb += pulp.lpSum([visit_vars[pk][pj] for pj in pois if pj != p0]) + isend_vars[pk] <= 1, \\\n",
    "              'Leave_' + pk + '_AtMostOnce'\n",
    "    for pi in [x for x in pois if x != p0]:\n",
    "        for pj in [y for y in pois if y != p0]:\n",
    "            pb += dummy_vars[pi] - dummy_vars[pj] + 1 <= (M - 1) * (1 - visit_vars[pi][pj]), \\\n",
    "                    'SubTourElimination_' + pi + '_' + pj\n",
    "    #pb.writeLP(\"traj_tmp.lp\")\n",
    "    \n",
    "    # solve problem: solver should be available in PATH\n",
    "    if USE_GUROBI == True:\n",
    "        gurobi_options = [('TimeLimit', '7200'), ('Threads', str(N_JOBS)), ('NodefileStart', '0.2'), ('Cuts', '2')]\n",
    "        pb.solve(pulp.GUROBI_CMD(path='gurobi_cl', options=gurobi_options)) # GUROBI\n",
    "    else:\n",
    "        pb.solve(pulp.COIN_CMD(path='cbc', options=['-threads', str(N_JOBS), '-strategy', '1', '-maxIt', '2000000']))#CBC\n",
    "    visit_mat = pd.DataFrame(data=np.zeros((len(pois), len(pois)), dtype=np.float), index=pois, columns=pois)\n",
    "    isend_vec = pd.Series(data=np.zeros(len(pois), dtype=np.float), index=pois)\n",
    "    for pi in pois:\n",
    "        isend_vec.loc[pi] = isend_vars[pi].varValue\n",
    "        for pj in pois: visit_mat.loc[pi, pj] = visit_vars[pi][pj].varValue\n",
    "    #visit_mat.to_csv('visit.csv')\n",
    "\n",
    "    # build the recommended trajectory\n",
    "    recseq = [p0]\n",
    "    while True:\n",
    "        pi = recseq[-1]\n",
    "        pj = visit_mat.loc[pi].idxmax()\n",
    "        value = visit_mat.loc[pi, pj]\n",
    "        #print(value, int(round(value)))\n",
    "        #print(recseq)\n",
    "        assert(int(round(value)) == 1)\n",
    "        recseq.append(pj)\n",
    "        if len(recseq) == L: \n",
    "            assert(int(round(isend_vec[pj])) == 1)\n",
    "            #print('===:', recseq, ':====')\n",
    "            return np.asarray([int(x) for x in recseq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(do_inference_ILP(ps0, L0, M0, w_u, w_p, f_u, f_p))\n",
    "#print(do_inference_ILP(ps0, L0, M0, w_u, w_p, f_u, f_p, y_true=y_true0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Structured SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyModel(StructuredModel):\n",
    "    \n",
    "    def __init__(self, n_states=None, n_features=None, n_edge_features=None, inference_fun=do_inference_listViterbi):\n",
    "        self.inference_method = 'customized'\n",
    "        self.inference_fun = inference_fun\n",
    "        self.class_weight = None\n",
    "        self.inference_calls = 0\n",
    "        self.n_states = n_states\n",
    "        self.n_features = n_features\n",
    "        self.n_edge_features = n_edge_features\n",
    "        self._set_size_joint_feature()\n",
    "        self._set_class_weight()\n",
    "\n",
    "        \n",
    "    def _set_size_joint_feature(self):\n",
    "        if None not in [self.n_states, self.n_features, self.n_edge_features]:\n",
    "            self.size_joint_feature = self.n_states * self.n_features + \\\n",
    "                                      self.n_states * self.n_states * self.n_edge_features\n",
    "   \n",
    "\n",
    "    def loss(self, y, y_hat):\n",
    "        #return np.mean(np.asarray(y) != np.asarray(y_hat))     # hamming loss (normalised)\n",
    "        return np.sum(np.asarray(y) != np.asarray(y_hat))     # hamming loss\n",
    "        #return loss_F1(y, y_hat)      # F1 loss\n",
    "        #return loss_pairsF1(y, y_hat) # pairsF1 loss\n",
    "        #return loss_pairsF1(np.array(y), np.array(y_hat)) # pairsF1 loss\n",
    "\n",
    "    \n",
    "    def initialize(self, X, Y):\n",
    "        assert(len(X) == len(Y))\n",
    "        n_features = X[0][0].shape[1]\n",
    "        if self.n_features is None: \n",
    "            self.n_features = n_features\n",
    "        else:\n",
    "            assert(self.n_features == n_featurees)\n",
    "\n",
    "        n_states = len(np.unique(np.hstack([y.ravel() for y in Y])))\n",
    "        if self.n_states is None: \n",
    "            self.n_states = n_states\n",
    "        else:\n",
    "            assert(self.n_states == n_states)\n",
    "            \n",
    "        n_edge_features = X[0][1].shape[2]\n",
    "        if self.n_edge_features is None:\n",
    "            self.n_edge_features = n_edge_features\n",
    "        else:\n",
    "            assert(self.n_edge_features == n_edge_features)\n",
    "            \n",
    "        self._set_size_joint_feature()\n",
    "        self._set_class_weight()\n",
    "        \n",
    "        # joint feature scaling\n",
    "        #n_samples = len(Y)\n",
    "        #node_features_all = np.zeros((n_samples, self.n_states, self.n_features), dtype=np.float)\n",
    "        #edge_features_all = np.zeros((n_samples, self.n_states, self.n_states, self.n_edge_features), dtype=np.float)\n",
    "        #for ii in range(n_samples):\n",
    "        #    x0, x1, y = X[ii][0], X[ii][1], Y[ii]\n",
    "        #    node_features_all[ii, y[0], :] = x0[y[0], :]\n",
    "        #    for jj in range(len(y)-1):\n",
    "        #        ss, tt = y[jj], y[jj+1]\n",
    "        #        node_features_all[ii, tt, :] = x0[tt, :]\n",
    "        #        edge_features_all[ii, ss, tt, :] = x1[ss, tt, :]\n",
    "        \n",
    "        #node_max = np.max(node_features_all, axis=0)\n",
    "        #node_min = np.min(node_features_all, axis=0)\n",
    "        #edge_max = np.max(edge_features_all, axis=0)\n",
    "        #edge_min = np.min(edge_features_all, axis=0)\n",
    "        #assert(node_max.shape == (self.n_states, self.n_features))\n",
    "        #assert(node_min.shape == (self.n_states, self.n_features))\n",
    "        #assert(edge_max.shape == (self.n_states, self.n_states, self.n_edge_features))\n",
    "        #assert(edge_min.shape == (self.n_states, self.n_states, self.n_edge_features))\n",
    "        \n",
    "        #node_mean = np.mean(node_features_all, axis=0)\n",
    "        #edge_mean = np.mean(edge_features_all, axis=0)\n",
    "        #node_std = np.std(node_features_all, axis=0)\n",
    "        #edge_std = np.std(edge_features_all, axis=0)\n",
    "        #assert(node_mean.shape == (self.n_states, self.n_features))\n",
    "        #assert(node_std.shape  == (self.n_states, self.n_features))\n",
    "        #assert(edge_mean.shape == (self.n_states, self.n_states, self.n_edge_features))\n",
    "        #assert(edge_std.shape  == (self.n_states, self.n_states, self.n_edge_features))\n",
    "        \n",
    "        # save for scaling test data\n",
    "        #self.node_max = node_max; self.node_min = node_min\n",
    "        #self.edge_max = edge_max; self.edge_min = edge_min\n",
    "        #self.node_mean = node_mean; self.node_std = node_std\n",
    "        #self.edge_mean = edge_mean; self.edge_std = edge_std\n",
    "        \n",
    "        # scaling features\n",
    "        #for ii in range(n_samples):\n",
    "        #    unaries, pw = scale_features_linear(X[ii][0], X[ii][1],\n",
    "        #                                        node_max=self.node_max, node_min=self.node_min,\n",
    "        #                                        edge_max=self.edge_max, edge_min=self.edge_min)\n",
    "        #    X[ii] = (unaries, pw, X[ii][2])\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\"%s(n_states: %d, inference_method: %s, n_features: %d, n_edge_features: %d)\"\n",
    "                % (type(self).__name__, self.n_states, self.inference_method, self.n_features, self.n_edge_features))\n",
    "    \n",
    "    \n",
    "    def joint_feature(self, x, y):\n",
    "        assert(not isinstance(y, tuple))\n",
    "        unary_features = x[0] # unary features of all POIs: n_POIs x n_features\n",
    "        pw_features = x[1]    # pairwise features of all transitions: n_POIs x n_POIs x n_edge_features\n",
    "        query = x[2]          # query = (startPOI, length)\n",
    "        n_nodes = query[1]\n",
    "        \n",
    "        #print('y:', y)\n",
    "        \n",
    "        #assert(unary_features.ndim == 2)\n",
    "        #assert(pw_features.ndim == 3)\n",
    "        #assert(len(query) == 3)\n",
    "        assert(n_nodes == len(y))\n",
    "        assert(unary_features.shape == (self.n_states, self.n_features))\n",
    "        assert(pw_features.shape == (self.n_states, self.n_states, self.n_edge_features))\n",
    "        \n",
    "        node_features = np.zeros((self.n_states, self.n_features), dtype=np.float)\n",
    "        edge_features = np.zeros((self.n_states, self.n_states, self.n_edge_features), dtype=np.float)\n",
    "        \n",
    "        node_features[y[0], :] = unary_features[y[0], :]\n",
    "        for j in range(len(y)-1):\n",
    "            ss, tt = y[j], y[j+1]\n",
    "            node_features[tt, :] = unary_features[tt, :]\n",
    "            edge_features[ss, tt, :] = pw_features[ss, tt, :]\n",
    "        \n",
    "        # sum node/edge features after scaling: \n",
    "        # equivalent to share parameters between features of different POIs/transitions\n",
    "        joint_feature_vector = np.hstack([node_features.ravel(), edge_features.ravel()])\n",
    "        \n",
    "        return joint_feature_vector\n",
    "            \n",
    "    \n",
    "    def loss_augmented_inference(self, x, y, w, relaxed=None):\n",
    "        #print('loss_augmented_inference:', y)\n",
    "        # inference procedure for training: (x, y) from training set (with features already scaled)\n",
    "        #\n",
    "        # argmax_y_hat np.dot(w, joint_feature(x, y_hat)) + loss(y, y_hat)\n",
    "        # \n",
    "        # the loss function should be decomposible in order to use Viterbi decoding, here we use Hamming loss\n",
    "        #\n",
    "        # x[0]: (unscaled) unary features of all POIs: n_POIs x n_features\n",
    "        # x[1]: (unscaled) pairwise features of all transitions: n_POIs x n_POIs x n_edge_features\n",
    "        # x[2]: query = (startPOI, length)\n",
    "        unary_features = x[0]\n",
    "        pw_features = x[1]\n",
    "        query = x[2]\n",
    "        \n",
    "        assert(unary_features.ndim == 2)\n",
    "        assert(pw_features.ndim == 3)\n",
    "        assert(len(query) == 2)\n",
    "        \n",
    "        ps = query[0]\n",
    "        L = query[1]\n",
    "        M = unary_features.shape[0]  # total number of POIs\n",
    "        \n",
    "        self._check_size_w(w)\n",
    "        unary_params = w[:self.n_states * self.n_features].reshape((self.n_states, self.n_features))\n",
    "        pw_params = w[self.n_states * self.n_features:].reshape((self.n_states, self.n_states, self.n_edge_features))\n",
    "        \n",
    "        y_hat = do_inference_viterbi(ps, L, M, unary_params, pw_params, unary_features, pw_features, y_true=y)\n",
    "        \n",
    "        #y_hat = do_inference_ILP(ps, L, N, unary_params, pw_params, unary_features, pw_features, y_true=y)\n",
    "        #assert(len(y_hat) == len(set(y_hat)))\n",
    "        \n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "    def inference(self, x, w, relaxed=False, return_energy=False):\n",
    "        #print('inference')\n",
    "        # inference procedure for testing: x from test set (features needs to be scaled)\n",
    "        #\n",
    "        # argmax_y np.dot(w, joint_feature(x, y))\n",
    "        #\n",
    "        # x[0]: (unscaled) unary features of all POIs: n_POIs x n_features\n",
    "        # x[1]: (unscaled) pairwise features of all transitions: n_POIs x n_POIs x n_edge_features\n",
    "        # x[2]: query = (startPOI, length)\n",
    "        unary_features = x[0]\n",
    "        pw_features = x[1]\n",
    "        query = x[2]\n",
    "        \n",
    "        assert(unary_features.ndim == 2)\n",
    "        assert(pw_features.ndim == 3)\n",
    "        assert(len(query) == 2)\n",
    "        \n",
    "        ps = query[0]\n",
    "        L = query[1]\n",
    "        M = unary_features.shape[0]  # total number of POIs\n",
    "        \n",
    "        self._check_size_w(w)\n",
    "        unary_params = w[:self.n_states * self.n_features].reshape((self.n_states, self.n_features))\n",
    "        pw_params = w[self.n_states * self.n_features:].reshape((self.n_states, self.n_states, self.n_edge_features))\n",
    "        \n",
    "        #y_pred = do_inference_viterbi(ps, L, M, unary_params, pw_params, unary_features, pw_features)\n",
    "        #y_pred = do_inference_listViterbi(ps, L, M, unary_params, pw_params, unary_features, pw_features)\n",
    "        #y_pred = do_inference_ILP(ps, L, M, unary_params, pw_params, unary_features, pw_features)\n",
    "        #assert(len(y_pred) == len(set(y_pred)))\n",
    "        y_pred = self.inference_fun(ps, L, M, unary_params, pw_params, unary_features, pw_features)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute node features (singleton)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_node_features(startPOI, nPOI, poi_ix, poi_info, poi_clusters, cats, clusters):\n",
    "    \"\"\"\n",
    "    Generate feature vectors for all POIs given query (startPOI, nPOI)\n",
    "    \"\"\"\n",
    "    assert(isinstance(cats, list))\n",
    "    assert(isinstance(clusters, list))\n",
    "    \n",
    "    columns = DF_COLUMNS[3:]\n",
    "    poi_distmat = POI_DISTMAT\n",
    "    query_id_dict = QUERY_ID_DICT\n",
    "    key = (p0, trajLen) = (startPOI, nPOI)\n",
    "    assert(key in query_id_dict)\n",
    "    assert(p0 in poi_info.index)\n",
    "    \n",
    "    # DEBUG: use uniform node features\n",
    "    nrows = len(poi_ix)\n",
    "    ncols = len(columns) + len(cats) + len(clusters) - 2\n",
    "    #return np.ones((nrows, ncols), dtype=np.float)\n",
    "    #return np.zeros((nrows, ncols), dtype=np.float)\n",
    "    \n",
    "    poi_list = poi_ix\n",
    "    df_ = pd.DataFrame(index=np.arange(len(poi_list)), columns=columns)\n",
    "        \n",
    "    for i in range(df_.index.shape[0]):\n",
    "        poi = poi_list[i]\n",
    "        lon, lat = poi_info.loc[poi, 'poiLon'], poi_info.loc[poi, 'poiLat']\n",
    "        pop, nvisit = poi_info.loc[poi, 'popularity'], poi_info.loc[poi, 'nVisit']\n",
    "        cat, cluster = poi_info.loc[poi, 'poiCat'], poi_clusters.loc[poi, 'clusterID']\n",
    "        duration = poi_info.loc[poi, 'avgDuration']\n",
    "        idx = df_.index[i]\n",
    "        df_.set_value(idx, 'category', tuple((cat == np.array(cats)).astype(np.int) * 2 - 1))\n",
    "        df_.set_value(idx, 'neighbourhood', tuple((cluster == np.array(clusters)).astype(np.int) * 2 - 1))\n",
    "        df_.loc[idx, 'popularity'] = LOG_SMALL if pop < 1 else np.log10(pop)\n",
    "        df_.loc[idx, 'nVisit'] = LOG_SMALL if nvisit < 1 else np.log10(nvisit)\n",
    "        df_.loc[idx, 'avgDuration'] = LOG_SMALL if duration < 1 else np.log10(duration)\n",
    "        df_.loc[idx, 'trajLen'] = trajLen\n",
    "        df_.loc[idx, 'sameCatStart'] = 1 if cat == poi_all.loc[p0, 'poiCat'] else -1\n",
    "        df_.loc[idx, 'distStart'] = poi_distmat.loc[poi, p0]\n",
    "        df_.loc[idx, 'diffPopStart'] = pop - poi_info.loc[p0, 'popularity']\n",
    "        df_.loc[idx, 'diffNVisitStart'] = nvisit - poi_info.loc[p0, 'nVisit']\n",
    "        df_.loc[idx, 'diffDurationStart'] = duration - poi_info.loc[p0, 'avgDuration']\n",
    "        df_.loc[idx, 'sameNeighbourhoodStart'] = 1 if cluster == poi_clusters.loc[p0, 'clusterID'] else -1\n",
    "    \n",
    "    # features other than category and neighbourhood\n",
    "    X = df_[list(set(df_.columns) - {'category', 'neighbourhood'})].values  \n",
    "    \n",
    "    # boolean features: category (+1, -1)\n",
    "    cat_features = np.vstack([list(df_.loc[x, 'category']) for x in df_.index])\n",
    "    \n",
    "    # boolean features: neighbourhood (+1, -1)\n",
    "    neigh_features = np.vstack([list(df_.loc[x, 'neighbourhood']) for x in df_.index])\n",
    "    \n",
    "    return np.hstack([X, cat_features, neigh_features]).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute edge features (transiton / pairwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_edge_features(trajid_list, poi_ix, traj_dict, poi_info):    \n",
    "    feature_names = ['poiCat', 'popularity', 'nVisit', 'avgDuration', 'clusterID']\n",
    "    n_features = len(feature_names)\n",
    "    \n",
    "    # DEBUG: use uniform edge features\n",
    "    #return np.ones((len(poi_ix), len(poi_ix), n_features), dtype=np.float)\n",
    "    #return np.zeros((len(poi_ix), len(poi_ix), n_features), dtype=np.float)\n",
    "    \n",
    "    transmat_cat                        = gen_transmat_cat(trajid_list, traj_dict, poi_info)\n",
    "    transmat_pop,      logbins_pop      = gen_transmat_pop(trajid_list, traj_dict, poi_info)\n",
    "    transmat_visit,    logbins_visit    = gen_transmat_visit(trajid_list, traj_dict, poi_info)\n",
    "    transmat_duration, logbins_duration = gen_transmat_duration(trajid_list, traj_dict, poi_info)\n",
    "    transmat_neighbor, poi_clusters     = gen_transmat_neighbor(trajid_list, traj_dict, poi_info)\n",
    "    \n",
    "    poi_features = pd.DataFrame(data=np.zeros((len(poi_ix), len(feature_names))), \\\n",
    "                                columns=feature_names, index=poi_ix)\n",
    "    poi_features.index.name = 'poiID'\n",
    "    poi_features['poiCat'] = poi_info.loc[poi_ix, 'poiCat']\n",
    "    poi_features['popularity'] = np.digitize(poi_info.loc[poi_ix, 'popularity'], logbins_pop)\n",
    "    poi_features['nVisit'] = np.digitize(poi_info.loc[poi_ix, 'nVisit'], logbins_visit)\n",
    "    poi_features['avgDuration'] = np.digitize(poi_info.loc[poi_ix, 'avgDuration'], logbins_duration)\n",
    "    poi_features['clusterID'] = poi_clusters.loc[poi_ix, 'clusterID']\n",
    "    \n",
    "    edge_features = np.zeros((len(poi_ix), len(poi_ix), n_features), dtype=np.float64)\n",
    "    \n",
    "    for j in range(len(poi_ix)): # NOTE: POI order\n",
    "        pj = poi_ix[j]\n",
    "        cat, pop = poi_features.loc[pj, 'poiCat'], poi_features.loc[pj, 'popularity']\n",
    "        visit, cluster = poi_features.loc[pj, 'nVisit'], poi_features.loc[pj, 'clusterID']\n",
    "        duration = poi_features.loc[pj, 'avgDuration']\n",
    "        \n",
    "        for k in range(len(poi_ix)): # NOTE: POI order\n",
    "            pk = poi_ix[k]\n",
    "            edge_features[j, k, :] = np.log10( np.array(\n",
    "                    [transmat_cat.loc[cat, poi_features.loc[pk, 'poiCat']], \\\n",
    "                     transmat_pop.loc[pop, poi_features.loc[pk, 'popularity']], \\\n",
    "                     transmat_visit.loc[visit, poi_features.loc[pk, 'nVisit']], \\\n",
    "                     transmat_duration.loc[duration, poi_features.loc[pk, 'avgDuration']], \\\n",
    "                     transmat_neighbor.loc[cluster, poi_features.loc[pk, 'clusterID']]] ) )\n",
    "    return edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SSVM:\n",
    "    def __init__(self, C=1.0, inference_fun=do_inference_listViterbi, debug=False):\n",
    "        assert(C > 0)\n",
    "        self.C = C\n",
    "        self.inference_fun = inference_fun\n",
    "        self.debug = debug\n",
    "        self.trained = False\n",
    "        \n",
    "        if ABS_SCALER == True:\n",
    "            self.scaler = MaxAbsScaler(copy=False)\n",
    "        else:\n",
    "            self.scaler = MinMaxScaler(feature_range=(-1,1), copy=False)\n",
    "            #self.scaler = StandardScaler(copy=False)\n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self, trajid_set_train):\n",
    "        self.poi_info = calc_poi_info(list(trajid_set_train), traj_all, poi_all)\n",
    "\n",
    "        # build POI_ID <--> POI__INDEX mapping for POIs used to train CRF\n",
    "        # which means only POIs in traj such that len(traj) >= 2 are included\n",
    "        poi_set = set()\n",
    "        for x in trajid_set_train:\n",
    "            if len(traj_dict[x]) >= 2:\n",
    "                poi_set = poi_set | set(traj_dict[x])\n",
    "        self.poi_ix = sorted(poi_set)\n",
    "        self.poi_id_dict, self.poi_id_rdict = dict(), dict()\n",
    "        for idx, poi in enumerate(self.poi_ix):\n",
    "            self.poi_id_dict[poi] = idx\n",
    "            self.poi_id_rdict[idx] = poi\n",
    "\n",
    "        # generate training data\n",
    "        train_traj_list = [traj_dict[k] for k in trajid_set_train if len(traj_dict[k]) >= 2]\n",
    "        node_features_list = Parallel(n_jobs=N_JOBS)\\\n",
    "                             (delayed(calc_node_features)\\\n",
    "                              (tr[0], len(tr), self.poi_ix, self.poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                               cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST) for tr in train_traj_list)\n",
    "        self.edge_features = calc_edge_features(list(trajid_set_train), self.poi_ix, traj_dict, self.poi_info)\n",
    "\n",
    "        # feature scaling\n",
    "        # should each example be flattened to one vector before scaling?\n",
    "        self.fdim = node_features_list[0].shape\n",
    "        X_node_all = np.vstack(node_features_list)\n",
    "        #print(self.fdim)\n",
    "        #print(X_node_all.shape)\n",
    "        #X_node_all = X_node_all.reshape(len(node_features_list), -1) # flatten every example to a vector\n",
    "        X_node_all = self.scaler.fit_transform(X_node_all)\n",
    "        X_node_all = X_node_all.reshape(-1, self.fdim[0], self.fdim[1])\n",
    "\n",
    "        assert(len(train_traj_list) == X_node_all.shape[0])\n",
    "        X_train = [(X_node_all[k, :, :], \\\n",
    "                    self.edge_features.copy(), \\\n",
    "                    (self.poi_id_dict[train_traj_list[k][0]], len(train_traj_list[k]))) \\\n",
    "                   for k in range(len(train_traj_list))]\n",
    "        y_train = [np.array([self.poi_id_dict[k] for k in tr]) for tr in train_traj_list]\n",
    "        assert(len(X_train) == len(y_train))\n",
    "\n",
    "        # train\n",
    "        sm = MyModel(inference_fun=self.inference_fun)\n",
    "        verbose = 5 if self.debug == True else 0\n",
    "        self.osssvm = OneSlackSSVM(model=sm, C=self.C, n_jobs=N_JOBS, verbose=verbose)\n",
    "        self.osssvm.fit(X_train, y_train, initialize=True)\n",
    "        self.trained = True\n",
    "        print('SSVM training finished.')\n",
    "        \n",
    "\n",
    "\n",
    "    def predict(self, startPOI, nPOI):\n",
    "        assert(self.trained == True)\n",
    "        if startPOI not in self.poi_ix: return None\n",
    "        X_node_test = calc_node_features(startPOI, nPOI, self.poi_ix, self.poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                         cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "\n",
    "        # feature scaling\n",
    "        # should each example be flattened to one vector before scaling?\n",
    "        #X_node_test = X_node_test.reshape(1, -1) # flatten test example to a vector\n",
    "        X_node_test = self.scaler.transform(X_node_test)\n",
    "        #X_node_test = X_node_test.reshape(self.fdim)\n",
    "\n",
    "        X_test = [(X_node_test, self.edge_features, (self.poi_id_dict[startPOI], nPOI))]\n",
    "        y_hat = self.osssvm.predict(X_test)\n",
    "\n",
    "        return np.array([self.poi_id_rdict[x] for x in y_hat[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested cross-validation with Monte-Carlo cross-validation as inner loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inference_methods = [do_inference_greedy, do_inference_viterbi, do_inference_listViterbi, do_inference_ILP]\n",
    "methods_suffix = ['greedy', 'viterbi', 'listViterbi', 'ILP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method_ix = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recdict_ssvm = dict()\n",
    "cnt = 1\n",
    "keys = sorted(TRAJ_GROUP_DICT.keys())\n",
    "mc_portion = 0.1\n",
    "\n",
    "# outer loop to evaluate the test performance by cross validation\n",
    "for i in range(len(keys)):\n",
    "    ps, L = keys[i]\n",
    "    \n",
    "    best_C = 1\n",
    "    #best_F1 = 0; best_pF1 = 0\n",
    "    best_Tau = 0\n",
    "    keys_cv = keys[:i] + keys[i+1:]\n",
    "    \n",
    "    # tune regularisation constant C\n",
    "    for ssvm_C in C_SET:\n",
    "        print('\\n--------------- try_C: %f ---------------\\n' % ssvm_C); sys.stdout.flush() \n",
    "        F1_ssvm = []; pF1_ssvm = []; Tau_ssvm = []        \n",
    "        \n",
    "        # inner loop to evaluate the performance of a model with a specified C by Monte-Carlo cross validation\n",
    "        for j in range(MC_NITER):\n",
    "            while True: # make sure the start POI in test set are also in training set\n",
    "                rand_ix = np.arange(len(keys_cv)); np.random.shuffle(rand_ix)\n",
    "                test_ix = rand_ix[:int(MC_PORTION*len(rand_ix))]\n",
    "                assert(len(test_ix) > 0)\n",
    "                trajid_set_train = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "                for j in test_ix: \n",
    "                    trajid_set_train = trajid_set_train - TRAJ_GROUP_DICT[keys_cv[j]]\n",
    "                poi_set = set()\n",
    "                for tid in trajid_set_train: poi_set = poi_set | set(traj_dict[tid])\n",
    "                good_partition = True\n",
    "                for j in test_ix: \n",
    "                    if keys_cv[j][0] not in poi_set: good_partition = False; break\n",
    "                if good_partition == True: break\n",
    "\n",
    "            # train\n",
    "            ssvm = SSVM(C=ssvm_C, inference_fun=inference_methods[method_ix])#, debug=True)\n",
    "            ssvm.train(trajid_set_train)\n",
    "            \n",
    "            # test\n",
    "            for j in test_ix:\n",
    "                ps_cv, L_cv = keys_cv[j]\n",
    "                y_hat = ssvm.predict(ps_cv, L_cv)\n",
    "                if y_hat is not None:\n",
    "                    F1, pF1, tau = evaluate(y_hat, TRAJ_GROUP_DICT[keys_cv[j]])\n",
    "                    F1_ssvm.append(F1); pF1_ssvm.append(pF1); Tau_ssvm.append(tau)\n",
    "        \n",
    "        #mean_F1 = np.mean(F1_ssvm); mean_pF1 = np.mean(pF1_ssvm)\n",
    "        mean_Tau = np.mean(Tau_ssvm)\n",
    "        print('mean_Tau: %.3f' % mean_Tau)\n",
    "        if mean_Tau > best_Tau:\n",
    "            best_Tau = mean_Tau\n",
    "            best_C = ssvm_C\n",
    "    print('\\n--------------- %d/%d, Query: (%d, %d), Best_C: %f ---------------\\n' % (cnt, len(keys), ps, L, best_C))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # train model using all examples in training set and measure performance on test set\n",
    "    trajid_set_train = set(trajid_set_all) - TRAJ_GROUP_DICT[keys[i]]\n",
    "    ssvm = SSVM(C=best_C)\n",
    "    if ssvm.train(trajid_set_train) == True:\n",
    "        y_hat = ssvm.predict(ps, L)\n",
    "        if y_hat is not None:\n",
    "            recdict_ssvm[(ps, L)] = {'PRED': y_hat, 'W': ssvm.osssvm.w, 'C': ssvm.C}\n",
    "        \n",
    "    cnt += 1; #print_progress(cnt, len(keys)); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F1_ssvm = []; pF1_ssvm = []; tau_ssvm = []\n",
    "for key in sorted(recdict_ssvm.keys()):\n",
    "    F1, pF1, tau = evaluate(recdict_ssvm[key]['PRED'], TRAJ_GROUP_DICT[key])\n",
    "    F1_ssvm.append(F1); pF1_ssvm.append(pF1); tau_ssvm.append(tau)\n",
    "print('SSVM: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "      (np.mean(F1_ssvm), np.std(F1_ssvm)/np.sqrt(len(F1_ssvm)), \\\n",
    "       np.mean(pF1_ssvm), np.std(pF1_ssvm)/np.sqrt(len(pF1_ssvm)), \\\n",
    "       np.mean(tau_ssvm), np.std(tau_ssvm)/np.sqrt(len(tau_ssvm))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fssvm = os.path.join(data_dir, 'ssvm-' + methods_suffix[method_ix] + '-' + dat_suffix[dat_ix] + '.pkl')\n",
    "fssvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(recdict_ssvm, open(fssvm, 'bw'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
