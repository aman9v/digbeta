{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on Generated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run notebook ```ssvm.ipynb```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run 'ssvm.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load trained parameters and prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = os.path.join(data_dir, 'ssvm-listViterbi-Glas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssvm_lv = pickle.load(open(fname, 'rb'))  # a dict: query -> {'PRED': trajectory, 'C': ssvm-c, 'W': model_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment_on_toy_data(query, C, W):\n",
    "    trajid_set = set(trajid_set_all) - TRAJ_GROUP_DICT[query]\n",
    "    poi_set = set()\n",
    "    for tid in trajid_set: poi_set = poi_set | set(traj_dict[tid])\n",
    "    poi_list = sorted(poi_list)\n",
    "    n_states = len(poi_set)\n",
    "    n_edge_features = 5\n",
    "    n_node_features = (len(W) - n_states * n_states * n_edge_features) // n_states\n",
    "    unary_params = W[:n_states * n_node_features].reshape(n_states, n_node_features)\n",
    "    pw_params = W[n_states * n_node_features:].reshape((n_states, n_states, n_edge_features))    \n",
    "    lengthes = [3, 4, 5, 6, 7]\n",
    "    \n",
    "    poi_id_dict, poi_id_rdict = dict(), dict()\n",
    "    for idx, poi in enumerate(poi_list):\n",
    "        poi_id_dict[poi] = idx\n",
    "        poi_id_rdict[idx] = poi\n",
    "        \n",
    "    # compute features\n",
    "    poi_info = calc_poi_info(list(trajid_set), traj_all, poi_all)\n",
    "    traj_list = [traj_dict[k] for k in trajid_set if len(traj_dict[k]) >= 2]\n",
    "    node_features_list = Parallel(n_jobs=N_JOBS)\\\n",
    "                         (delayed(calc_node_features)\\\n",
    "                          (tr[0], len(tr), poi_list, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                           cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST) for tr in traj_list)\n",
    "    edge_features = calc_edge_features(list(trajid_set), poi_list, traj_dict, poi_info)\n",
    "\n",
    "    # feature scaling\n",
    "    X_node_all = np.vstack(node_features_list)\n",
    "    scaler = MaxAbsScaler(copy=False)\n",
    "    X_node_all = scaler.fit_transform(X_node_all)\n",
    "    assert(len(traj_list) == X_node_all.shape[0])\n",
    "    \n",
    "    # generate trajectories\n",
    "    fake_labels = []\n",
    "    for poi in sorted(poi_list):\n",
    "        for L in lengthes:\n",
    "            X_node_test = calc_node_features(poi, L, poi_list, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                                             cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "            X_node_test = scaler.transform(X_node_test)  # feature scaling\n",
    "            unary_features = X_node_test\n",
    "            pw_features = edge_features.copy()\n",
    "            y_pred = do_inference_listViterbi(poi, L, len(poi_set), unary_params, pw_params, unary_features, pw_features)\n",
    "            fake_labels.append([poi_id_rdict[p] for p in y_pred])\n",
    "            \n",
    "    # do leave-one-out cross validation on generated trajectories\n",
    "    predictions = dict()\n",
    "    for i in range(len(fake_labels)):\n",
    "        train_labels = fake_labels[:i] + fake_labels[i+1:]\n",
    "        node_features_all = Parallel(n_jobs=N_JOBS)\\\n",
    "                            (delayed(calc_node_features)\\\n",
    "                             (tr[0], len(tr), poi_list, poi_info, poi_clusters=POI_CLUSTERS, \\\n",
    "                              cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST) for tr in train_labels)\n",
    "        X_node_train = np.vstack(node_features_all)\n",
    "        scaler_train = MaxAbsScaler(copy=False)\n",
    "        X_node_train = scaler_train.fit_transform(X_node_train)\n",
    "        assert(len(fake_labels) == X_node_train.shape[0])\n",
    "        X_train = [(X_node_train[k, :, :], edge_features.copy(), \n",
    "                    (poi_id_dict[train_labels[k][0]], len(train_labels[k]))) for k in range(len(train_labels))]\n",
    "        y_train = [np.array([poi_id_dict[k] for k in tr]) for tr in train_labels]\n",
    "        assert(len(X_train) == len(y_train))\n",
    "        sm = MyModel(inference_fun=do_inference_listViterbi)\n",
    "        verbose = 0\n",
    "        osssvm = OneSlackSSVM(model=sm, C=C, n_jobs=N_JOBS, verbose=verbose)\n",
    "        try:\n",
    "            osssvm.fit(X_train, y_train, initialize=True)\n",
    "            print('SSVM training finished.')\n",
    "        except:\n",
    "            sys.stderr.write('SSVM training FAILED.\\n')\n",
    "\n",
    "        # predict using trained model\n",
    "        startPOI, nPOI = fake_labels[i][0], len(fake_labels[i])\n",
    "        X_node_test = calc_node_features(startPOI, nPOI, poi_list, poi_info, poi_clusters=POI_CLUSTERS, \n",
    "                                         cats=POI_CAT_LIST, clusters=POI_CLUSTER_LIST)\n",
    "        X_node_test = scaler_train.transform(X_node_test)\n",
    "        X_test = [(X_node_test, edge_features, (poi_id_dict[query[0]], query[1]))]\n",
    "        y_hat = osssvm.predict(X_test)\n",
    "        predictions[(startPOI, nPOI)] = {'PRED': np.array([poi_id_rdict[p] for p in y_hat[0]]), 'REAL':fake_labels[i]}\n",
    "        \n",
    "    # evaluation\n",
    "    F1_ssvm = []; pF1_ssvm = []; tau_ssvm = []\n",
    "    for key in sorted(predictions.keys()):\n",
    "        F1 = calc_F1(predictions[key]['REAL'], predictions[key]['PRED'])\n",
    "        pF1 = calc_pairsF1(predictions[key]['REAL'], predictions[key]['PRED'])\n",
    "        tau = calc_kendalltau(predictions[key]['REAL'], predictions[key]['PRED'])\n",
    "        F1_ssvm.append(F1); pF1_ssvm.append(pF1); tau_ssvm.append(tau)\n",
    "    print('SSVM: F1 (%.3f, %.3f), pairsF1 (%.3f, %.3f), Tau (%.3f, %.3f)' % \\\n",
    "          (np.mean(F1_ssvm), np.std(F1_ssvm)/np.sqrt(len(F1_ssvm)), \\\n",
    "           np.mean(pF1_ssvm), np.std(pF1_ssvm)/np.sqrt(len(pF1_ssvm)), \\\n",
    "           np.mean(tau_ssvm), np.std(tau_ssvm)/np.sqrt(len(tau_ssvm))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for query in sorted(ssvm_lv.keys()):\n",
    "    experiment_on_toy_data(query, ssvm_lv[query]['C'], ssvm_lv[query]['W'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
