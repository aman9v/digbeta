{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Trajectory Recommendation - Shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#% matplotlib inline\n",
    "\n",
    "import os, sys, time\n",
    "import math, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data/data-new'\n",
    "dat_suffix = ['Osak', 'Glas', 'Edin', 'Toro', 'Melb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dat_ix = 0  #NOTE: this variable should be defined in another notebook that runs this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BIN_CLUSTER = 5  # discritization parameter\n",
    "LOG_SMALL = -10  # log(x) when x is a very small positive real number\n",
    "LOG_ZERO = -1000 # log(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpoi = os.path.join(data_dir, 'poi-' + dat_suffix[dat_ix] + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_all = pd.read_csv(fpoi)\n",
    "poi_all.set_index('poiID', inplace=True)\n",
    "poi_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ftraj = os.path.join(data_dir, 'traj-' + dat_suffix[dat_ix] + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traj_all = pd.read_csv(ftraj)\n",
    "traj_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_user = traj_all['userID'].unique().shape[0]\n",
    "num_poi = traj_all['poiID'].unique().shape[0]\n",
    "num_traj = traj_all['trajID'].unique().shape[0]\n",
    "pd.DataFrame({'#user': num_user, '#poi': num_poi, '#traj': num_traj, '#traj/user': num_traj/num_user}, \\\n",
    "             index=[str(dat_suffix[dat_ix])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of the number of POIs in trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ax = traj_all['trajLen'].hist(bins=20)\n",
    "#ax.set_yscale('log')\n",
    "#ax.set_xlabel('#POIs in trajectory'); ax.set_ylabel('#Trajectories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of POI visit duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ax = traj_all['poiDuration'].hist(bins=20)\n",
    "#ax.set_xscale('log')\n",
    "#ax.set_yscale('log')\n",
    "#ax.set_xlabel('POI visit duration (sec)'); ax.set_ylabel('#POI visits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print computing progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_progress(cnt, total):\n",
    "    \"\"\"Display a progress bar\"\"\"\n",
    "    assert(cnt > 0 and total > 0 and cnt <= total)\n",
    "    length = 80\n",
    "    ratio = cnt / total\n",
    "    n = int(length * ratio)\n",
    "    sys.stdout.write('\\r[%-80s] %d%%' % ('-'*n, int(ratio*100)))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract trajectory, i.e., a list of POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_traj(tid, traj_all):\n",
    "    traj = traj_all[traj_all['trajID'] == tid].copy()\n",
    "    traj.sort_values(by=['startTime'], ascending=True, inplace=True)\n",
    "    return traj['poiID'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute POI properties, e.g., popularity, total number of visit, average visit duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_poi_info(trajid_list, traj_all, poi_all):\n",
    "    assert(len(trajid_list) > 0)\n",
    "    poi_info = traj_all[traj_all['trajID'] == trajid_list[0]][['poiID', 'poiDuration']].copy() \n",
    "    for i in range(1, len(trajid_list)):\n",
    "        traj = traj_all[traj_all['trajID'] == trajid_list[i]][['poiID', 'poiDuration']]\n",
    "        poi_info = poi_info.append(traj, ignore_index=True)\n",
    "    \n",
    "    poi_info = poi_info.groupby('poiID').agg([np.mean, np.size])\n",
    "    poi_info.columns = poi_info.columns.droplevel()\n",
    "    poi_info.reset_index(inplace=True)\n",
    "    poi_info.rename(columns={'mean':'avgDuration', 'size':'nVisit'}, inplace=True)\n",
    "    poi_info.set_index('poiID', inplace=True) \n",
    "    poi_info['poiCat'] = poi_all.loc[poi_info.index, 'poiCat']\n",
    "    poi_info['poiLon'] = poi_all.loc[poi_info.index, 'poiLon']\n",
    "    poi_info['poiLat'] = poi_all.loc[poi_info.index, 'poiLat']\n",
    "    \n",
    "    # POI popularity: the number of distinct users that visited the POI\n",
    "    pop_df = traj_all[traj_all['trajID'].isin(trajid_list)][['poiID', 'userID']].copy()\n",
    "    pop_df = pop_df.groupby('poiID').agg(pd.Series.nunique)\n",
    "    pop_df.rename(columns={'userID':'nunique'}, inplace=True)\n",
    "    poi_info['popularity'] = pop_df.loc[poi_info.index, 'nunique']\n",
    "    \n",
    "    return poi_info.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute distance between two POIs using [Haversine formula](http://en.wikipedia.org/wiki/Great-circle_distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_dist_vec(longitudes1, latitudes1, longitudes2, latitudes2):\n",
    "    \"\"\"Calculate the distance (unit: km) between two places on earth, vectorised\"\"\"\n",
    "    # convert degrees to radians\n",
    "    lng1 = np.radians(longitudes1)\n",
    "    lat1 = np.radians(latitudes1)\n",
    "    lng2 = np.radians(longitudes2)\n",
    "    lat2 = np.radians(latitudes2)\n",
    "    radius = 6371.0088 # mean earth radius, en.wikipedia.org/wiki/Earth_radius#Mean_radius\n",
    "\n",
    "    # The haversine formula, en.wikipedia.org/wiki/Great-circle_distance\n",
    "    dlng = np.fabs(lng1 - lng2)\n",
    "    dlat = np.fabs(lat1 - lat2)\n",
    "    dist =  2 * radius * np.arcsin( np.sqrt( \n",
    "                (np.sin(0.5*dlat))**2 + np.cos(lat1) * np.cos(lat2) * (np.sin(0.5*dlng))**2 ))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Auxiliary Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance between POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POI_DISTMAT = pd.DataFrame(data=np.zeros((poi_all.shape[0], poi_all.shape[0]), dtype=np.float), \\\n",
    "                           index=poi_all.index, columns=poi_all.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ix in poi_all.index:\n",
    "    POI_DISTMAT.loc[ix] = calc_dist_vec(poi_all.loc[ix, 'poiLon'], \\\n",
    "                                        poi_all.loc[ix, 'poiLat'], \\\n",
    "                                        poi_all['poiLon'], \\\n",
    "                                        poi_all['poiLat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trajid_set_all = sorted(traj_all['trajID'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_info_all = calc_poi_info(trajid_set_all, traj_all, poi_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary maps every trajectory ID to the actual trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traj_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for trajid in trajid_set_all:\n",
    "    traj = extract_traj(trajid, traj_all)\n",
    "    assert(trajid not in traj_dict)\n",
    "    traj_dict[trajid] = traj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a *query* (in IR terminology) using tuple (start POI, #POI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_ID_DICT = dict()  # (start, length) --> qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = [(traj_dict[x][0], len(traj_dict[x])) \\\n",
    "        for x in sorted(traj_dict.keys()) if len(traj_dict[x]) > 1]\n",
    "cnt = 0\n",
    "for key in keys:\n",
    "    if key not in QUERY_ID_DICT:   # (start, length) --> qid\n",
    "        QUERY_ID_DICT[key] = cnt\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('#traj in total:', len(trajid_set_all))\n",
    "print('#traj (length >= 2):', traj_all[traj_all['trajLen'] >= 2]['trajID'].unique().shape[0])\n",
    "print('#traj length max:', traj_all['trajLen'].max())\n",
    "print('#query tuple:', len(QUERY_ID_DICT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of trajectories for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAJ_GROUP_DICT = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tid in traj_dict:\n",
    "    if len(traj_dict[tid]) > 1:\n",
    "        key = (traj_dict[tid][0], len(traj_dict[tid]))\n",
    "        if key in TRAJ_GROUP_DICT: TRAJ_GROUP_DICT[key].add(tid)\n",
    "        else:                      TRAJ_GROUP_DICT[key] = set({tid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [key for key in sorted(TRAJ_GROUP_DICT.keys())]\n",
    "y = [len(TRAJ_GROUP_DICT[key]) for key in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ix1 = np.argmin(y)\n",
    "ix2 = np.argmax(y)\n",
    "#print('min:', x[ix1], y[ix1])\n",
    "#print('max:', x[ix2], y[ix2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.scatter(list(range(len(x))), sorted(y, reverse=True))\n",
    "#plt.ylabel('#Trajectories')\n",
    "#plt.xlabel('Query index')\n",
    "#plt.xlim(xmin=-1)\n",
    "#plt.ylim(ymin=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. POI Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI Features given query (`startPOI`, `nPOI`):\n",
    "1. `category`: one-hot encoding of POI category, encode `True` as `1` and `False` as `-1`\n",
    "1. `neighbourhood`: one-hot encoding of POI cluster, encode `True` as `1` and `False` as `-1`\n",
    "1. `popularity`: log of POI popularity, i.e., the number of distinct users that visited the POI\n",
    "1. `nVisit`: log of the total number of visit by all users\n",
    "1. `avgDuration`: log of average POI visit duration\n",
    "1. `trajLen`: trajectory length, i.e., the number of POIs `nPOI` in trajectory, copy from query\n",
    "1. `sameCatStart`: 1 if POI category is the same as that of `startPOI`, -1 otherwise\n",
    "1. `distStart`: distance (haversine formula) from `startPOI`\n",
    "1. `diffPopStart`: difference in POI popularity from `startPOI` (NO LOG as it could be negative)\n",
    "1. `diffNVisitStart`: difference in the total number of visit from `startPOI`\n",
    "1. `diffDurationStart`: difference in average POI visit duration from the actual duration spent at `startPOI`\n",
    "1. `sameNeighbourhoodStart`: 1 if POI resides in the same cluster as that of `startPOI`, -1 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DF_COLUMNS = ['poiID', 'label', 'queryID', 'category', 'neighbourhood', 'popularity', 'nVisit', 'avgDuration', \\\n",
    "              'trajLen', 'sameCatStart', 'distStart', 'diffPopStart', 'diffNVisitStart', 'diffDurationStart', \\\n",
    "              'sameNeighbourhoodStart']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Factorised Transition Probabilities between POIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate a transition matrix for each feature of POI, transition probabilities between different POIs can be computed by taking the Kronecker product of the individual transition matrix corresponding to each feature (with normalisation and a few constraints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 POI Features for Factorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI features used to factorise transition matrix of Markov Chain with POI features (vector) as states:\n",
    "- Category of POI\n",
    "- Popularity of POI (discritize with uniform log-scale bins, #bins <=5 )\n",
    "- The number of POI visits (discritize with uniform log-scale bins, #bins <=5 )\n",
    "- The average visit duration of POI (discritise with uniform log-scale bins, #bins <= 5)\n",
    "- The neighborhood relationship between POIs (clustering POI(lat, lon) using k-means, #clusters <= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the number of transition first, then normalise each row while taking care of zero by adding each cell a number $k=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalise_transmat(transmat_cnt):\n",
    "    transmat = transmat_cnt.copy()\n",
    "    assert(isinstance(transmat, pd.DataFrame))\n",
    "    for row in range(transmat.index.shape[0]):\n",
    "        rowsum = np.sum(transmat.iloc[row] + 1)\n",
    "        assert(rowsum > 0)\n",
    "        transmat.iloc[row] = (transmat.iloc[row] + 1) / rowsum\n",
    "    return transmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POIs in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_train = sorted(poi_info_all.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary to map POIs to $[0, \\dots, M]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POI_DICT = dict()\n",
    "POI_DICT = {poi:pid for pid, poi in enumerate(poi_train)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Transition Matrix between POI Cateogries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_cats = poi_all.loc[poi_train, 'poiCat'].unique().tolist()\n",
    "poi_cats.sort()\n",
    "POI_CAT_LIST = poi_cats\n",
    "POI_CAT_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transmat_cat(trajid_list, traj_dict, poi_info, poi_cats=POI_CAT_LIST):\n",
    "    transmat_cat_cnt = pd.DataFrame(data=np.zeros((len(poi_cats), len(poi_cats)), dtype=np.float), \\\n",
    "                                    columns=poi_cats, index=poi_cats)\n",
    "    for tid in trajid_list:\n",
    "        t = traj_dict[tid]\n",
    "        if len(t) > 1:\n",
    "            for pi in range(len(t)-1):\n",
    "                p1 = t[pi]\n",
    "                p2 = t[pi+1]\n",
    "                assert(p1 in poi_info.index and p2 in poi_info.index)\n",
    "                cat1 = poi_info.loc[p1, 'poiCat']\n",
    "                cat2 = poi_info.loc[p2, 'poiCat']\n",
    "                transmat_cat_cnt.loc[cat1, cat2] += 1\n",
    "    return normalise_transmat(transmat_cat_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gen_transmat_cat(trajid_set_all, traj_dict, poi_info_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Transition Matrix between POI Popularity Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_pops = poi_info_all.loc[poi_train, 'popularity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretize POI popularity with uniform log-scale bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expo_pop1 = np.log10(max(1, min(poi_pops)))\n",
    "expo_pop2 = np.log10(max(poi_pops))\n",
    "#print(expo_pop1, expo_pop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbins_pop = BIN_CLUSTER\n",
    "logbins_pop = np.logspace(np.floor(expo_pop1), np.ceil(expo_pop2), nbins_pop+1)\n",
    "logbins_pop[0] = 0  # deal with underflow\n",
    "if logbins_pop[-1] < poi_info_all['popularity'].max():\n",
    "    logbins_pop[-1] = poi_info_all['popularity'].max() + 1\n",
    "logbins_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ax = pd.Series(poi_pops).hist(figsize=(5, 3), bins=logbins_pop)\n",
    "#ax.set_xlim(xmin=0.1)\n",
    "#ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transmat_pop(trajid_list, traj_dict, poi_info, logbins_pop=logbins_pop):\n",
    "    nbins = len(logbins_pop) - 1\n",
    "    transmat_pop_cnt = pd.DataFrame(data=np.zeros((nbins, nbins), dtype=np.float), \\\n",
    "                                    columns=np.arange(1, nbins+1), index=np.arange(1, nbins+1))\n",
    "    for tid in trajid_list:\n",
    "        t = traj_dict[tid]\n",
    "        if len(t) > 1:\n",
    "            for pi in range(len(t)-1):\n",
    "                p1 = t[pi]\n",
    "                p2 = t[pi+1]\n",
    "                assert(p1 in poi_info.index and p2 in poi_info.index)\n",
    "                pop1 = poi_info.loc[p1, 'popularity']\n",
    "                pop2 = poi_info.loc[p2, 'popularity']\n",
    "                pc1, pc2 = np.digitize([pop1, pop2], logbins_pop)\n",
    "                transmat_pop_cnt.loc[pc1, pc2] += 1\n",
    "    return normalise_transmat(transmat_pop_cnt), logbins_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gen_transmat_pop(trajid_set_all, traj_dict, poi_info_all)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Transition Matrix between the Number of POI Visit Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_visits = poi_info_all.loc[poi_train, 'nVisit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretize the number of POI visit with uniform log-scale bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expo_visit1 = np.log10(max(1, min(poi_visits)))\n",
    "expo_visit2 = np.log10(max(poi_visits))\n",
    "#print(expo_visit1, expo_visit2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbins_visit = BIN_CLUSTER\n",
    "logbins_visit = np.logspace(np.floor(expo_visit1), np.ceil(expo_visit2), nbins_visit+1)\n",
    "logbins_visit[0] = 0  # deal with underflow\n",
    "if logbins_visit[-1] < poi_info_all['nVisit'].max():\n",
    "    logbins_visit[-1] = poi_info_all['nVisit'].max() + 1\n",
    "logbins_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ax = pd.Series(poi_visits).hist(figsize=(5, 3), bins=logbins_visit)\n",
    "#ax.set_xlim(xmin=0.1)\n",
    "#ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transmat_visit(trajid_list, traj_dict, poi_info, logbins_visit=logbins_visit):\n",
    "    nbins = len(logbins_visit) - 1\n",
    "    transmat_visit_cnt = pd.DataFrame(data=np.zeros((nbins, nbins), dtype=np.float), \\\n",
    "                                      columns=np.arange(1, nbins+1), index=np.arange(1, nbins+1))\n",
    "    for tid in trajid_list:\n",
    "        t = traj_dict[tid]\n",
    "        if len(t) > 1:\n",
    "            for pi in range(len(t)-1):\n",
    "                p1 = t[pi]\n",
    "                p2 = t[pi+1]\n",
    "                assert(p1 in poi_info.index and p2 in poi_info.index)\n",
    "                visit1 = poi_info.loc[p1, 'nVisit']\n",
    "                visit2 = poi_info.loc[p2, 'nVisit']\n",
    "                vc1, vc2 = np.digitize([visit1, visit2], logbins_visit)\n",
    "                transmat_visit_cnt.loc[vc1, vc2] += 1\n",
    "    return normalise_transmat(transmat_visit_cnt), logbins_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gen_transmat_visit(trajid_set_all, traj_dict, poi_info_all)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Transition Matrix between POI Average Visit Duration Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poi_durations = poi_info_all.loc[poi_train, 'avgDuration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expo_duration1 = np.log10(max(1, min(poi_durations)))\n",
    "expo_duration2 = np.log10(max(poi_durations))\n",
    "#print(expo_duration1, expo_duration2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbins_duration = BIN_CLUSTER\n",
    "logbins_duration = np.logspace(np.floor(expo_duration1), np.ceil(expo_duration2), nbins_duration+1)\n",
    "logbins_duration[0] = 0  # deal with underflow\n",
    "logbins_duration[-1] = np.power(10, expo_duration2+2)\n",
    "logbins_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ax = pd.Series(poi_durations).hist(figsize=(5, 3), bins=logbins_duration)\n",
    "#ax.set_xlim(xmin=0.1)\n",
    "#ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transmat_duration(trajid_list, traj_dict, poi_info, logbins_duration=logbins_duration):\n",
    "    nbins = len(logbins_duration) - 1\n",
    "    transmat_duration_cnt = pd.DataFrame(data=np.zeros((nbins, nbins), dtype=np.float), \\\n",
    "                                         columns=np.arange(1, nbins+1), index=np.arange(1, nbins+1))\n",
    "    for tid in trajid_list:\n",
    "        t = traj_dict[tid]\n",
    "        if len(t) > 1:\n",
    "            for pi in range(len(t)-1):\n",
    "                p1 = t[pi]\n",
    "                p2 = t[pi+1]\n",
    "                assert(p1 in poi_info.index and p2 in poi_info.index)\n",
    "                d1 = poi_info.loc[p1, 'avgDuration']\n",
    "                d2 = poi_info.loc[p2, 'avgDuration']\n",
    "                dc1, dc2 = np.digitize([d1, d2], logbins_duration)\n",
    "                transmat_duration_cnt.loc[dc1, dc2] += 1\n",
    "    return normalise_transmat(transmat_duration_cnt), logbins_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gen_transmat_duration(trajid_set_all, traj_dict, poi_info_all)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Transition Matrix between POI Neighborhood Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans in scikit-learn seems unable to use custom distance metric and no implementation of [Haversine formula](http://en.wikipedia.org/wiki/Great-circle_distance), use Euclidean distance to approximate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = poi_all.loc[poi_train, ['poiLon', 'poiLat']]\n",
    "nclusters = BIN_CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=nclusters, random_state=987654321)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = kmeans.predict(X)\n",
    "POI_CLUSTER_LIST = sorted(np.unique(clusters))\n",
    "POI_CLUSTERS = pd.DataFrame(data=clusters, index=poi_train)\n",
    "POI_CLUSTERS.index.name = 'poiID'\n",
    "POI_CLUSTERS.rename(columns={0:'clusterID'}, inplace=True)\n",
    "POI_CLUSTERS['clusterID'] = POI_CLUSTERS['clusterID'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plot of POI coordinates with clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#diff = poi_all.loc[poi_train, ['poiLon', 'poiLat']].max() - poi_all.loc[poi_train, ['poiLon', 'poiLat']].min()\n",
    "#ratio = diff['poiLon'] / diff['poiLat']\n",
    "#height = 6; width = int(round(ratio)*height)\n",
    "#plt.figure(figsize=[width, height])\n",
    "#plt.scatter(poi_all.loc[poi_train, 'poiLon'], poi_all.loc[poi_train, 'poiLat'], c=clusters, s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_transmat_neighbor(trajid_list, traj_dict, poi_info, poi_clusters=POI_CLUSTERS):\n",
    "    nclusters = len(poi_clusters['clusterID'].unique())\n",
    "    transmat_neighbor_cnt = pd.DataFrame(data=np.zeros((nclusters, nclusters), dtype=np.float), \\\n",
    "                                         columns=np.arange(nclusters), index=np.arange(nclusters))\n",
    "    for tid in trajid_list:\n",
    "        t = traj_dict[tid]\n",
    "        if len(t) > 1:\n",
    "            for pi in range(len(t)-1):\n",
    "                p1 = t[pi]\n",
    "                p2 = t[pi+1]\n",
    "                assert(p1 in poi_info.index and p2 in poi_info.index)\n",
    "                c1 = poi_clusters.loc[p1, 'clusterID']\n",
    "                c2 = poi_clusters.loc[p2, 'clusterID']\n",
    "                transmat_neighbor_cnt.loc[c1, c2] += 1\n",
    "    return normalise_transmat(transmat_neighbor_cnt), poi_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gen_transmat_neighbor(trajid_set_all, traj_dict, poi_info_all)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_F1(traj_act, traj_rec, noloop=True):\n",
    "    '''Compute recall, precision and F1 for recommended trajectories'''\n",
    "    assert(isinstance(noloop, bool))\n",
    "    assert(len(traj_act) > 0)\n",
    "    assert(len(traj_rec) > 0)\n",
    "    \n",
    "    if noloop == True:\n",
    "        intersize = len(set(traj_act) & set(traj_rec))\n",
    "    else: # if there are sub-tours in both ground truth and prediction\n",
    "        match_tags = np.zeros(len(traj_act), dtype=np.bool)\n",
    "        for poi in traj_rec:\n",
    "            for j in range(len(traj_act)):\n",
    "                if match_tags[j] == False and poi == traj_act[j]:\n",
    "                    match_tags[j] = True\n",
    "                    break\n",
    "        intersize = np.nonzero(match_tags)[0].shape[0]\n",
    "        \n",
    "    recall = intersize / len(traj_act)\n",
    "    precision = intersize / len(traj_rec)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the pairs-F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "cpdef float calc_pairsF1(y, y_hat):\n",
    "    assert(len(y) > 0)\n",
    "    assert(len(y) == len(set(y))) # no loops in y\n",
    "    cdef int n, nr, n0, n0r, nc, poi1, poi2, i, j\n",
    "    n = len(y)\n",
    "    nr = len(y_hat)\n",
    "    n0 = int(n*(n-1) / 2)\n",
    "    n0r = int(nr*(nr-1) / 2)\n",
    "    \n",
    "    # y determines the correct visiting order\n",
    "    order_dict = dict()\n",
    "    for i in range(n):\n",
    "        order_dict[y[i]] = i\n",
    "        \n",
    "    nc = 0\n",
    "    for i in range(nr):\n",
    "        poi1 = y_hat[i]\n",
    "        for j in range(i+1, nr):\n",
    "            poi2 = y_hat[j]\n",
    "            if poi1 in order_dict and poi2 in order_dict and poi1 != poi2:\n",
    "                if order_dict[poi1] < order_dict[poi2]: nc += 1\n",
    "\n",
    "    cdef float precision, recall, F1\n",
    "    precision = (1.0 * nc) / (1.0 * n0r)\n",
    "    recall = (1.0 * nc) / (1.0 * n0)\n",
    "    if nc == 0:\n",
    "        F1 = 0\n",
    "    else:\n",
    "        F1 = 2. * precision * recall / (precision + recall)\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kendall's $\\tau$ as evaluation metric: \n",
    "\n",
    "The ranks of all POIs in trajectory $\\mathbf{y}$ should be greater than all other POIs that do not appear in trajectory $\\mathbf{y}$, which we require that they have the same rank (use rank $0$ by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_rank(y, M, default_rank=0):\n",
    "    \"\"\"\n",
    "    compute the rank of all POIs given a trajectory\n",
    "    y - trajectory: a sequence of POIs without duplication\n",
    "    M - total number of POIs\n",
    "    default_rank - the rank for all POIs do not appear in y\n",
    "    \"\"\"\n",
    "    assert(len(y) > 0)\n",
    "    assert(len(y) <= M)\n",
    "    assert(default_rank >= 0)\n",
    "    assert(default_rank <= M)\n",
    "    rank = np.ones(M) * default_rank\n",
    "    for j in range(len(y)):\n",
    "        poi = y[j]\n",
    "        prank = M - j\n",
    "        rank[poi - 1] = prank\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y1 = [1, 3, 2, 5]\n",
    "#y2 = [1, 2, 5, 3]\n",
    "#M = 10\n",
    "#default = 0\n",
    "#print(gen_rank(y1, M, default))\n",
    "#print(gen_rank(y2, M, default))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the effect of the rank for all POIs not in trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for default in np.arange(0, 10.5, .5):\n",
    "#    assert(len(y1) == len(y2))\n",
    "#    if default >= M - len(y1) + 1: continue\n",
    "#    r1 = gen_rank(y1, M, default)\n",
    "#    r2 = gen_rank(y2, M, default)\n",
    "#    #print(r1, r2)\n",
    "#    print(default, kendalltau(r1, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#default = 7  # incorrect when missing value >= the lowest rank for observed data\n",
    "#r1 = gen_rank(y1, M, default)\n",
    "#r2 = gen_rank(y2, M, default)\n",
    "#print(r1, r2)\n",
    "#print(default, kendalltau(r1, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Kendall's $\\tau$ (taking care of ties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_kendalltau(y, y_hat):\n",
    "    #assert(len(y) == len(y_hat))\n",
    "    M = len(POI_DICT)\n",
    "    assert(len(y) <= M)\n",
    "    assert(len(y_hat) <= M)\n",
    "    \n",
    "    r1 = gen_rank([POI_DICT[p] for p in y], M)\n",
    "    r2 = gen_rank([POI_DICT[p] for p in y_hat], M)\n",
    "    \n",
    "    return kendalltau(r1, r2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calc_kendalltau(y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute all evaluation metrics given a prediction and the set of ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(y_hat, ground_truth_id_set, use_max=True):\n",
    "    \"\"\"\n",
    "    compute all evaluation metrics: \n",
    "    - F1 score on points, \n",
    "    - F1 score on pairs,\n",
    "    - Kendall's tau.\n",
    "    \n",
    "    y_hat - the prediction for query x\n",
    "    ground_truth_id_set - the set of trajecory IDs for query x\n",
    "    M - the total number of POIs\n",
    "    use_max - True: use the maximum of all scores for each metric, False: use the mean\n",
    "    \"\"\"\n",
    "    assert(len(y_hat) > 0)\n",
    "    assert(len(ground_truth_id_set) > 0)\n",
    "    \n",
    "    ground_truth = [traj_dict[x] for x in ground_truth_id_set]\n",
    "    F1 = np.zeros(len(ground_truth), dtype=np.float)\n",
    "    pF1 = np.zeros(len(ground_truth), dtype=np.float)\n",
    "    Tau = np.zeros(len(ground_truth), dtype=np.float)\n",
    "    for j in range(len(ground_truth)):\n",
    "        assert(len(y_hat) == len(ground_truth[j]))\n",
    "        F1[j] = calc_F1(ground_truth[j], y_hat)\n",
    "        pF1[j] = calc_pairsF1(ground_truth[j], y_hat)\n",
    "        Tau[j] = calc_kendalltau(ground_truth[j], y_hat)\n",
    "    if use_max == True:  # use maximum similarity score\n",
    "        return np.max(F1), np.max(pF1), np.max(Tau)\n",
    "    else:                # use mean similarity score\n",
    "        return np.mean(F1), np.mean(pF1), np.mean(Tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to check the equality of two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vars_equal(d1, d2):\n",
    "    \"\"\" Check equality of two variables\"\"\"\n",
    "    \n",
    "    def list_equal(d1, d2):\n",
    "        assert type(d1) == type(d2) == list\n",
    "        assert len(d1) == len(d2)\n",
    "        for j in range(len(d1)): assert vars_equal(d1[j], d2[j])\n",
    "        return True\n",
    "    \n",
    "    def set_equal(d1, d2):\n",
    "        assert type(d1) == type(d2) == set\n",
    "        assert list_equal(sorted(d1), sorted(d2))\n",
    "        return True\n",
    "    \n",
    "    def dict_equal(d1, d2):\n",
    "        assert type(d1) == type(d2) == dict\n",
    "        assert len(d1.keys()) == len(d2.keys())\n",
    "        assert pd.Series(sorted(d1.keys())).equals(pd.Series(sorted(d1.keys())))\n",
    "        for key in d1.keys(): assert vars_equal(d1[key], d2[key])\n",
    "        return True\n",
    "\n",
    "    assert type(d1) == type(d2)\n",
    "    int_types = {int, np.int0, np.int8, np.int16, np.int32, np.int64}\n",
    "    float_types = {float, np.float16, np.float32, np.float64, np.float128}\n",
    "    if type(d1) == str:           assert d1 == d2\n",
    "    elif type(d1) in int_types:   assert d1 == d2\n",
    "    elif type(d1) in float_types: assert np.isclose(d1, d2)  # np.isclose(10, 10.0001) is True\n",
    "    elif type(d1) == list:        assert list_equal(d1, d2)\n",
    "    elif type(d1) == set:         assert set_equal(d1, d2)\n",
    "    elif type(d1) == dict:        assert dict_equal(d1, d2)\n",
    "    elif type(d1) == np.ndarray:  assert np.allclose(d1, d2)\n",
    "    elif type(d1) in {pd.DataFrame, pd.Series}: assert d1.equals(d2)\n",
    "    else: assert False, 'UNrecognised type: %s\\n' % type(d1)\n",
    "    return True"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
