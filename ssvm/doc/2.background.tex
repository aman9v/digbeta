\section{The cutting-plane methods}
\label{sec:problem}

The goal of cutting-plane methods is to find/localise a point in a convex \textit{target set} $Z \in \mathbb{R}^n$,
or determine that $Z$ is empty in some cases. 
The method does not assume any direct access to the description of $Z$,
such as the objective and constraint functions in an optimisation problem, except through a \textit{cutting-plane oracle}.
The method generates a query point $q$ and pass it to the oracle, 
the oracle either tells us that $q \in Z$ (in which case we are done), or it returns a hyperplane which separates $q$ from $Z$.
This hyperplane is called a \textit{cutting-plane}, or \textit{cut}, since it eliminates a half-space from our search.

Cutting-plane methods are also known as \textit{localisation} methods. 
A conceptual description of cutting-plane methods is shown in Algorithm~\ref{alg:cutting-plane}.


\begin{algorithm}[htbp]
\caption{Cutting-plane algorithm}
\label{alg:cutting-plane}
\begin{algorithmic}[1]
\STATE \textbf{Given}: an initial polyhedron $\mathcal{P}_0$ that contains $Z$.
\STATE $k = 0$
\REPEAT
    \STATE Generate a query point $q^{(k+1)}$ in $\mathcal{P}_k$
    \STATE Query the oracle at $q^{(k+1)}$
    \IF{~The oracle determines that $q^{(k+1)} \in Z$~}
        \RETURN $q^{(k+1)}$
    \ELSIF{~The oracle returns a cutting-plane $a_{k+1}^\top z \le b_{k+1}$~}
        \STATE Update constraints: $\mathcal{P}_{k+1} = \mathcal{P}_k \cap \{z | a_{k+1}^\top z \le b_{k+1} \}$
    \ENDIF
    \STATE $k = k + 1$
\UNTIL{Convergence or $\mathcal{P}_{k+1} = \emptyset$}
\end{algorithmic}
\end{algorithm}


\noindent
For a convex optimisation problem with $m$ constraints,

\begin{equation}
\label{eq:cvxprob}
\begin{aligned}
\min_{z} ~& f_0(z)        & \\
s.t.~~   ~& f_i(z) \le 0, & i = 1, \dots, m
\end{aligned} 
\end{equation}
where $f_0, \dots, f_m$ are convex and differentiable, the target set $Z$ is the optimal (or $\varepsilon$-suboptimal) set.

Given a query point $q$, the oracle first checks for feasibility.
If $q$ is not feasible, this means that at least one constraint in problem (\ref{eq:cvxprob}) is violated.
Suppose constraint $f_j(z) \le 0$ is violated by $q$, then we have $f_j(q) > 0$.
In addition, as $f_j(z)$ is convex and differentiable, we have the inequality
\begin{equation}
\label{eq:funprop}
f_j(z) \ge f_j(q) + \nabla f_j(q)^\top (z - q),~ j = 0, \dots, m
\end{equation}
We conclude that if $f_j(q) + \nabla f_j(q)^\top (z - q) > 0$, then $f_j(z) > 0$, 
which violated the constraint $f_j(z) \le 0$ in problem (\ref{eq:cvxprob}).
Thus, any feasible point should satisfy the inequality
\begin{equation}
\label{eq:feacut}
f_j(q) + \nabla f_j(q)^\top (z - q) \le 0.
\end{equation}
This is called a \textit{feasibility cut} for problem (\ref{eq:cvxprob}) since it cuts away the half-space 
$\{z | f_j(q) + \nabla f_j(q)^\top (z - q) > 0 \}$ with infeasible points.
If more than one constraint is violated by $q$, we can generate a \emph{feasibility cut} for each violated constraint.

On the other hand, if $q$ is feasible, and suppose $\nabla f_0(q) \ne 0$ (otherwise $q$ is optimal and we are done),
from Equation (\ref{eq:funprop}) we have
\begin{equation*}
f_0(z) > f_0(q), \text{~if~} \nabla f_0(q)^\top (z - q) > 0.
\end{equation*}
In other words, any point that satisfies inequality $\nabla f_0(q)^\top (z - q) > 0$ has an objective value larger than $f_0(q)$ 
and hence cannot be optimal.
It follows that we can form a cutting-plane
\begin{equation}
\label{eq:objcut}
\nabla f_0(q)^\top (z - q) \le 0,
\end{equation}
which is called an \textit{objective cut} for problem (\ref{eq:cvxprob}) and 
it cuts out the half-space $\{z | \nabla f_0(q)^\top (z - q) > 0 \}$ with non-optimal points.

If we keep track of the best (smallest) objective value for all feasible query points during the querying, i.e.,
$f_\text{best} = f_0(q_\text{best}) = \min\{f_0(q^{(t)}) \mid q^{(t)} ~\text{is feasible}\}$,
since the optimal point has an objective value at most $f_\text{best}$, 
we can cut away the half-space of points $\{z | f_0(z) > f_\text{best} \}$ with objective values greater than $f_\text{best}$, 
which means we add a constraint $f_0(z) \le f_\text{best}$, and from Equation~(\ref{eq:funprop}), 
we have a deep objective cut~\cite{boydlocalization},
\begin{equation}
\label{eq:deepobjcut}
f_0(q) + \nabla f_0(q)^\top (z - q) - f_\text{best} \le 0,
\end{equation}
where $q$ is the current query point and is feasible. If $q = q_\text{best}$, this cut reduces to the objective cut~(\ref{eq:objcut}).

%If $q$ is feasible and $\nabla f_0(q) = 0$ then $q$ is optimal.
For non-differentiable problems, the gradients $\nabla f_j(z)$ can generally be replaced by sub-gradients.


\subsection{Generate query points}
\label{sec:query}

We would like to generate a query point $q^{(k+1)}$ in the current polyhedron $\mathcal{P}_{k}$ such that 
the resulting cut reduces the size of $\mathcal{P}_{k+1}$ as much as possible.
However, when we query the oracle at point $q^{(k+1)}$, we do not know in which direction the generated cut will be excluded.
If we measure the informativeness of the $k$-th cut using the volume reduction ratio $\frac{V(\mathcal{P}_{k+1})}{V(\mathcal{P}_{k})}$,
we seek a point $q^{(k+1)}$ such that, no matter which direction to cut (returned by the oracle), we can obtain a certain guaranteed volume reduction.


\subsubsection{Method of Kelley-Cheney-Goldstein}
\label{sec:kcg}

Given query points $q^{(1)}, \dots, q^{(k)}$, one approach to generate the next query point $q^{(k+1)}$ is solving a linear programming (LP)
~\cite{wulff2013analytic},
\begin{equation}
\label{eq:kcg}
\begin{aligned}
\min_{z,\theta} ~& \theta  \\
s.t.~~   ~& \theta \ge f_0(q^{(i)}) + \nabla f_0(q^{(i)})^\top (z - q^{(i)}),~ \forall i \le k \\
          & A_k^\top z \le \mathbf{b}_k,
\end{aligned}
\end{equation}
where $A_k^\top z \le \mathbf{b}_k$ are the set of constraints that define polyhedron $\mathcal{P}_k$.

Let $t_i(z) = f_0(q^{(i)}) + \nabla f_0(q^{(i)})^\top (z - q^{(i)})$,
then $t_i(z)$ is a hyperplane tangent to $f_0(z)$ at point $q^{(i)}$.
We can rewrite LP (\ref{eq:kcg}) as
\begin{equation*}
\begin{aligned}
\min_{z,\theta} ~& \theta \\
s.t.~~ ~& \theta \ge \max_{z \in \mathcal{P}_k}~ t_i(z),~ i=1,\dots,k.
\end{aligned}
\end{equation*}

Let $z_i^* = \argmax_{z \in \mathcal{P}_k} t_i(z)$,
it follows that $z_i^*$ is either a vertex or a point lies on an edge of polyhedron $\mathcal{P}_k$
(this can be shown intuitively when $\mathcal{P}_k$ is a $2$-dimensional region),
and the optimal solution of LP (\ref{eq:kcg}) is $z^* = \argmax_{z_i} t_i(z_i^*)$,
it follows that the next query point $q^{(k+1)} = z^*$ is either a vertex or a point lies on an edge of $\mathcal{P}_k$.
In fact, if we solve LP (\ref{eq:kcg}) using the simplex algorithm, the optimal solution is guaranteed to be a vertex of $\mathcal{P}_k$.

In other words,
the method of \emph{Kelley-Cheney-Goldstein} is to greedily use the vertex of the current polyhedron $\mathcal{P}_k$ 
that maximise the convex objective $f_0(z)$ as the next query point.



\subsubsection{Chebyshev center method}
\label{sec:chebyshev}

If we rescale the gradients $\nabla f_0(q^{(i)})$ to unit length in problem (\ref{eq:kcg}), 
it results in finding the center of the largest Euclidean ball that lies inside the current polyhedron $\mathcal{P}_k$~\cite{elzinga1975central},
in other words, we find the next query point $q^{(k+1)}$ by solving
\begin{equation}
\label{eq:chebyshev}
\begin{aligned}
\min_{z} ~& \theta  \\
s.t.~~   ~& \theta \ge f_0(q^{(i)}) + \frac{\nabla f_0(q^{(i)})}{\|\nabla f_0(q^{(i)})\|} ^\top (z - q^{(i)}),~ \forall i \le k \\
          & A_k^\top z \le \mathbf{b}_k.
\end{aligned}
\end{equation}

This variant is called the \emph{Chebyshev center} method, which is shown to have significantly better convergence properties than the method of Kelley-Cheney-Goldstein~\cite{goffin2002convex}.


\subsubsection{Analytic center cutting plane method}
\label{sec:accpm}

Given a linear constraint $a_i^\top z \le b_i$, we define a slack variable $s_i \in \mathbb{R}$ as $s_i = b_i - a_i^\top z$,
that is, $s_i$ measures how far the current solution is from the constraint.
The analytic center is defined as the unique maximiser of the function~\cite{wulff2013analytic}
\begin{equation}
\label{eq:accpm}
\argmax_z \prod_i s_i = \argmax_z ~ \sum_{i=1}^k \log(b_i - a_i^\top z) + \sum_{j=1}^m \log(d_j - c_j^\top z),
\end{equation}
where we assume constraints $f_j(z) \le 0$ in problem (\ref{eq:cvxprob}) are linear and rewrite them as $c_j^\top z \le d_j$.
The unique maximiser of (\ref{eq:accpm}) can be efficiently found using Newton iterations~\cite{goffin2002convex}.

The analytic center cutting plane method (ACCPM) chooses the analytic center of polyhedron 
\begin{equation*}
\mathcal{P}_k = \{ z | c_j^\top z \le d_j, ~ j=1, \dots, m \text{~and~} a_i^\top z \le b_i, ~ i=1, \dots, k \}
\end{equation*}
to query the oracle.
ACCPM seems to give a good trade-off in terms of simplicity and practical performance~\cite{boydlocalization}.


\subsubsection{Center of gravity/Bayes point method}
\label{sec:cg}

Assume set $\mathcal{C} \subseteq \mathbb{R}^n$ is bounded and has nonempty interior. 
The center of gravity of $\mathcal{C}$ is defined as
\begin{equation}
\textbf{cg}(\mathcal{C}) = \frac{\int_\mathcal{C} z dz}{\int_\mathcal{C} dz}.
\end{equation}

The center of gravity (CG) method chooses the point $q^{(k+1)} = \textbf{cg}(\mathcal{P}_{k})$ to query the oracle~\cite{louche2015cutting}.
It turns out that this method has a very good convergence property in terms of the worst-case volume reduction factor,
in particular, we always have
\begin{equation}
\frac{V(\mathcal{P}_{k+1})}{V(\mathcal{P}_{k})} \le 1 - \frac{1}{e} \approx 0.63,
\end{equation}
in other words, the volume of the localisation polyhedron is reduced by at least $37\%$ at each iteration,
and this guarantee is completely independent of all problem parameters, including the dimension $n$.
However, it is \textit{extremely difficult} to compute the center of gravity of a polyhedron in $\mathbb{R}^n$, described by a set of linear inequalities,
which makes this method impractical.
Variants that compute an approximate center of gravity have been developed, and some of these approximations can be used to create a practical CG method~\cite{boydlocalization}.


\section{Training structured SVM using cutting-plane methods}
\label{sec:ssvm_train}


\subsection{Training the $n$-slack formulation of structured SVM}
\label{sec:nslackssvm}

Given $n$ training examples $(\mathbf{x}_1, \mathbf{y}_1), \dots, (\mathbf{x}_n, \mathbf{y}_n)$, 
the structured SVM with margin-rescaling\footnote{For brevity, structured SVM (SSVM) with slack-rescaling are not described in this document.}
can be formulated as a quadratic program (QP)
\begin{equation}
\label{eq:nslackform}
\begin{aligned}
\min_{\mathbf{w}, ~\bm{\xi} \ge 0} ~& \frac{1}{2} \mathbf{w}^\top \mathbf{w} + \frac{C}{n} \sum_{i=1}^n \xi_i \\
s.t.~~ ~& \mathbf{w}^\top \Psi(\mathbf{x}_i, \mathbf{y}_i) - \mathbf{w}^\top \Psi(\mathbf{x}_i, \bar{\mathbf{y}}) \ge 
       \Delta(\mathbf{y}_i, \bar{\mathbf{y}}) - \xi_i, ~(\forall i,~ \bar{\mathbf{y}} \neq \mathbf{y}_i)
\end{aligned}
\end{equation}
where $\mathbf{w}$ is the parameter vector, $C > 0$ is a regularisation constant,
$\Delta(\mathbf{y}, \bar{\mathbf{y}})$ is a discrepancy function that measures the loss 
for predicting $\bar{\mathbf{y}}$ given ground truth $\mathbf{y}$.
and $\xi_i$ is a slack variable that represents the \emph{hinge loss} associated with 
the prediction for the $i$-th example~\cite{tsochantaridis2005large},
\begin{equation}
\label{eq:nslackloss}
\xi_i = \max \left( 0,~ 
        \max_{\bar{\mathbf{y}} \in \mathcal{Y}} 
        \left\{ \Delta(\mathbf{y}_i, \bar{\mathbf{y}}) + \mathbf{w}^\top \Psi(\mathbf{x}_i, \bar{\mathbf{y}}) \right\} -
        \mathbf{w}^\top \Psi(\mathbf{x}_i, \mathbf{y}_i) \right).
\end{equation}
This formulation is called "$n$-slack" as we have one slack variable for each example in training set. \eat{citation}

To train the $n$-slack formulation of structured SVM, one option is simply enumerating all constraints and 
solve optimisation problem (\ref{eq:nslackform}) using a standard QP solver, 
however, this approach is impractical as there is a constraint for every incorrect label $\bar{\mathbf{y}}$.
Instead, we use a cutting-plane algorithm that repeatedly solves QP (\ref{eq:nslackform}) with respect to different set of constraints, 
and each iteration generates a new constraint that helps reduce the feasible region of the problem, 
until a specified precision $\varepsilon$ is achieved~\cite{joachims2009predicting}, as described in Algorithm~\ref{alg:nslacktrain}.

\begin{algorithm}[htbp]
\caption{Cutting-plane algorithm for training $n$-slack formulation of structured SVM (with margin-rescaling)}
\label{alg:nslacktrain}
\begin{algorithmic}[1]
\STATE \textbf{Input}: $\left( (\mathbf{x}_1, \mathbf{y}_1), \dots, (\mathbf{x}_n, \mathbf{y}_n) \right),~ C,~ \varepsilon$
\STATE $\mathcal{W} = \emptyset,~\mathcal{S}_i = \emptyset,~ k = 1,~ \mathbf{w}^{(k)} = \mathbf{0},~ \bm{\xi}^{(k)} = \mathbf{0}$
\REPEAT
    \FOR{$i = 1,\dots,n$}
        \STATE $\triangleright$ Query the oracle at point $q^{(k)} = (\mathbf{w}^{(k)}, \bm{\xi}^{(k)})$ as follows
        \STATE Do loss-augmented inference:~
               $\hat{\mathbf{y}}^{(k)} = \argmax_{\bar{\mathbf{y}} \in \mathcal{Y}} \{ \Delta(\mathbf{y}_i, \bar{\mathbf{y}}) + 
                \langle \mathbf{w}^{(k)},~ \Psi(\mathbf{x}_i, \bar{\mathbf{y}}) \rangle \}$ 
        \IF{~$q^{(k)}$ is not $\varepsilon$-feasible:~ 
             $\langle \mathbf{w}^{(k)},~ \Psi(\mathbf{x}_i, \mathbf{y}_i) - \Psi(\mathbf{x}_i, \hat{\mathbf{y}}^{(k)}) \rangle + 
             \varepsilon < \Delta(\mathbf{y}_i, \hat{\mathbf{y}}^{(k)}) - \xi_i^{(k)}$~}
            \STATE $\triangleright$ Form a \emph{feasibility cut} and update constraints
            \STATE $\mathcal{W} = \mathcal{W} \cup 
                    \left\{ \langle \mathbf{w},~ \Psi(\mathbf{x}_i, \mathbf{y}_i) - \Psi(\mathbf{x}_i, \hat{\mathbf{y}}^{(k)}) \rangle \ge 
                    \Delta(\mathbf{y}_i, \hat{\mathbf{y}}^{(k)}) - \xi_i \right\},~ \mathcal{S}_i = \mathcal{S}_i \cup \{\hat{\mathbf{y}}^{(k)} \}$ 
            \STATE Generate the next query point $q^{(k+1)} = (\mathbf{w}^{(k+1)}, \bm{\xi}^{(k+1)})$ 
                   by solving QP~(\ref{eq:nslackform}) w.r.t. all constraints in $\mathcal{W}$
            \STATE $k = k+1$
        \ENDIF
    \ENDFOR
%\UNTIL{$\mathcal{W}$ has not changed during iteration}
\UNTIL{$q^{(k)}$ is $\varepsilon$-feasible for all training examples}
\RETURN $q^{(k)}$
\end{algorithmic}
\end{algorithm}

Alternatively, the query point in Algorithm~\ref{alg:nslacktrain} will contain only $\mathbf{w}^{(k)}$
if we compute the loss $\xi_i^{(k)}$ on the fly~\cite{tsochantaridis2004support}
\begin{equation*}
\xi_i^{(k)} = \max \left( 0,~ 
              \max_{\bar{\mathbf{y}} \in \mathcal{S}_i} 
              \left\{ \Delta(\mathbf{y}_i, \bar{\mathbf{y}}) + \langle \mathbf{w}^{(k)}, \Psi(\mathbf{x}_i, \bar{\mathbf{y}}) \rangle \right\} -
              \langle \mathbf{w}^{(k)}, \Psi(\mathbf{x}_i, \mathbf{y}_i) \rangle \right).
\end{equation*}


\subsection{Training the $1$-slack formulation of structured SVM}
\label{sec:1slackssvm}

Another formulation of structured SVM which results in more efficient training is called "$1$-slack" formulation (with margin-rescaling),
it replaces the $n$ cutting-plane models of the hinge loss (one for each training example) with a single cutting-plane model for 
the sum of the hinge-losses~\cite{joachims2009cutting}, as a result, only one slack variable is needed,
\begin{equation}
\label{eq:1slackform}
\begin{aligned}
\min_{\mathbf{w}, ~\xi \ge 0} ~& \frac{1}{2} \mathbf{w}^\top \mathbf{w} + C \xi \\
s.t.~~ ~& \forall(\bar{\mathbf{y}}_1, \dots, \bar{\mathbf{y}}_n) \in \mathcal{Y}^n: 
          \frac{1}{n} \sum_{i=1}^n 
          \left( \mathbf{w}^\top \Psi(\mathbf{x}_i, \mathbf{y}_i) - \mathbf{w}^\top \Psi(\mathbf{x}_i, \bar{\mathbf{y}}_i) \right) \ge
          \frac{1}{n} \sum_{i=1}^n \Delta(\mathbf{y}_i, \bar{\mathbf{y}}_i) - \xi.
\end{aligned}
\end{equation}
Here the slack variable $\xi$ represents the \emph{sum of the hinge-losses} over all training examples,
\begin{equation}
\label{eq:1slackloss}
\xi = \max \left( 0,~ 
      \max_{(\bar{\mathbf{y}}_1, \dots, \bar{\mathbf{y}}_n) \in \mathcal{Y}^n} 
      \left\{ 
      \frac{1}{n} \sum_{i=1}^n \left( \Delta(\mathbf{y}_i, \bar{\mathbf{y}}_i) + \mathbf{w}^\top \Psi(\mathbf{x}_i, \bar{\mathbf{y}}_i) \right)
      \right\} - \frac{1}{n} \sum_{i=1}^n \mathbf{w}^\top \Psi(\mathbf{x}_i, \mathbf{y}_i)
      \right).
\end{equation}


Compared with the $n$-slack formulation described in Section~\ref{sec:nslackssvm}, 
the $1$-slack formulation of structured SVM increases the number of constraints exponentially~\cite{joachims2009cutting},
which means enumerating all constraints is also impractical.
Algorithm~\ref{alg:1slacktrain} described an approach similar to Algorithm~\ref{alg:nslacktrain} that uses a cutting-plane method to 
train the $1$-slack formulation of structured SVM.


\begin{algorithm}[htbp]
\caption{Cutting-plane algorithm for training $1$-slack formulation of structured SVM (with margin-rescaling)}
\label{alg:1slacktrain}
\begin{algorithmic}[1]
\STATE \textbf{Input}: $S = \left( (\mathbf{x}_1, \mathbf{y}_1), \dots, (\mathbf{x}_n, \mathbf{y}_n) \right),~ C,~ \varepsilon$
\STATE $\mathcal{W} = \emptyset$
%\REPEAT
\FOR{$k = 1,\dots,+\infty$}
    \STATE Generate query point $q^{(k)} = (\mathbf{w}^{(k)}, \xi^{(k)})$ by solving QP~(\ref{eq:1slackform}) w.r.t. all constraints in $\mathcal{W}$
    \STATE $\triangleright$ Query the oracle at point $q^{(k)}$ as follows
    \STATE Do loss-augmented inference:~
           $\hat{\mathbf{y}}_i^{(k)} = \argmax_{\bar{\mathbf{y}} \in \mathcal{Y}} \left\{ \Delta(\mathbf{y}_i, \bar{\mathbf{y}}) + 
            \langle \mathbf{w}^{(k)},~ \Psi(\mathbf{x}_i, \bar{\mathbf{y}}) \rangle \right\},~ \forall i$
    \IF{~$q^{(k)}$ is $\varepsilon$-feasible:~ $\frac{1}{n} \sum_{i=1}^n 
         \langle \mathbf{w}^{(k)},~ \Psi(\mathbf{x}_i, \mathbf{y}_i) - \Psi(\mathbf{x}_i, \hat{\mathbf{y}}_i^{(k)}) \rangle + \varepsilon \ge 
         \frac{1}{n} \sum_{i=1}^n \Delta(\mathbf{y}_i, \hat{\mathbf{y}}_i^{(k)}) - \xi^{(k)}$~}
        \RETURN $q^{(k)}$
    \ELSE
        \STATE Form a \emph{feasibility cut} and update constraints:~
               $\mathcal{W} = \mathcal{W} \cup \left\{ 
                \frac{1}{n} \sum_{i=1}^n \langle \mathbf{w},~ \Psi(\mathbf{x}_i, \mathbf{y}_i) - \Psi(\mathbf{x}_i, \hat{\mathbf{y}}_i^{(k)}) \rangle \ge 
                \frac{1}{n} \sum_{i=1}^n \Delta(\mathbf{y}_i, \hat{\mathbf{y}}_i^{(k)}) - \xi \right\}$
    \ENDIF
%\UNTIL{$\frac{1}{n} \sum_{i=1}^n 
%        \left( \mathbf{w}^\top \Psi(\mathbf{x}_i, \mathbf{y}_i) - \mathbf{w}^\top \Psi(\mathbf{x}_i, \hat{\mathbf{y}}_i) \right) + 
%        \varepsilon \ge \frac{1}{n} \sum_{i=1}^n \Delta(\mathbf{y}_i, \hat{\mathbf{y}}_i) - \xi$}
%\RETURN $(\mathbf{w}, \xi)$
\ENDFOR
\end{algorithmic}
\end{algorithm}


\section{Discussion}
\label{sec:ssvm_discussion}

From Algorithm~\ref{alg:nslacktrain} and Algorithm~\ref{alg:1slacktrain}, we observe that:
\begin{itemize}
\item To generate a query point $q$, it solves a QP with the same objective as the original optimisation problem and
      all constraints/cuts returned by previous queries. 
\item The Wolfe-dual programs of both QP (\ref{eq:nslackform}) and QP (\ref{eq:1slackform}) are QPs~\cite{tsochantaridis2005large,joachims2009cutting}.
\item All cutting-planes returned by the oracle are \emph{feasibility cuts}.
\item The training algorithm will \emph{stop} if the current query point $q$ is feasible, 
      in other words, it does not explicitly form an \emph{objective cut} when $q$ is feasible,
      which is reasonable as the algorithms optimise the objective when generating each query point.
\end{itemize}



\subsection{Query generation method}
\label{sec:ssvm_query}

Recall that in Section~\ref{sec:problem}, we have an objective $f_0(z)$ to minimise, in the case of $1$-slack formulation of structured SVM,
$f_0(z)$ is the quadratic objective in Equation~(\ref{eq:1slackform}), 
\begin{equation}
\label{eq:optobj}
f_0(z) = \frac{1}{2} \mathbf{w}^\top \mathbf{w} + C\xi,
\end{equation}
where $z = [\mathbf{w}, \xi]^\top$.
For the $n$-slack formulation, 
\begin{equation}
\begin{aligned}
f_0(z) = \frac{1}{2} \mathbf{w}^\top \mathbf{w} + \frac{C}{n} \sum_{i=1}^n \xi_i.
\end{aligned}
\end{equation}

The query point generation of both formulation of SSVM can be written as
\begin{equation*}
\begin{aligned}
\min_{z} ~& f_0(z) \\
s.t.~~ ~& A_k^\top z \le \mathbf{b}_k,
\end{aligned}
\end{equation*}
where $A_k^\top z \le \mathbf{b}_k$ is equivalent to the set of constraints in $\mathcal{W}$.



\subsection{Explicit objective cut generation}
\label{sec:ssvm_objcut}

Given query point $q = \left[ \mathbf{w}^{(k)}, \xi^{(k)} \right]^\top$, if $q$ is feasible, we can form an \emph{objective cut}
\begin{equation}
\label{eq:objcut_1slack}
\begin{aligned}
 & \nabla f_0(q)^\top (z - q) \\
=& \left[ \left[ \left.\frac{\partial f_0}{\partial \mathbf{w}}\right|_{\mathbf{w} = \mathbf{w}^{(k)}}, 
                 \left.\frac{\partial f_0}{\partial \xi}\right|_{\xi = \xi^{(k)}} \right]^\top \right]^\top 
   \left( \left[ \mathbf{w}, \xi \right]^\top - \left[ \mathbf{w}^{(k)}, \xi^{(k)} \right]^\top \right)  \\
=& \left[ \mathbf{w}^{(k)}, C \right] \left[ \mathbf{w} - \mathbf{w}^{(k)},~ \xi - \xi^{(k)} \right]^\top  \\
=& \langle \mathbf{w}^{(k)},~ \mathbf{w} - \mathbf{w}^{(k)} \rangle + C (\xi - \xi^{(k)}) \le 0.
\end{aligned}
\end{equation}

We have a similar \emph{objective cut} for the $n$-slack formulation of structured SVM
\begin{equation}
\label{eq:objcut_nslack}
\langle \mathbf{w}^{(k)}, \mathbf{w} - \mathbf{w}^{(k)} \rangle + \frac{C}{n} \sum_{i=1}^n (\xi_i - \xi_i^{(k)}) \le 0.
\end{equation}


\eat{
\subsubsection{Feasibility cut}
\label{sec:ssvm_feacut}

On the other hand, if $q = (\mathbf{w}^{(k)}, \xi^{(k)})$ is not feasible, the following constraint must be violated by $q$,
\begin{equation}
\label{eq:cut_1slackssvm}
\frac{1}{n} \sum_{i=1}^n \langle \mathbf{w},~ \Psi(\mathbf{x}_i, \mathbf{y}_i) - \Psi(\mathbf{x}_i, \hat{\mathbf{y}}_i^{(k)}) \rangle \ge 
\frac{1}{n} \sum_{i=1}^n \Delta(\mathbf{y}_i, \hat{\mathbf{y}}_i^{(k)}) - \xi.
\end{equation}
Let 
\begin{equation}
\label{eq:constraint_k}
f_k(z) = \frac{1}{n} \sum_{i=1}^n \Delta(\mathbf{y}_i, \hat{\mathbf{y}}_i^{(k)}) - 
         \frac{1}{n} \sum_{i=1}^n \langle \mathbf{w},~ \Psi(\mathbf{x}_i, \mathbf{y}_i) - \Psi(\mathbf{x}_i, \hat{\mathbf{y}}_i^{(k)}) \rangle - \xi,
\end{equation}
where $z = [\mathbf{w}, \xi]^\top$.
We can rewrite constraint (\ref{eq:cut_1slackssvm}) as $f_k(z) \le 0$.
Since $q$ violates this constraint, we can construct a \emph{feasibility cut}
\begin{equation}
\label{eq:feacut_1slack}
\begin{aligned}
 & f_k(q) + \nabla f_k(q)^\top (z - q) \\
=& f_k(q) + 
   \left[ \left[ \left.\frac{\partial f_k}{\partial \mathbf{w}}\right|_{\mathbf{w} = \mathbf{w}^{(k)}}, 
                 \left.\frac{\partial f_k}{\partial \xi}\right|_{\xi = \xi^{(k)}} \right]^\top \right]^\top 
   \left( \left[ \mathbf{w}, \xi \right]^\top - \left[ \mathbf{w}^{(k)}, \xi^{(k)} \right]^\top \right)  \\
=& f_k(q) + \left[ -\frac{1}{n} \sum_{i=1}^n \left( \Psi(\mathbf{x}_i, \mathbf{y}_i) - \Psi(\mathbf{x}_i, \hat{\mathbf{y}}_i^{(k)}) \right),  -1 \right] 
   \left[ \mathbf{w} - \mathbf{w}^{(k)},~ \xi - \xi^{(k)} \right]^\top  \\
=& \frac{1}{n} \sum_{i=1}^n \Delta(\mathbf{y}_i, \hat{\mathbf{y}}_i^{(k)}) - 
   \frac{1}{n} \sum_{i=1}^n \langle \mathbf{w}^{(k)},~ \Psi(\mathbf{x}_i, \mathbf{y}_i) - \Psi(\mathbf{x}_i, \hat{\mathbf{y}}_i^{(k)}) \rangle - 
   \xi^{(k)} + \langle -\frac{1}{n} \sum_{i=1}^n \left( \Psi(\mathbf{x}_i, \mathbf{y}_i) - \Psi(\mathbf{x}_i, \hat{\mathbf{y}}_i^{(k)}) \right),~
   \mathbf{w} - \mathbf{w}^{(k)} \rangle - \left( \xi - \xi^{(k)} \right)  \\
=& \frac{1}{n} \sum_{i=1}^n \Delta(\mathbf{y}_i, \hat{\mathbf{y}}_i^{(k)}) - 
   \frac{1}{n} \sum_{i=1}^n \langle \mathbf{w},~ \Psi(\mathbf{x}_i, \mathbf{y}_i) - \Psi(\mathbf{x}_i, \hat{\mathbf{y}}_i^{(k)}) \rangle - \xi \le 0.
\end{aligned}
\end{equation}

We found that inequalities (\ref{eq:cut_1slackssvm}) and (\ref{eq:feacut_1slack}) are identical.
This is \emph{not unexpected} as the hyperplane tangent to $f_k(z)$ (also a hyperplane) at point $q$ is \emph{identical} to hyperplane $f_k(z)$
(assuming the same domain for $z$). 

Similarly, for the $n$-slack formulation of structured SVM, 
suppose a constraint related to example $(\mathbf{x}_j, \mathbf{y}_j)$ is violated by query point $q$, 
as described in Algorithm~\ref{alg:nslacktrain}, the feasibility cut becomes
\begin{equation}
\label{eq:feacut_nslack}
g_k(z) = \Delta(\mathbf{y}_j, \hat{\mathbf{y}}^{(k)}) - 
\langle \mathbf{w},~ \Psi(\mathbf{x}_j, \mathbf{y}_j) - \Psi(\mathbf{x}_j, \hat{\mathbf{y}}^{(k)}) \rangle - \xi_j \le 0.
\end{equation}
}


\subsection{Generate query point using the method of Kelley-Cheney-Goldstein and the Chebyshev center method}
\label{sec:compare}

Suppose we use the method of Kelley-Cheney-Goldstein or the Chebyshev center method to generate query point 
when training the $1$-slack/$n$-slack formulation of structured SVM,
we can compare them with query point generation methods used in Algorithm~\ref{alg:nslacktrain} and Algorithm~\ref{alg:1slacktrain}.

Let 
\begin{align}
f_k(z) &= \frac{1}{n} \sum_{i=1}^n \Delta(\mathbf{y}_i, \hat{\mathbf{y}}_i^{(k)}) - 
          \frac{1}{n} \sum_{i=1}^n \langle \mathbf{w},~ \Psi(\mathbf{x}_i, \mathbf{y}_i) - \Psi(\mathbf{x}_i, \hat{\mathbf{y}}_i^{(k)}) \rangle - \xi
          \label{eq:constraint_k} \\
g_k(z) &= \Delta(\mathbf{y}_j, \hat{\mathbf{y}}^{(k)}) - 
          \langle \mathbf{w},~ \Psi(\mathbf{x}_j, \mathbf{y}_j) - \Psi(\mathbf{x}_j, \hat{\mathbf{y}}^{(k)}) \rangle - \xi_j \le 0 
          \label{eq:feacut_nslack}
\end{align}



\subsubsection{The $1$-slack formulation}
\label{sec:compare_1slack}

Given query points $q^{(1)}, \dots, q^{(k)}$ and the feasibility cuts returned by oracle (after querying these points), then
\begin{align*}
 & f_0(q^{(k)}) + \nabla f_0(q^{(k)})^\top (z - q^{(k)}) \\
=& \frac{1}{2} \langle \mathbf{w}^{(k)},~ \mathbf{w}^{(k)} \rangle + C\xi^{(k)} + 
   \left[ \mathbf{w}^{(k)}, C \right] \left[ \mathbf{w} - \mathbf{w}^{(k)},~ \xi - \xi^{(k)} \right]^\top  \\
=& \langle \mathbf{w}^{(k)}, \mathbf{w} \rangle - \frac{1}{2} \langle \mathbf{w}^{(k)}, \mathbf{w}^{(k)} \rangle + C\xi,
\end{align*}
where $q^{(k)} = (\mathbf{w}^{(k)}, \xi^{(k)})$ and $f_0(\cdot)$ is defined in Equation~(\ref{eq:optobj}).

If we use the method of \emph{Kelley-Cheney-Goldstein} (Section~\ref{sec:kcg}) to generate the next query point $q^{(k+1)}$,
we need to solve the following optimisation problem,
\begin{equation}
\label{eq:1slack_kcg}
\begin{aligned}
\min_{z} ~& \theta \\
s.t.~~ ~& \theta \ge \langle \mathbf{w}^{(k)}, \mathbf{w} \rangle - \frac{1}{2} \langle \mathbf{w}^{(k)}, \mathbf{w}^{(k)} \rangle + C\xi,~ \forall k \\
        & f_k(z) \le 0,~ \forall k \\
        & -\xi \le 0,
\end{aligned}
\end{equation}
where $z = [\mathbf{w}, \xi]^\top$ and $f_k(z)$ is defined in Equation~(\ref{eq:constraint_k}).


We need to solve a similar problem if we use the \emph{Chebyshev center} method (Section~\ref{sec:chebyshev}) to generate the next query point,
\begin{equation}
\label{eq:1slack_chebyshev}
\begin{aligned}
\min_{z} ~& \theta \\
s.t.~~ ~& \theta \ge 
          \frac{1}{D_k} \langle \mathbf{w}^{(k)}, \mathbf{w} \rangle + 
          (\frac{1}{2} - \frac{1}{D_k}) \langle \mathbf{w}^{(k)}, \mathbf{w}^{(k)} \rangle + 
          \frac{C}{D_k}\xi + C (1 - \frac{1}{D_k}) \xi^{(k)},~ \forall k \\
        & f_k(z) \le 0,~ \forall k \\
        & -\xi \le 0,
\end{aligned}
\end{equation}
where $D_k = \|\nabla f_0(q^{(k)})\| = \sqrt{\langle \mathbf{w}^{(k)}, \mathbf{w}^{(k)} \rangle + C^2}$ is a normalisation constant.


The method to generate the next query point used in Algorithm~\ref{alg:1slacktrain} can be rewritten as
\begin{equation}
\label{eq:1slack_query}
\begin{aligned}
\min_{z} ~& \frac{1}{2} \mathbf{w}^\top \mathbf{w} + C \xi \\
s.t.~~ ~& f_k(z) \le 0,~ \forall k \\
        & -\xi \le 0.
\end{aligned}
\end{equation}


Since $f_k(z)$ is a linear function, we know that both problem (\ref{eq:1slack_kcg}) and (\ref{eq:1slack_chebyshev}) are linear programs (LP),
and problem (\ref{eq:1slack_query}) is a quadratic program (QP). 
%We can see the difference clearly if we rewrite the objective of problem (\ref{eq:1slack_query}) to $\min_{z}\theta$ and add a new constraint
%$\theta \ge \frac{1}{2} \mathbf{w}^\top \mathbf{w} + C \xi$.
We can see the difference clearly from the \emph{epigraph form} of problem (\ref{eq:1slack_query}) 
\begin{align*}
\min_{z} ~& \theta \\
s.t.~~ ~& \theta \ge \frac{1}{2} \mathbf{w}^\top \mathbf{w} + C \xi \\
        & f_k(z) \le 0,~ \forall k \\
        & -\xi \le 0.
\end{align*}


\subsubsection{The $n$-slack formulation}
\label{sec:compare_nslack}

Similarly, for the $n$-slack formulation of structured SVM,
if we use the method of \emph{Kelley-Cheney-Goldstein} (Section~\ref{sec:kcg}) to generate the next query point $q^{(k+1)}$,
we need to solve an optimisation problem,
\begin{equation}
\label{eq:nslack_kcg}
\begin{aligned}
\min_{z} ~& \theta \\
s.t.~~ ~& \theta \ge \langle \mathbf{w}^{(k)}, \mathbf{w} \rangle - \frac{1}{2} \langle \mathbf{w}^{(k)}, \mathbf{w}^{(k)} \rangle + 
\frac{C}{n} \langle \mathbf{1}, \bm{\xi} \rangle,~ \forall k \\
        & g_k(z) \le 0,~ \forall k \\
        & -\xi_i \le 0,~ i = 1, \dots, n
\end{aligned}
\end{equation}
where $z = [\mathbf{w}, \bm{\xi}]^\top$, $\mathbf{1}$ is a $n$ dimensional vector of all $1$'s,
and $g_k(z)$ is defined in Equation~(\ref{eq:feacut_nslack}).


We need to solve a similar problem if we use the \emph{Chebyshev center} method (Section~\ref{sec:chebyshev}) to generate the next query point,
\begin{equation}
\label{eq:nslack_chebyshev}
\begin{aligned}
\min_{z} ~& \theta \\
s.t.~~ ~& \theta \ge 
          \frac{1}{G_k} \langle \mathbf{w}^{(k)}, \mathbf{w} \rangle + 
          (\frac{1}{2} - \frac{1}{G_k}) \langle \mathbf{w}^{(k)}, \mathbf{w}^{(k)} \rangle + 
          \frac{C}{nG_k} \langle \mathbf{1}, \bm{\xi} \rangle + 
          \frac{C}{n} (1 - \frac{1}{G_k}) \langle \mathbf{1}, \bm{\xi}^{(k)} \rangle,~ \forall k \\
        & g_k(z) \le 0,~ \forall k \\
        & -\xi_i \le 0,~ i = 1, \dots, n
\end{aligned}
\end{equation}
here 
$G_k 
= \sqrt{\langle \mathbf{w}^{(k)}, \mathbf{w}^{(k)} \rangle + \langle \frac{C}{n} \mathbf{1}, \frac{C}{n} \mathbf{1} \rangle} 
= \sqrt{\langle \mathbf{w}^{(k)}, \mathbf{w}^{(k)} \rangle + \frac{C^2}{n}}$
is a normalisation constant.


The method to generate the next query point used in Algorithm~\ref{alg:1slacktrain} can be rewritten (the epigraph form) as
\begin{equation}
\label{eq:nslack_genquery}
\begin{aligned}
\min_{z} ~& \theta \\
s.t.~~ ~& \theta \ge \frac{1}{2} \mathbf{w}^\top \mathbf{w} + \frac{C}{n} \langle \mathbf{1}, \bm{\xi} \rangle \\
        & g_k(z) \le 0,~ \forall k \\
        & -\xi_i \le 0.~ i = 1, \dots, n
\end{aligned}
\end{equation}

We observe similar differences as described in Section~\ref{sec:compare_1slack}.


\subsection{Efficient training via dualisation}
\label{sec:innerdual}

Recall that in Section~\ref{sec:nslackssvm}, 
the seperation oracle solves a loss-augmented inference problem for each query (Algorithm~\ref{alg:nslacktrain}),
which significantly reduces the scalability of the training algorithm.
Techniques have been developped to overcome this repeated inference, 
by exploring the dual problem of either the loss-augmented inference or 
the hinge-loss~\cite{taskar2004dissertation,taskar2005learning,meshi2010learning, bach2015paired},
which we review briefly in this section.

\eat{
\subsubsection{Dualise loss-augmented inference}
\label{sec:dualinf}
}

The $n$-slack formulation of structured SVM~(\ref{eq:nslackform}) described in Section~\ref{sec:nslackssvm} is equivalent to
\begin{align}
\min_{\mathbf{w}, ~\bm{\xi} \ge 0} ~& \frac{1}{2} \mathbf{w}^\top \mathbf{w} + \frac{C}{n} \sum_{i=1}^n \xi_i \\
s.t.~~ ~& \mathbf{w}^\top \Psi(\mathbf{x}_i, \mathbf{y}_i) + \xi_i \ge
          \max_{\bar{\mathbf{y}} \in \mathcal{Y}_i} 
          \left\{\mathbf{w}^\top \Psi(\mathbf{x}_i, \bar{\mathbf{y}}) + \Delta(\mathbf{y}_i, \bar{\mathbf{y}}) \right\},~\forall i. \label{eq:lossauginf}
\end{align}

If we can find a \emph{concise} formulation of the right-hand side of Equation~\ref{eq:lossauginf} (i.e., the loss-augmented inference),
in other words, the number of variables and constraints in the formulation is \emph{polynomial} in $L_i$, the number of variables in $\mathbf{y}_i$,
we can write its Lagrangian dual problem as 
\begin{align*}
\min_{\bm{\lambda}_i \ge \mathbf{0}} ~& h_i(\mathbf{w}, \bm{\lambda}_i) \\
s.t.~~ ~& g_i(\mathbf{w}, \bm{\lambda}_i) \le 0,
\end{align*}
where $h_i(\cdot)$ and $g_i(\cdot)$ are convex in both $\mathbf{w}$ and $\bm{\lambda}_i$.
Combining this minimisation over $\bm{\lambda}_i$ with the minimisation over $\mathbf{w}$ and $\bm{\xi}$,
we have a joint and compact convex minimisation problem
\begin{equation}
\label{eq:dualinf}
\begin{aligned}
\min_{\mathbf{w}, \bm{\xi}, \bm{\lambda}} ~& \frac{1}{2} \mathbf{w}^\top \mathbf{w} + \frac{C}{n} \sum_{i=1}^n \xi_i \\
s.t.~~ ~& \mathbf{w}^\top \Psi(\mathbf{x}_i, \mathbf{y}_i) + \xi_i \ge h_i(\mathbf{w}, \bm{\lambda}_i), ~\forall i \\
        & g_i(\mathbf{w}, \bm{\lambda}_i) \le 0, ~\forall i \\
        & \bm{\xi} \ge \mathbf{0}, ~\bm{\lambda}_i \ge \mathbf{0}, ~\forall i.
\end{aligned}
\end{equation}

Problem (\ref{eq:dualinf}) is a quadratic program with polynomial number of variables and constraints, 
and can be solved using existing off-the-shelf QP solvers.
Details of this approach are available in \cite{taskar2005learning}.

\eat{
\subsubsection{Dualise losses}
\label{sec:dualloss}

A slightly different approach is to approximate the loss (Equation~\ref{eq:nslackloss}) using linear programming (LP) relaxation and 
then dualise the LP. 

For a graph $G$ with a set of nodes $N$ and a set of edges $E$, 
we first assume that the structured label $\mathbf{y}$ is multivariate and has $d$ variables $(y_1, \dots, y_d)$,
in addition, the joint feature map $\Psi(\mathbf{x}, \mathbf{y})$ is assumed to decompose into singleton and pairwise factors,
\begin{equation*}
\mathbf{w}^\top \Psi(\mathbf{x}, \mathbf{y}) = 
\sum_{j \in N} \mathbf{w}_j^\top \phi_j(\mathbf{x}, y_j) + \sum_{jk \in E} \mathbf{w}_{jk}^\top \phi_{jk}(\mathbf{x}, y_j, y_k),
\end{equation*}
where $\mathbf{w}_j$ and $\mathbf{w}_{jk}$ are vectors of weights.

Furthermore, the discrepancy term $\Delta(\mathbf{y}, \bar{\mathbf{y}})$ is assumed to decompose according to variables in $\mathbf{y}$.
The loss of one training example can be approximated by the following LP relaxation
\begin{equation*}
\begin{aligned}
\max_{\bm{\mu} \ge \mathbf{0}} ~& \bm{\mu}^\top \bm{\theta}(\mathbf{w}) \\
s.t.~~ ~& \sum_{y_k} \mu_{jk}(y_j, y_k) = \mu_j(y_j), \\
        & \sum_{y_j} \mu_{jk}(y_j, y_k) = \mu_k(y_k), \\
        & \sum_{y_j} \mu_j(y_j) = 1.
\end{aligned}
\end{equation*}

Let $g(\bm{\delta}, \bm{\theta}(\mathbf{w}))$ be the dual (as described in~\cite{werner2007linear}) of the above LP relaxation,
problem (\ref{eq:nslackform}) can be rewritten as
\begin{equation*}
\min_{\mathbf{w}, ~\bm{\delta}_i} ~\frac{1}{2} \mathbf{w}^\top \mathbf{w} + \frac{C}{n} \sum_{i=1}^n g_i(\bm{\delta}_i, \bm{\theta}_i(\mathbf{w})),
\end{equation*}
which is an unconstrained minimisation problem that is jointly convex over $\mathbf{w}$ and $\bm{\delta}_i,\forall i$.
(Note that $g_i(\cdot)$ is not strictly convex but this can be overcome using a trick described in~\cite{meshi2010learning}.)

As $\bm{\delta}_i$ depends only on the $i$-th training example, block coordinate descent of $g_i(\bm{\delta}_i, \bm{\theta}_i(\mathbf{w}))$ 
can be performed in closed form~\cite{werner2007linear, globerson2008fixing}, 
which leads to the following learning loop:
\begin{enumerate}
\item perform coordinate descent, update $\bm{\delta}_i^{(t+1)}, \forall i$ using closed form equation,
\item take sub-gradient of the optimisation objective with respect to $\mathbf{w}$,
\item update parameters $\mathbf{w}^{(t+1)}$ using stochastic sub-gradient descent.
\end{enumerate}
Details of this approach are available in \cite{meshi2010learning}.
}

\eat{
\section{Trajectory recommendation via structured prediction}
\label{sec:recommend}

Given a set of point of interest (POI) $\mathcal{P} = \{p_1, \dots, p_M\}$ and a trajectory query $\mathbf{x} = (p_s, p_e, L)$,
where $p_s$ and $p_e$ are the desired start/end POI respectively, $L$ is the desired number of POIs.
We can model the desired trajectory with respect to query $\mathbf{x}$ as a chain of discrete variables, 
with the first and last variables being observed, and each variable has $|\mathcal{P}|$ states.
To make a recommendation, we find a trajectory that achieves the highest score
\begin{equation*}
\mathbf{y}^* = \argmax_{\mathbf{y} \in \mathcal{Y}} f(\mathbf{x}, \mathbf{y}),
\end{equation*}
where $\mathcal{Y}$ is the set of all possible trajectory with POIs in $\mathcal{P}$ and satisfies query $\mathbf{x}$,
$f(\mathbf{x}, \mathbf{y})$ is a function that scores the compatibility between query $\mathbf{x}$ and a specific trajectory $\mathbf{y}$.

One common choice of the compatibility function $f(\cdot)$ is the linear form
$f(\mathbf{x}, \mathbf{y}) = \mathbf{w}^\top \Psi(\mathbf{x}, \mathbf{y})$,
where $\mathbf{w}$ is a vector of model parameters and 
the vector $\Psi(\mathbf{x}, \mathbf{y})$ is called a \emph{joint feature map} 
which captures features extracted from both query $\mathbf{x}$ and trajectory $\mathbf{y}$.


\subsection{Joint feature}
\label{sec:jointfeature}

The design of joint feature $\Psi(\cdot)$ is problem specific, 
in the setting of trajectory recommendation, we assume $\Psi(\cdot)$ can be decomposed into singleton and pairwise features, 
and share parameters among nodes and edges, i.e.,
\begin{equation}
\label{eq:jointfeature}
\mathbf{w}^\top \Psi(\mathbf{x}, \mathbf{y}) = \sum_{j=1}^L \mathbf{w}_u^\top \phi_j(\mathbf{x}, y_j) +
                                               \sum_{j=1}^{L-1} \mathbf{w}_p^\top \phi_{j, j+1}(\mathbf{x}, y_j, y_{j+1}),
\end{equation}
where $\mathbf{w}_u$ and $\mathbf{w}_p$ are parameter vectors,
$y_j$ is the $j$-th POI in trajectory $\mathbf{y}$, 
$\phi_j$ is a feature vector of POI $y_j$ with respect to query $\mathbf{x}$ (Table~\ref{tab:poifeature}),
$\phi_{j,j+1}$ is a pairwise feature vector that captures the affinity of transition from POI $y_j$ to POI $y_{j+1}$ and
here we use the transition probabilities between individual POI features as described in Table~\ref{tab:tranfeature}.
This joint feature design shares parameters among POIs/transitions in a trajectory.


\subsection{Problem formulation}
\label{sec:trajform}

Given a joint feature design $\Psi(\cdot)$ and a discrepancy function $\Delta(\cdot)$,
we can formulate trajectory recommendation problem using the $n$-slack formulation of structured SVM described in Section~\ref{sec:nslackssvm},
for an example $(\mathbf{x}, \mathbf{y})$, we have a constraint
\begin{equation}
\label{eq:trajcons}
\mathbf{w}^\top \Psi(\mathbf{x}, \mathbf{y}) + \xi \ge
          \max_{\bar{\mathbf{y}} \in \mathcal{Y}} 
          \left\{\mathbf{w}^\top \Psi(\mathbf{x}, \bar{\mathbf{y}}) + \Delta(\mathbf{y}, \bar{\mathbf{y}}) \right\},
\end{equation}
considering the sub-tour elimination constraints in a trajectory, we can formulate the right-hand side of constraint (\ref{eq:trajcons}), 
i.e., the loss-augmented inference,
using an integer linear program (ILP), assuming the discrepancy function $\Delta(\cdot)$ can be represented as a linear function, 
%Hamming loss seems not possible
\begin{alignat}{5}
& \max_{u,v} ~&& \frac{1}{2} \sum_{j=1}^M \sum_{k=1}^M u_{jk} \mathbf{w}_u^\top \phi_j(\mathbf{x}, p_j) + 
                 \sum_{j=1}^M \sum_{k=1}^M u_{jk} \mathbf{w}_p^\top \phi_{j, k}(\mathbf{x}, p_j, p_k) + \Delta(\mathbf{y}, \bar{\mathbf{y}}) \\
& s.t. ~~ ~&& u_{jk} \in \{0, 1\}, ~u_{jj} = 0, ~v_j \in \mathbf{Z},~ p_j \in \mathcal{P}, ~\forall j, k = 1, \cdots, M    \label{eq:cons1} \\
&        && \sum_{k=2}^M u_{1k} = \sum_{j=1}^{M-1} u_{jM} = 1, ~\sum_{j=2}^M u_{j1} = \sum_{k=1}^{M-1} u_{Mk} = 0  \label{eq:cons2} \\
&        && \sum_{j=1}^{M-1} u_{jl} = \sum_{k=2}^M x_{lk} \le 1,   ~\forall l=2, \cdots, M-1                       \label{eq:cons3} \\
&        && \sum_{j=1}^{M-1} \sum_{k=2}^M u_{jk} = L-1,                                                            \label{eq:cons4} \\
&        && v_j - v_k + 1 \le (M-1) (1-u_{jk}),                     \forall j, k = 2, \cdots, M                    \label{eq:cons5}
\end{alignat}
where $u_{jk}$ is a binary decision variable that determines whether the transition from $p_j$ to $p_k$ is in the resulting trajectory,
$L$ is the number of POIs in trajectory $\mathbf{y}$, and $\bar{\mathbf{y}}$ is the trajectory corresponding to the optimal solution of this ILP.
For brevity, we arrange the POIs such that $p_1 = p_s$ and $p_M = p_e$.
Firstly, the desired trajectory should start from $p_s$ and end at $p_e$ (Constraint~\ref{eq:cons2}).
In addition, any POI could be visited at most once (Constraint~\ref{eq:cons3}).
Moreover, only $L-1$ transitions between POIs are permitted (Constraint~\ref{eq:cons4}),
i.e., the number of POI visits should be exactly $L$ (including $p_s$ and $p_e$).
The last constraint, where $v_i$ is an auxiliary variable,
enforces that only a single sequence of POIs without sub-tours is permitted in the trajectory.


\subsection{Discrepancy function}
\label{sec:lossfunc}

Similar to the joint feature map $\Psi(\cdot)$, the design of $\Delta(\cdot)$ is also problem specific.
To measure the discrepancy between the predicted trajectory $\bar{\mathbf{y}}$ and the ground truth $\mathbf{y}$, 
one option could be the \emph{Hamming loss}, as trajectory is a sequence of POI visits, 
\begin{equation}
\label{eq:hammingloss}
%\Delta(\mathbf{y}, \bar{\mathbf{y}}) = \frac{1}{L} \sum_{j=1}^L \textbf{xor} (y_j, \bar{y}_j) = \frac{1}{L} \sum_{j=1}^L y_j \oplus \bar{y}_j,
\Delta(\mathbf{y}, \bar{\mathbf{y}}) = \frac{1}{L} \sum_{j=1}^L (y_j \neq \bar{y}_j) = \frac{1}{L} \sum_{j=1}^L \left(1 - \delta_{y_j, \bar{y}_j} \right),
\end{equation}
%In the formulation described in Section~\ref{sec:trajform}, 
where $\delta_{y_j, \bar{y}_j}$ is the Kronecker delta,
and the prediction $\bar{\mathbf{y}}$ can be computed as follows,
\begin{align}
\bar{y}_1 &= y_1, \\
\bar{y}_j &= j \sum_{k=1}^M u_{\bar{y}_{j-1}, k},~ j=2,\dots,L.  \label{eq:poipred}
\end{align}

As both $y_j$ and $\bar{y}_j$ in Equation~(\ref{eq:hammingloss}) take integer values,
and the index $\bar{y}_{j-1}$ in Equation~(\ref{eq:poipred}) is a variable,
it seems that Hamming loss is not a linear function of $u$ in this case.
%The $\textbf{xor}$ operator in Equation~(\ref{eq:hammingloss}) is linear, however,
%the indexing operator in Equation~(\ref{eq:poipred}) is \emph{non-linear} since $\bar{y}_{j-1}$ is a variable.
%On the other hand, 
However, if we measure the discrepancy using the number of mispredicted POIs (normalised),
\begin{equation}
\Delta(\mathbf{y}, \bar{\mathbf{y}}) = \frac{1}{L} \mid \textbf{set}(\mathbf{y}) \setminus \textbf{set}(\bar{\mathbf{y}}) \mid 
                                     = \frac{1}{L} \sum_{j=1}^L \left( 1 - \sum_{k=1}^M u_{y_j, k} \right),
\end{equation}
or the number of mispredicted transitions (normalised),
\begin{equation}
\Delta(\mathbf{y}, \bar{\mathbf{y}}) = \frac{1}{L} \sum_{j=1}^{L-1} \left( 1 - u_{y_j, y_{j+1}} \right),
\end{equation}
which are linear as $y_j$ is a constant.



% inference method
%Given the trained $\mathbf{w}$ and $\xi$, the prediction of $\mathbf{x}$ is 
%$\hat{\mathbf{y}} = \argmax_{\bar{\mathbf{y}} \in \mathcal{Y}} \langle \mathbf{w},~ \Psi(\mathbf{x}, \bar{\mathbf{y}}) \rangle$
%Which means whether sub-tours are permitted depends on the inference method, it follows that cutting-plane cannot help to eliminate sub-tours.
}
